{
  "total_cost": 0,
  "task_id": "pytest-dev__pytest-7432",
  "model": "gpt-4o-2024-08-06",
  "spans": [
    {
      "name": "KeywordExtract",
      "metadata": {
        "start": "2024-08-19T12:21:20.902601",
        "usage": "1266.4589881896973 ms"
      },
      "request": {
        "repo_tokens": [
          "pytest_collect_directory",
          "add_attribute",
          "_getobj",
          "delattr",
          "_assertion_pass",
          "disabled",
          "compile",
          "fullsource",
          "_ensure_relative_to_basetemp",
          "_teardown_yield_fixture",
          "store_mark",
          "Frame",
          "find_spec",
          "resolve_from_str",
          "MarkGenerator",
          "catch_warnings_for_item",
          "_folded_skips",
          "exitstatus",
          "MODULE_NOT_FOUND_ERROR",
          "reprtraceback",
          "_match_lines",
          "run",
          "not_expr",
          "DEFAULT_ABSOLUTE_TOLERANCE",
          "_should_truncate_item",
          "toterminal",
          "getfailedcollections",
          "Source",
          "STRING_TYPES",
          "_is_setup_py",
          "_visit_filter",
          "_getnextfixturedef",
          "append_skipped",
          "expression",
          "REPORT_COLLECTING_RESOLUTION",
          "runpython",
          "PYC_EXT",
          "_getlines",
          "mkpydir",
          "scopemismatch",
          "reset",
          "print_teardown_sections",
          "_E",
          "_inject_setup_function_fixture",
          "Expression",
          "tmp_path",
          "explanation_param",
          "CLOSE_STDIN",
          "get",
          "get_closest_marker",
          "FunctionDefinition",
          "PytestArg",
          "pytest",
          "FormattedExcinfo",
          "scope",
          "main",
          "_fillfixtures",
          "pytestconfig",
          "maketxtfile",
          "_getconftestmodules",
          "_find_spec",
          "cached_result",
          "ApproxDecimal",
          "_parse_parametrize_args",
          "safeformat",
          "check_interactive_exception",
          "recursionindex",
          "cleanup_numbered_dir",
          "append_failure",
          "_with_exception",
          "applymarker",
          "_check_all_skipped",
          "evaluate_skip_marks",
          "ApproxNumpy",
          "bin_xml_escape",
          "set_location",
          "exprinfo",
          "RESULT_LOG",
          "_dump_lines",
          "_yield_comparisons",
          "re_match_lines_random",
          "_recursive_debug",
          "classnamefilter",
          "makepyfile",
          "FUNCARGNAMES",
          "INTERNAL_ERROR",
          "repr_instance",
          "record_tuples",
          "CollectReport",
          "_get_pdb_wrapper_class",
          "setuponly",
          "_import_plugin_specs",
          "pytest_runtest_teardown",
          "getstatementrange",
          "RunResult",
          "UnitTestCase",
          "write_log_entry",
          "DummyRewriteHook",
          "deprecated_call",
          "_write_pyc_fp",
          "get_terminal_writer",
          "_param_ids_from",
          "exec_",
          "visit_Call",
          "inline_genitems",
          "writeorg",
          "no_summary",
          "__eq__",
          "update_testcase_duration",
          "USAGE_MSG",
          "_init_checker_class",
          "collect_one_node",
          "_code",
          "_check_non_top_pytest_plugins",
          "append_pass",
          "_iter_rewritable_modules",
          "teardown",
          "setattr",
          "_determine_main_color",
          "DontReadFromInput",
          "_start",
          "push_format_context",
          "OR",
          "value",
          "consider_preparse",
          "unittest",
          "_param_ids_generated",
          "_PYTEST_DIR",
          "exconly",
          "add_stats",
          "ReprTraceback",
          "exec_module",
          "InvocationParams",
          "_colorama_workaround",
          "get_source",
          "set_level",
          "_pformat_dispatch",
          "consider_pluginarg",
          "FixtureDef",
          "_format_args",
          "add_property",
          "source",
          "NFPlugin",
          "pytest_deselected",
          "_format_assertmsg",
          "from_parent",
          "_exec_lsof",
          "__module__",
          "_set_opt_strings",
          "pytest_fixture_setup",
          "pytest_runtest_call",
          "FIXTURE_POSITIONAL_ARGUMENTS",
          "RaisesContext",
          "_excinfo",
          "get_direct_param_fixture_func",
          "_getcrashline",
          "AssertionRewriter",
          "LsofFdLeakChecker",
          "_recursive_list_map",
          "isattrs",
          "xfailed_key",
          "junitxml",
          "_mk_tmp",
          "importlib",
          "_ascii_escaped_by_config",
          "_pyfuncitem",
          "getcall",
          "write_fspath_result",
          "_matches_prefix_or_glob_option",
          "_trace",
          "HelpAction",
          "NODE_USE_FROM_PARENT",
          "pytest_assertion_pass",
          "LEVELNAME_FMT_REGEX",
          "_get_pos",
          "notset",
          "Cache",
          "stop_capturing",
          "get_location",
          "T",
          "V",
          "_log_cli_enabled",
          "on_rm_rf_error",
          "processoption",
          "_config",
          "LFPluginCollSkipfiles",
          "TestCaseFunction",
          "PYTESTER_COPY_EXAMPLE",
          "__contains__",
          "get_timeout_config_value",
          "DOCTEST_REPORT_CHOICES",
          "ParameterSet",
          "hasplugin",
          "capstdout",
          "iscoroutinefunction",
          "pytest_pycollect_makeitem",
          "_compare_eq_sequence",
          "PYTEST_COLLECT_MODULE",
          "location",
          "_report_kwargs_from_json",
          "genitems",
          "isinitpath",
          "stop",
          "_getfailureheadline",
          "getcrashentry",
          "CFG_PYTEST_SECTION",
          "DEFAULT_RELATIVE_TOLERANCE",
          "log_outcome",
          "pytest_runtest_makereport",
          "append_collect_skipped",
          "_FixtureFunc",
          "register",
          "get_default_arg_names",
          "cleanup_candidates",
          "EMPTY_PARAMETERSET_OPTION",
          "_check_record_param_type",
          "format_explanation",
          "makeconftest",
          "ParsedCall",
          "_finalize",
          "_addfinalizer",
          "getworkerinfoline",
          "pytest_fixture_post_finalizer",
          "_get_flag_lookup",
          "resume_fixture",
          "findsource",
          "doctest",
          "fileno",
          "_validate_usepdb_cls",
          "Class",
          "filter_traceback_for_conftest_import_failure",
          "_non_printable_ascii_translate_table",
          "cut",
          "_readline_workaround",
          "helpconfig",
          "getfirstlinesource",
          "DOCTEST_REPORT_CHOICE_NDIFF",
          "prepend",
          "reprentries",
          "old_mark_config_key",
          "create_cleanup_lock",
          "ensure_deletable",
          "rewrite",
          "scopename2class",
          "verbosity",
          "derive_importpath",
          "make_numbered_dir",
          "resolve",
          "_inject_setup_module_fixture",
          "entrysep",
          "DoctestTextfile",
          "RPAREN",
          "families",
          "pytest_terminal_summary",
          "_inject_setup_method_fixture",
          "PyobjMixin",
          "__array_priority__",
          "get_scope_node",
          "TerminalRepr",
          "getmodpath",
          "pastebin",
          "_should_repr_global_name",
          "Junit",
          "fnmatch_lines_random",
          "repr_args",
          "_node_location_to_relpath",
          "skipping",
          "_get_allow_bytes_flag",
          "section",
          "_ensure_unconfigure",
          "_getscopeitem",
          "warning_types",
          "messages",
          "filter_traceback",
          "getimfunc",
          "getcalls",
          "isatty",
          "README_CONTENT",
          "name2fixturedefs",
          "SysModulesSnapshot",
          "get_log_level_for_setting",
          "showlocals",
          "match",
          "PytestUnhandledCoroutineWarning",
          "get_empty_parameterset_mark",
          "tmpdir",
          "_resolve_arg_value_types",
          "call_runtest_hook",
          "done",
          "_get_teardown_reports",
          "add_report_section",
          "PytestPluginManager",
          "parse_summary_nouns",
          "getnode",
          "COLLECT_DIRECTORY_HOOK",
          "try_makedirs",
          "suspend",
          "_get_increasing_order",
          "getplugin",
          "ensure_extended_length_path",
          "pos",
          "D",
          "at_level",
          "attrs",
          "popen",
          "_importtestmodule",
          "visit_Attribute",
          "_get_unknown_ini_keys",
          "PrintHelp",
          "timing",
          "reject",
          "MyOptionParser",
          "_gethookproxy",
          "parseconfig",
          "scope2index",
          "getbasetemp",
          "_addexcinfo",
          "_width_of_current_line",
          "startTest",
          "_report_keyboardinterrupt",
          "snap",
          "_result",
          "is_true",
          "_format",
          "format_session_duration",
          "from_item",
          "parseoutcomes",
          "warn",
          "reorder_items_atscope",
          "monkeypatch",
          "variable",
          "resolve_fixture_function",
          "default_plugins",
          "_log_text",
          "resume_capturing",
          "_add_simple",
          "raises",
          "COLLECT_FAKEMODULE_ATTRIBUTES",
          "_assert_state",
          "wcswidth",
          "execute",
          "approx",
          "matchreport",
          "LineComp",
          "add_global_property",
          "FaultHandlerHooks",
          "path_matches_patterns",
          "stop_global_capturing",
          "capstderr",
          "_version",
          "illegal_xml_re",
          "hookspec",
          "wrap_pytest_function_for_tracing",
          "short_test_summary",
          "parse_num",
          "_checkargnotcontained",
          "collect_by_name",
          "parsefactories",
          "getmodulecol",
          "getpathnode",
          "result",
          "__test__",
          "EOF",
          "ColoredLevelFormatter",
          "node_reporter",
          "merge_family",
          "DOCTEST_REPORT_CHOICE_NONE",
          "_rget_with_confmod",
          "pytester",
          "fail",
          "TeeCaptureIO",
          "pytest_unconfigure",
          "listoutcomes",
          "_validate_args",
          "mark_rewrite",
          "showversion",
          "iter_markers_with_node",
          "getvalue",
          "reorder_items",
          "_warn_incompatibility_with_xunit2",
          "_is_marked_for_rewrite",
          "_add_stats",
          "repr",
          "evaluate",
          "SEP",
          "SubRequest",
          "getreportopt",
          "ApproxScalar",
          "PytestUnknownMarkWarning",
          "_ET",
          "get_exconly",
          "get_open_files",
          "CACHEDIR_TAG_CONTENT",
          "hasopt",
          "_get_main_color",
          "check_testcase_implements_trial_reporter",
          "_is_numpy_array",
          "getsource",
          "ids",
          "tmp_path_factory",
          "_disable_output_capturing_for_darwin",
          "is_globally_capturing",
          "LPAREN",
          "display",
          "pytest_collectreport",
          "LOCK_TIMEOUT",
          "DEFAULT_LOG_FORMAT",
          "global_and_fixture_disabled",
          "fixture",
          "_expecting_failure",
          "pytest_cmdline_parse",
          "_enter_pdb",
          "add_marker",
          "cache_key",
          "freeze_includes",
          "node",
          "from_item_and_call",
          "linecomp",
          "makeini",
          "_format_action_invocation",
          "generic_visit",
          "_W",
          "extract_from",
          "pytest_collection_modifyitems",
          "create_new_paste",
          "getinicfg",
          "_report_unserialization_failure",
          "setupplan",
          "issequence",
          "StoreKey",
          "isset",
          "makefile",
          "LoggingPlugin",
          "__hash__",
          "DoctestItem",
          "traceback",
          "_ALLOW_MARKERS",
          "head_line",
          "_assert_expr_to_lineno",
          "autouse",
          "_importconftest",
          "PYTEST_TAG",
          "args",
          "_FileHandler",
          "_checkversion",
          "copy",
          "FastFilesCompleter",
          "PytestDeprecationWarning",
          "set_fixture",
          "visit_Compare",
          "_ensure_removed_sysmodule",
          "showfspath",
          "_parsearg",
          "start_capturing",
          "ArgumentError",
          "_truncate_by_char_count",
          "_prepareconfig",
          "pytest_pycollect_makemodule",
          "_has_param_ids",
          "__array_ufunc__",
          "summary_errors",
          "AlwaysDispatchingPrettyPrinter",
          "_ellipsize",
          "pytest_collection",
          "get_data",
          "pytest_report_collectionfinish",
          "_args_converter",
          "DOCTEST_REPORT_CHOICE_UDIFF",
          "conftest_options",
          "find_prefixed",
          "accept",
          "_check_scope",
          "outcomes",
          "_format_boolop",
          "DropShorterLongHelpFormatter",
          "_is_doctest",
          "HookRecorder",
          "getreports",
          "showhelp",
          "pytest_cmdline_main",
          "__attrs_post_init__",
          "MARK_GEN",
          "fspath",
          "USAGE_ERROR",
          "_opentestcase",
          "_recurse",
          "create_terminal_writer",
          "mark",
          "register_assert_rewrite",
          "MINUS_K_COLON",
          "locate_config",
          "_Markable",
          "syspath_prepend",
          "rm_rf",
          "fixturenames",
          "write_line",
          "MarkDecorator",
          "tb",
          "addUnexpectedSuccess",
          "pytest_collection_finish",
          "WARNING_CAPTURED_HOOK",
          "getstatementrange_ast",
          "ReprFuncArgs",
          "find_suffixes",
          "fill_unfilled",
          "syspathinsert",
          "_S",
          "num_mock_patch_args",
          "UsageError",
          "teardown_nose",
          "_validate_keys",
          "resume_global_capture",
          "IGNORE_PAM",
          "indent",
          "istext",
          "showlongtestinfo",
          "Code",
          "terminal",
          "_split_explanation",
          "assert_contains",
          "runpython_c",
          "_io",
          "cwd_relative_nodeid",
          "show_fixtures_per_test",
          "NOT",
          "pytest_warning_recorded",
          "_get_plugin_specs_as_list",
          "_strtobool",
          "addsection",
          "deindent",
          "NotSetType",
          "set_when",
          "_perform_collect",
          "delenv",
          "_format_repr_exception",
          "should_do_markup",
          "fromdictargs",
          "__setitem__",
          "_genfunctions",
          "fault_handler_stderr_key",
          "raiseerror",
          "fnmatch_ex",
          "reprfuncargs",
          "Traceback",
          "pytest_runtestloop",
          "_setupstate",
          "item_capture",
          "longrepr",
          "TestReport",
          "capfd",
          "_try_repr_or_str",
          "repr_excinfo",
          "__init__",
          "_truncate_recursive_traceback",
          "emit",
          "_resume",
          "getini",
          "_warn_already_imported",
          "argnames",
          "repr_failure",
          "_get_multicapture",
          "cls",
          "getlocation",
          "parseconfigure",
          "repr_locals",
          "_argcomplete",
          "obj",
          "assert_outcomes",
          "_validate_ids",
          "visit_Name",
          "report_collect",
          "finish",
          "_compilecounter",
          "iter_markers",
          "_patch_unwrap_mock_aware",
          "_get_line_with_reprcrash_message",
          "AssertionState",
          "ihook",
          "call_optional",
          "_highlight",
          "_get_override_ini_value",
          "ExceptionRepr",
          "putaround",
          "deactivate_fixture",
          "set_log_path",
          "wrap_session",
          "pytest_warning_captured",
          "filename_arg",
          "get_plugin_manager",
          "function",
          "ReprExceptionInfo",
          "getfuncargnames",
          "ImportMode",
          "suspend_fixture",
          "SafeRepr",
          "LOGLEVEL_COLOROPTS",
          "__len__",
          "_is_last_item",
          "summary_stats",
          "__next__",
          "getrawcode",
          "_initrequest",
          "_CACHE_PREFIX_DIRS",
          "path",
          "inline_run",
          "pytest_make_parametrize_id",
          "name",
          "catch_log_handler_key",
          "set_trace",
          "parse_known_and_unknown_args",
          "PseudoFixtureDef",
          "_fail",
          "_callfinalizers",
          "CollectErrorRepr",
          "KeywordMatcher",
          "runner",
          "call_fixture_func",
          "teardown_all",
          "errisinstance",
          "hookimpl",
          "capfdbinary",
          "addini",
          "IDENT",
          "runpytest_inprocess",
          "Failed",
          "flow_marker",
          "_AnyPurePath",
          "extract_suffixes",
          "_color_for_type_default",
          "pytest_collectstart",
          "setenv",
          "_schedule_finalizers",
          "_HandlerType",
          "isiterable",
          "FSCollector",
          "BINOP_MAP",
          "style",
          "_log",
          "pytest_internalerror",
          "pytest_enter_pdb",
          "visit_Starred",
          "_update_message",
          "FixtureLookupError",
          "FixtureManager",
          "doctest_namespace",
          "_getentrysource",
          "makedir",
          "_bytes_to_ascii",
          "ensure_newline",
          "saferepr",
          "evaluate_xfail_marks",
          "_basetemp",
          "chdir",
          "_matchfactories",
          "activate_fixture",
          "DOCTEST_REPORT_CHOICE_CDIFF",
          "gethookproxy",
          "_get_assertion_exprs",
          "Exception",
          "mode",
          "CaptureResult",
          "str",
          "isparseable",
          "OK",
          "get_unpacked_marks",
          "tbfilter",
          "pytest_report_teststatus",
          "pytestPDB",
          "_setup_fixtures",
          "_running_on_ci",
          "is_generator",
          "_idval",
          "_write_progress_information_filling_space",
          "getpluginversioninfo",
          "stopTest",
          "_consider_importhook",
          "PdbInvoke",
          "caplog",
          "OptionGroup",
          "error",
          "load_config_dict_from_file",
          "LFPluginCollWrapper",
          "NodeMeta",
          "_is_capturing",
          "FixtureRequest",
          "_esctable",
          "_cachedir",
          "_get_global_properties_node",
          "_write_report_lines_from_hooks",
          "_iter_all_modules",
          "_bestrelpath_cache",
          "extraline",
          "ReprFileLocation",
          "format",
          "reprlocals",
          "_ANSI_ESCAPE_SEQ",
          "_create",
          "set",
          "pytest_report_from_serializable",
          "essential_plugins",
          "invocation_dir",
          "_get_first_non_fixture_func",
          "scopeproperty",
          "getitem",
          "_sys_snapshot",
          "names",
          "is_capturing",
          "install_importhook",
          "DEFAULT_MAX_LINES",
          "initialnames",
          "LogXML",
          "__getitem__",
          "count_towards_summary",
          "for_config",
          "get_config",
          "FuncFixtureInfo",
          "Testdir",
          "parse_hookimpl_opts",
          "tracebackcutdir",
          "addExpectedFailure",
          "get_public_names",
          "cacheshow",
          "TracebackEntry",
          "relline",
          "_get_auto_indent",
          "_getconftest_pathlist",
          "get_scope_package",
          "Package",
          "IDENT_PREFIX",
          "patchsysdict",
          "start_global_capturing",
          "record_testsuite_property",
          "warns",
          "_pytest",
          "pytest_itemcollected",
          "compile_",
          "assert_contains_lines",
          "chain",
          "parts",
          "FSHookProxy",
          "_NodeReporter",
          "_locationline",
          "TempdirFactory",
          "helper",
          "and_expr",
          "_write_entry_lines",
          "Skip",
          "_teardown_towards",
          "undo",
          "MultipleDoctestFailures",
          "set_session",
          "async_warn_and_skip",
          "lineno",
          "MoreQuietAction",
          "Token",
          "LineMatcher",
          "context",
          "get_extended_length_path_str",
          "fixtures",
          "__call__",
          "wcwidth",
          "countoutcomes",
          "nose",
          "_makeitem",
          "reports",
          "TYPE_CHECKING",
          "_compute_fixture_value",
          "_outrep_summary",
          "debugging",
          "assertrepr_compare",
          "_read_pyc",
          "_approx_scalar",
          "_issue_warning_captured",
          "get_cache_dir",
          "PytestAssertRewriteWarning",
          "scope2props",
          "records",
          "Argument",
          "_getvaluepath",
          "_import_pdb_cls",
          "listchain",
          "get_dirs_from_args",
          "_early_rewrite_bailout",
          "freeze_support",
          "CaptureIO",
          "params",
          "isnosetest",
          "makepyprojecttoml",
          "get_real_func",
          "get_common_ancestor",
          "warnings",
          "skipped_by_mark_key",
          "config",
          "list",
          "newinstance",
          "getfixturemarker",
          "fslocation",
          "wrap_function_to_error_out_if_called_directly",
          "delitem",
          "abspath",
          "_getautousenames",
          "_try_load_conftest",
          "for_later",
          "_plugin_nameversions",
          "repr_traceback",
          "_postmortem_traceback",
          "EncodedFile",
          "TERMINALWRITER_WRITER",
          "mkdir",
          "ishidden",
          "getitems",
          "_addoption_instance",
          "validate_basetemp",
          "import_path",
          "PytestWarning",
          "_processopt",
          "_force_symlink",
          "symlink_or_skip",
          "REGEX_TYPE",
          "assign",
          "_getindent",
          "finish_recording",
          "own_mark_names",
          "_format_lines",
          "plugins",
          "setdefault",
          "FILLFUNCARGS",
          "reportinfo",
          "fnmatch_lines",
          "gethookrecorder",
          "_params_converter",
          "get_terminal_width",
          "MonkeyPatch",
          "NOTSET",
          "_to_json",
          "Mark",
          "_handle_teardown_sections",
          "ExceptionInfo",
          "encoding",
          "annotated_getattr",
          "_remove_ansi_escape_sequences",
          "readlines",
          "console_main",
          "cmdline",
          "__delitem__",
          "inline_runsource",
          "summary_warnings",
          "pytest_exception_interact",
          "typename",
          "CaptureFixture",
          "write_captured_output",
          "CHECKER_CLASS",
          "_warn_about_missing_assertion",
          "SysCaptureBinary",
          "_make_xunit_fixture",
          "get_option_ini",
          "WarningReport",
          "NO_TESTS_COLLECTED",
          "close",
          "register_cleanup_lock_removal",
          "_inject_setup_teardown_fixtures",
          "deprecated",
          "eval",
          "_is_skipped",
          "ExceptionChainRepr",
          "pytest_generate_tests",
          "_testcase",
          "handleError",
          "_show_fixtures_per_test",
          "collect",
          "showfixtures",
          "_traceback",
          "astcache",
          "RUNNER_CLASS",
          "_get_runner",
          "pytest_runtest_logstart",
          "get_user",
          "TimeoutExpired",
          "_given_basetemp",
          "skip",
          "SysCapture",
          "addfinalizer",
          "start",
          "pytest_runtest_protocol",
          "hasnew",
          "from_current",
          "text",
          "ConftestImportFailure",
          "append",
          "assertoutcome",
          "pytest_runtest_logfinish",
          "names_closure",
          "readline",
          "fail_marker",
          "pytest_plugin_registered",
          "pytest_pyfunc_call",
          "writer",
          "prune_dependency_tree",
          "set_repr_style",
          "make_properties_node",
          "__slots__",
          "FixtureFunctionMarker",
          "rex_session_duration",
          "_parse_lsof_output",
          "_LiveLoggingStreamHandler",
          "popcall",
          "perform_collect",
          "simple",
          "clear",
          "_get_direct_parametrize_args",
          "_truncate_explanation",
          "Collector",
          "Notset",
          "message",
          "ApproxSequencelike",
          "_update_current_test_var",
          "_compare_eq_iterable",
          "listnames",
          "assertstate_key",
          "kwargs",
          "cache",
          "isdatacls",
          "truncate_if_required",
          "_PluggyPlugin",
          "no_re_match_line",
          "TokenType",
          "restore",
          "MINUS_K_DASH",
          "__exit__",
          "runpytest",
          "is_async_function",
          "_get_allow_unicode_flag",
          "_FixtureFunction",
          "_from_json",
          "pytest_load_initial_conftests",
          "try_cleanup",
          "_no_match_line",
          "_showfixtures_main",
          "_is_mocked",
          "to_xml",
          "instance",
          "repr_traceback_entry",
          "PytestConfigWarning",
          "getrepr",
          "Node",
          "parametrize",
          "strict",
          "is_rewrite_disabled",
          "write_sep",
          "truncate_locals",
          "_names",
          "_write_source",
          "_factorytraceback",
          "WarningsRecorder",
          "resultlog",
          "statement",
          "pytest_assertrepr_compare",
          "get_sections",
          "_NodeType",
          "_makepath",
          "get_lock_path",
          "__repr__",
          "spawn",
          "_legal_chars",
          "pop",
          "determine_setup",
          "fix_cache_order",
          "_parse_filter",
          "ReprEntry",
          "_reprcompare",
          "pytest_runtest_logreport",
          "_makefile",
          "CallSpec2",
          "_compare_eq_any",
          "_fixturemanager",
          "id",
          "_init_runner_class",
          "getfixturevalue",
          "_assert_start_repr",
          "_saferepr",
          "K",
          "StepwisePlugin",
          "when",
          "make_hook_recorder",
          "summary_failures",
          "pastebinfile_key",
          "_report_to_json",
          "CwdSnapshot",
          "_init_pdb",
          "_pluginmanager",
          "getfailures",
          "testdir",
          "expr",
          "_notin_text",
          "visit_BinOp",
          "ApproxMapping",
          "fullwidth",
          "ExitCode",
          "ResultLog",
          "FDCapture",
          "_ensure_supporting_files",
          "_seen",
          "lines",
          "_CACHE_PREFIX_VALUES",
          "MarkMatcher",
          "Interrupted",
          "unset_fixture",
          "assert_never",
          "_repr_failure_py",
          "keywords",
          "TerminalReporter",
          "strip",
          "_validate_plugins",
          "_wrapped_pdb_cls",
          "JUNIT_XML_DEFAULT_FAMILY",
          "pytest_report_header",
          "_saved",
          "ReprFailDoctest",
          "copy_example",
          "_initini",
          "_validate_if_using_arg_names",
          "parse_args",
          "EMPTY_BUFFER",
          "_get_number_flag",
          "recwarn",
          "mktemp",
          "summary_passes",
          "pytest_keyboard_interrupt",
          "addSuccess",
          "File",
          "directory_arg",
          "_T",
          "notify_exception",
          "__getattr__",
          "template",
          "_determine_show_progress_info",
          "_getreprcrash",
          "scopes",
          "_ensure_basetemp",
          "parse_known_args",
          "_PY_DIR",
          "xfail",
          "__ne__",
          "build_summary_stats_line",
          "_check_if_assertion_pass_impl",
          "ImportPathMismatchError",
          "unexpectedsuccess_key",
          "Exit",
          "stream",
          "_legal_xml_re",
          "setup",
          "nodeid",
          "consider_env",
          "normalize_mark_list",
          "_repr_style",
          "exit",
          "mangle_test_address",
          "_should_rewrite",
          "pytest_cmdline_preparse",
          "LineMatcher_fixture",
          "longreprtext",
          "_parse_parametrize_parameters",
          "buffer",
          "safe_getattr",
          "_get_continue_on_failure",
          "_match_lines_random",
          "from_config",
          "handler",
          "no_header",
          "NoCapture",
          "_rewrite_test",
          "PercentStyleMultiline",
          "_check_initialpaths_for_relpath",
          "__take_sys_modules_snapshot",
          "_split_lines",
          "reprfileloc",
          "Store",
          "FakeCollectModule",
          "suspend_capturing",
          "CollectError",
          "_config_for_test",
          "_compare_eq_set",
          "_surrounding_parens_on_own_lines",
          "get_statement_startend2",
          "_addoption",
          "pytest_addoption",
          "Function",
          "_collect",
          "_call_assertion_pass",
          "_compare_eq_dict",
          "pytest_runtest_setup",
          "UnformattedWarning",
          "_get_verbose_word",
          "__dir__",
          "token",
          "lex",
          "pytest_sessionfinish",
          "_getparser",
          "resume",
          "INTERRUPTED",
          "showheader",
          "_call_reprcompare",
          "is_potential_nosetest",
          "pytest_make_collect_report",
          "getfixtureinfo",
          "failed",
          "getfixturedefs",
          "funcargs",
          "_ensure_immutable_ids",
          "_get_report_choice",
          "addError",
          "pytest_collect_file",
          "width_of_current_line",
          "_in_venv",
          "matching_platform",
          "BaseReport",
          "DEPRECATED_EXTERNAL_PLUGINS",
          "write_docstring",
          "add_cleanup",
          "frame",
          "Parser",
          "_getpytestargs",
          "_translate_non_printable",
          "readouterr",
          "__version_tuple__",
          "python_api",
          "param",
          "_for_parametrize",
          "type",
          "sections",
          "__version__",
          "locals",
          "ensure_reset_dir",
          "visit_Assert",
          "stepwise",
          "_markers",
          "record_xml_attribute",
          "getfslineno",
          "_make_plural",
          "co_equal",
          "python",
          "version",
          "pytest_leave_pdb",
          "funcnamefilter",
          "_create_formatter",
          "_F",
          "_preparse",
          "AND",
          "_parse_ini_config",
          "idmaker",
          "FDCaptureBinary",
          "_FixtureCachedResult",
          "funcargnames",
          "cacheprovider",
          "cache_dir_from_config",
          "_non_numeric_type_error",
          "Xfail",
          "with_args",
          "_get_active_fixturedef",
          "yield_fixture",
          "ascii_escaped",
          "pop_format_context",
          "builtin_plugins",
          "re_match_lines",
          "resolve_package_path",
          "write_ensure_prefix",
          "setitem",
          "skipped",
          "_eval_scope_callable",
          "listextrakeywords",
          "SysPathsSnapshot",
          "PytestCacheWarning",
          "addSkip",
          "get_parametrized_fixture_keys",
          "duration",
          "ApproxBase",
          "_get_stderr_fileno",
          "_py_ext_re",
          "rex_outcome",
          "importorskip",
          "TempPathFactory",
          "pytest_ignore_collect",
          "WarningsChecker",
          "from_exc_info",
          "_pop_and_teardown",
          "isdict",
          "teardown_exact",
          "PYC_TAIL",
          "getoption",
          "PdbTrace",
          "add_funcarg_pseudo_fixture_def",
          "RE_IMPORT_ERROR_NAME",
          "SetupState",
          "KNOWN_TYPES",
          "fillfixtures",
          "markname",
          "append_error",
          "Session",
          "read",
          "maybe_wrap_pytest_function_for_tracing",
          "_state",
          "_in_suspended",
          "_PLUGGY_DIR",
          "_tmppath_factory",
          "getparam",
          "prepare",
          "maybe_delete_a_numbered_dir",
          "LFPlugin",
          "DoctestModule",
          "from_call",
          "NodeKeywords",
          "finalize",
          "evaluate_condition",
          "_prunetraceback",
          "addinivalue_line",
          "markup",
          "faulthandler",
          "PytestCollectionWarning",
          "append_collect_error",
          "compat",
          "parse",
          "MultiCapture",
          "runpytest_subprocess",
          "pytest_report_to_serializable",
          "runtestprotocol",
          "logging",
          "_suspend",
          "getvalueorskip",
          "pytest_configure",
          "OutcomeException",
          "_inject_setup_class_fixture",
          "excinfo",
          "AssertionRewritingHook",
          "_prepare_content",
          "_Key",
          "runitem",
          "_R",
          "clear_cache",
          "PyCollector",
          "record_testreport",
          "_diff_text",
          "_mark_plugins_for_rewrite",
          "consider_module",
          "_REPORTCHARS_DEFAULT",
          "XFailed",
          "LogCaptureFixture",
          "runtest",
          "rewrite_asserts",
          "create_module",
          "FILE_OR_DIR",
          "module",
          "_get_fixturestack",
          "pytest_sessionstart",
          "_show_fixture_action",
          "dir",
          "show_test_item",
          "record_property",
          "tmpdir_factory",
          "CallInfo",
          "get_optionflags",
          "_check_type",
          "make_numbered_dir_with_cleanup",
          "call_and_report",
          "nofuncargs",
          "Config",
          "reason",
          "_runtest_for",
          "get_lines_after",
          "version_tuple",
          "DEFAULT_MAX_CHARS",
          "Module",
          "Item",
          "_idvalset",
          "MatcherAdapter",
          "pytest_addhooks",
          "catch_log_records_key",
          "no_fnmatch_line",
          "__all__",
          "xml_key",
          "matchnodes",
          "read_global_capture",
          "_color_for_type",
          "Scanner",
          "_getini",
          "istestfunction",
          "_LiveLoggingNullHandler",
          "ParseError",
          "warning_record_to_str",
          "DEFAULT_LOG_DATE_FORMAT",
          "_compare_eq_cls",
          "_write_content",
          "assertion",
          "_PytestWrapper",
          "Metafunc",
          "_main",
          "_teardown_with_finalization",
          "_tryconvertpyarg",
          "resultlog_key",
          "get_real_method",
          "BASE_TYPE",
          "prog",
          "nodes",
          "reprcrash",
          "_resolve_arg_ids",
          "_assertion_supported",
          "getparent",
          "combined_with",
          "NoMatch",
          "_get_progress_information_message",
          "filter",
          "parse_setoption",
          "DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE",
          "_do_configure",
          "addoption",
          "getfixtureclosure",
          "TEST_OUTCOME",
          "ReprEntryNative",
          "_warn_or_fail_if_strict",
          "_striptext",
          "_call_with_optional_argument",
          "get_records",
          "getgroup",
          "_get_checker",
          "deselect_by_keyword",
          "_compare_eq_verbose",
          "capsysbinary",
          "capsys",
          "pop_outerr_to_orig",
          "category",
          "_set_initial_conftests",
          "_printcollecteditems",
          "Instance",
          "ReprTracebackNative",
          "pathlib",
          "nextitem",
          "parse_hookspec_opts",
          "formatrepr",
          "flush",
          "FixtureLookupErrorRepr",
          "_splitnode",
          "fail_fixturefunc",
          "visit_UnaryOp",
          "CaptureManager",
          "PytestExperimentalApiWarning",
          "getargs",
          "__enter__",
          "suspend_global_capture",
          "LogCaptureHandler",
          "__str__",
          "ischildnode",
          "_legal_ranges",
          "consider_conftest",
          "line",
          "write",
          "UNARY_MAP",
          "get_fslocation_from_item",
          "session",
          "store",
          "deselect_by_mark",
          "TESTS_FAILED",
          "spawn_pytest",
          "scopenum_function",
          "_WithException",
          "_matchnodes",
          "import_plugin",
          "TerminalWriter",
          "_FixtureValue",
          "istestclass",
          "_py36_windowsconsoleio_workaround",
          "__missing__",
          "hasinit",
          "visit_BoolOp",
          "post_mortem",
          "__iter__",
          "passed",
          "istestfunc",
          "ReprLocals",
          "getstatement",
          "_find_parametrized_scope",
          "_set_main_color",
          "builtin",
          "get_last_failed_paths",
          "_collectfile",
          "sep",
          "catching_logs",
          "addFailure",
          "setmulti2",
          "Skipped",
          "capture",
          "safe_isclass",
          "_typ_map",
          "tolerance"
        ]
      },
      "response": {
        "keywords": [
          "pytest_runtest_makereport",
          "skipping",
          "skip",
          "xfail",
          "evaluate_skip_marks",
          "evaluate_xfail_marks",
          "pytest",
          "get_location"
        ]
      }
    },
    {
      "name": "FilesKeywordSearch",
      "metadata": {
        "start": "2024-08-19T12:21:22.169098",
        "usage": "263.29517364501953 ms"
      },
      "request": {
        "tokens": [
          "pytest_runtest_makereport",
          "skipping",
          "skip",
          "xfail",
          "evaluate_skip_marks",
          "evaluate_xfail_marks",
          "pytest",
          "get_location"
        ]
      },
      "response": {
        "results": {
          "pytest": [
            "src/_pytest/faulthandler.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/capture.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/compat.py",
            "src/_pytest/skipping.py",
            "src/_pytest/pastebin.py",
            "src/pytest/__main__.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/monkeypatch.py",
            "src/_pytest/helpconfig.py",
            "src/pytest/__init__.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/terminal.py",
            "src/_pytest/unittest.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/config/argparsing.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/debugging.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/warnings.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/deprecated.py",
            "src/_pytest/freeze_support.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/main.py",
            "src/_pytest/reports.py",
            "src/_pytest/runner.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/timing.py",
            "src/_pytest/pytester.py",
            "src/pytest/collect.py",
            "src/_pytest/recwarn.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/logging.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/python_api.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/nodes.py"
          ],
          "skip": [
            "src/_pytest/doctest.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/helpconfig.py",
            "src/pytest/__init__.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/terminal.py",
            "src/_pytest/unittest.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/reports.py",
            "src/_pytest/pytester.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/hookspec.py"
          ],
          "xfail": [
            "src/_pytest/junitxml.py",
            "src/_pytest/mark/__init__.py",
            "src/pytest/__init__.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/unittest.py"
          ],
          "skipping": [
            "src/_pytest/stepwise.py",
            "src/_pytest/terminal.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/unittest.py"
          ],
          "pytest_runtest_makereport": [
            "src/_pytest/hookspec.py",
            "src/_pytest/skipping.py",
            "src/_pytest/runner.py",
            "src/_pytest/unittest.py"
          ],
          "get_location": [
            "src/_pytest/terminal.py"
          ],
          "evaluate_skip_marks": [
            "src/_pytest/skipping.py"
          ],
          "evaluate_xfail_marks": [
            "src/_pytest/skipping.py"
          ]
        }
      }
    },
    {
      "name": "FilesFilter",
      "metadata": {
        "start": "2024-08-19T12:21:22.432403",
        "usage": "6773.617267608643 ms"
      },
      "request": {
        "keyword_search_results": {
          "pytest": [
            "src/_pytest/faulthandler.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/capture.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/compat.py",
            "src/_pytest/skipping.py",
            "src/_pytest/pastebin.py",
            "src/pytest/__main__.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/monkeypatch.py",
            "src/_pytest/helpconfig.py",
            "src/pytest/__init__.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/terminal.py",
            "src/_pytest/unittest.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/config/argparsing.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/debugging.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/warnings.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/deprecated.py",
            "src/_pytest/freeze_support.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/main.py",
            "src/_pytest/reports.py",
            "src/_pytest/runner.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/timing.py",
            "src/_pytest/pytester.py",
            "src/pytest/collect.py",
            "src/_pytest/recwarn.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/logging.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/python_api.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/nodes.py"
          ],
          "skip": [
            "src/_pytest/doctest.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/helpconfig.py",
            "src/pytest/__init__.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/terminal.py",
            "src/_pytest/unittest.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/reports.py",
            "src/_pytest/pytester.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/hookspec.py"
          ],
          "xfail": [
            "src/_pytest/junitxml.py",
            "src/_pytest/mark/__init__.py",
            "src/pytest/__init__.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/unittest.py"
          ],
          "skipping": [
            "src/_pytest/stepwise.py",
            "src/_pytest/terminal.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/unittest.py"
          ],
          "pytest_runtest_makereport": [
            "src/_pytest/hookspec.py",
            "src/_pytest/skipping.py",
            "src/_pytest/runner.py",
            "src/_pytest/unittest.py"
          ],
          "get_location": [
            "src/_pytest/terminal.py"
          ],
          "evaluate_skip_marks": [
            "src/_pytest/skipping.py"
          ],
          "evaluate_xfail_marks": [
            "src/_pytest/skipping.py"
          ]
        }
      },
      "response": {
        "scratch_pad": "The issue at hand is related to the `--runxfail` option breaking the expected skip location reporting in pytest. The problem appears to be within the `src/_pytest/skipping.py` file, specifically involving the `pytest_runtest_makereport` hook. Let's break down the relevant keyword search results:\n\n1. **`pytest` keyword**: Widely used across the codebase, but `src/_pytest/skipping.py` is one of the relevant files as it directly relates to the skipping functionality.\n\n2. **`skip` keyword**: The file `src/_pytest/skipping.py` appears again and is highly relevant since the issue involves the skipping mechanism and its reporting.\n\n3. **`xfail` keyword**: `--runxfail` affects the xfail behavior, but unexpectedly impacts skipping. `src/_pytest/skipping.py` shows up here as well, suggesting it may be handling both behaviors incorrectly in tandem.\n\n4. **`skipping` keyword**: Indicates another connection to `src/_pytest/skipping.py`.\n\n5. **`pytest_runtest_makereport` keyword**: Directly mentioned in the issue, and found in `src/_pytest/skipping.py`. This confirms the hint provided in the issue report.\n\n6. **`evaluate_skip_marks` and `evaluate_xfail_marks`**: These functions are found in `src/_pytest/skipping.py`, which further indicates this file's role in handling both skip and xfail logic.\n\nConsidering all these, `src/_pytest/skipping.py` comprehensively covers all aspects of the reported issue, including skipping, xfail, and the related reporting hooks.",
        "file_paths": [
          "src/_pytest/skipping.py"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T12:21:29.206047",
        "usage": "9557.115077972412 ms"
      },
      "request": {
        "file_path": "src/_pytest/skipping.py"
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 The issue described is that when using `@pytest.mark.skip`/`skipif` marks to skip a test, the skip location reported should point to the item itself when running with `pytest -rs`. However, when running with `pytest -rs --runxfail`, the reported skip location points to a line in `src/_pytest/skipping.py`. This is incorrect behavior as `--runxfail` should only affect `xfail` and not interfere with other markers like `skip`.\n1.4 The expectation is that the skip location always points to the test that is being skipped, regardless of additional options like `--runxfail`. The issue might be caused by improper handling of `runxfail` in the `pytest_runtest_makereport` hook.\n\n2. Make file evaluation:\n2.1 The file `src/_pytest/skipping.py` contains the implementation of pytest skip and xfail functionalities.\n2.4 Upon analyzing the file, the `pytest_runtest_makereport` function is the hook where test reports are modified. There is a block of code within this hook that corrects the location reporting for tests skipped by `mark.skipif` or `mark.skip` by modifying `rep.longrepr`. This block seems to be intended to override the default report location to point to the test item line rather than where the skip was triggered internally in pytest.\n2.7 It is likely the issue arises from the check when both `skipped_by_mark_key` and `rep.skipped` are true, ensuring that `type(rep.longrepr)` is a tuple. The modification may be missing an additional check to ensure the condition applies when `--runxfail` is in effect, possibly affecting the `rep.outcome` or `rep.longrepr` values that are set even if `runxfail` is true.\n\n3. Output: The file is relevant because the issue lies in correcting the test report for skipped tests within `pytest_runtest_makereport`. This file is not the direct root cause since the issue is not in configuration or condition evaluation but rather in presentation of the report under specific circumstances.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [],
        "relevant_symbols": [
          "pytest_runtest_makereport",
          "evaluate_skip_marks",
          "skipped_by_mark_key",
          "pytest_addoption"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileSymbolSearch",
      "metadata": {
        "start": "2024-08-19T12:21:38.763216",
        "usage": "100.55232048034668 ms"
      },
      "request": {
        "files": [
          {
            "file_path": "src/_pytest/skipping.py",
            "relevant_symbol_keys": [
              "pytest_runtest_makereport",
              "evaluate_skip_marks",
              "skipped_by_mark_key",
              "pytest_addoption"
            ],
            "root_cause_symbol_keys": []
          }
        ],
        "with_content": true
      },
      "response": {
        "related_symbols": [
          {
            "name": "_pytest.assertion.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(l",
            "start_line": 23,
            "start_column": 602,
            "end_line": 0,
            "end_column": 1394,
            "start_byte": 602,
            "end_byte": 0
          },
          {
            "name": "_pytest.cacheprovider.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 398,
            "start_column": 14887,
            "end_line": 0,
            "end_column": 16670,
            "start_byte": 14887,
            "end_byte": 0
          },
          {
            "name": "_pytest.capture.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "rom typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        d",
            "start_line": 31,
            "start_column": 690,
            "end_line": 0,
            "end_column": 1171,
            "start_byte": 690,
            "end_byte": 0
          },
          {
            "name": "_pytest.debugging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips th",
            "start_line": 37,
            "start_column": 1099,
            "end_line": 0,
            "end_column": 1813,
            "start_byte": 1099,
            "end_byte": 0
          },
          {
            "name": "_pytest.doctest.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "le: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reaso",
            "start_line": 63,
            "start_column": 1753,
            "end_line": 0,
            "end_column": 3187,
            "start_byte": 1753,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fig\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _py",
            "start_line": 16,
            "start_column": 296,
            "end_line": 0,
            "end_column": 529,
            "start_byte": 296,
            "end_byte": 0
          },
          {
            "name": "_pytest.fixtures.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 1402,
            "start_column": 53994,
            "end_line": 0,
            "end_column": 54198,
            "start_byte": 53994,
            "end_byte": 0
          },
          {
            "name": "_pytest.helpconfig.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "l\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/refere",
            "start_line": 44,
            "start_column": 1360,
            "end_line": 0,
            "end_column": 2813,
            "start_byte": 1360,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "f you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n         ",
            "start_line": 80,
            "start_column": 2535,
            "end_line": 0,
            "end_column": 4163,
            "start_byte": 2535,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 487,
            "start_column": 17415,
            "end_line": 0,
            "end_column": 17902,
            "start_byte": 17415,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 389,
            "start_column": 13676,
            "end_line": 0,
            "end_column": 15031,
            "start_byte": 13676,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n\n\n# Whether skipped due to skip or skipif marks.\nskipped_by_mark_key = StoreKey[bool]()\n# Saves the xfail mark evaluation. Can be refreshed during call if None.\nxfailed_key = StoreKey[Optional[Xfail]]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if x",
            "start_line": 196,
            "start_column": 7063,
            "end_line": 0,
            "end_column": 9693,
            "start_byte": 7063,
            "end_byte": 0
          },
          {
            "name": "_pytest.main.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for ",
            "start_line": 47,
            "start_column": 1209,
            "end_line": 0,
            "end_column": 5788,
            "start_byte": 1209,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "itions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _py",
            "start_line": 63,
            "start_column": 1994,
            "end_line": 0,
            "end_column": 3613,
            "start_byte": 1994,
            "end_byte": 0
          },
          {
            "name": "_pytest.pastebin.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "est.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = pa",
            "start_line": 17,
            "start_column": 431,
            "end_line": 0,
            "end_column": 775,
            "start_byte": 431,
            "end_byte": 0
          },
          {
            "name": "_pytest.pytester.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "rgs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if",
            "start_line": 57,
            "start_column": 1461,
            "end_line": 0,
            "end_column": 2088,
            "start_byte": 1461,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n  ",
            "start_line": 74,
            "start_column": 2392,
            "end_line": 0,
            "end_column": 3831,
            "start_byte": 2392,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "n not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def no",
            "start_line": 44,
            "start_column": 1120,
            "end_line": 0,
            "end_column": 1447,
            "start_byte": 1120,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "grepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1",
            "start_line": 311,
            "start_column": 10294,
            "end_line": 0,
            "end_column": 10425,
            "start_byte": 10294,
            "end_byte": 0
          },
          {
            "name": "_pytest.setuponly.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "from _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser)",
            "start_line": 13,
            "start_column": 336,
            "end_line": 0,
            "end_column": 751,
            "start_byte": 336,
            "end_byte": 0
          },
          {
            "name": "_pytest.setupplan.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfr",
            "start_line": 11,
            "start_column": 265,
            "end_line": 0,
            "end_column": 558,
            "start_byte": 265,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
            "start_line": 29,
            "start_column": 715,
            "end_line": 0,
            "end_column": 1211,
            "start_byte": 715,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        if \"reason\" in mark.kwargs:\n            reason = mark.kwargs[\"reason\"]\n        elif mark.args:\n            reason = mark.args[0]\n        else:\n            reason = \"unconditional skip\"\n        return Skip(reason)\n\n    return None",
            "start_line": 158,
            "start_column": 5265,
            "end_line": 0,
            "end_column": 6229,
            "start_byte": 5265,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.skipped_by_mark_key",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skipped_by_mark_key = StoreKey[bool]()",
            "start_line": 225,
            "start_column": 7443,
            "end_line": 0,
            "end_column": 7481,
            "start_byte": 7443,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 259,
            "start_column": 8475,
            "end_line": 0,
            "end_column": 10433,
            "start_byte": 8475,
            "end_byte": 0
          },
          {
            "name": "_pytest.stepwise.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "mport TYPE_CHECKING\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest",
            "start_line": 11,
            "start_column": 246,
            "end_line": 0,
            "end_column": 725,
            "start_byte": 246,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "r ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip",
            "start_line": 109,
            "start_column": 2711,
            "end_line": 0,
            "end_column": 5988,
            "start_byte": 2711,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10761,
            "end_line": 0,
            "end_column": 11550,
            "start_byte": 10761,
            "end_byte": 0
          },
          {
            "name": "_pytest.warnings.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "kip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue",
            "start_line": 57,
            "start_column": 1758,
            "end_line": 0,
            "end_column": 2232,
            "start_byte": 1758,
            "end_byte": 0
          },
          {
            "name": "_pytest.resultlog.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "est.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pyt",
            "start_line": 18,
            "start_column": 390,
            "end_line": 0,
            "end_column": 722,
            "start_byte": 390,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": []
      }
    },
    {
      "name": "SymbolsEvaluate",
      "metadata": {
        "start": "2024-08-19T12:21:38.863774",
        "usage": "13310.415983200073 ms"
      },
      "request": {
        "related_symbols": [
          {
            "name": "_pytest.assertion.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(l",
            "start_line": 23,
            "start_column": 602,
            "end_line": 0,
            "end_column": 1394,
            "start_byte": 602,
            "end_byte": 0
          },
          {
            "name": "_pytest.cacheprovider.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 398,
            "start_column": 14887,
            "end_line": 0,
            "end_column": 16670,
            "start_byte": 14887,
            "end_byte": 0
          },
          {
            "name": "_pytest.capture.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "rom typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        d",
            "start_line": 31,
            "start_column": 690,
            "end_line": 0,
            "end_column": 1171,
            "start_byte": 690,
            "end_byte": 0
          },
          {
            "name": "_pytest.debugging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips th",
            "start_line": 37,
            "start_column": 1099,
            "end_line": 0,
            "end_column": 1813,
            "start_byte": 1099,
            "end_byte": 0
          },
          {
            "name": "_pytest.doctest.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "le: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reaso",
            "start_line": 63,
            "start_column": 1753,
            "end_line": 0,
            "end_column": 3187,
            "start_byte": 1753,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fig\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _py",
            "start_line": 16,
            "start_column": 296,
            "end_line": 0,
            "end_column": 529,
            "start_byte": 296,
            "end_byte": 0
          },
          {
            "name": "_pytest.fixtures.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 1402,
            "start_column": 53994,
            "end_line": 0,
            "end_column": 54198,
            "start_byte": 53994,
            "end_byte": 0
          },
          {
            "name": "_pytest.helpconfig.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "l\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/refere",
            "start_line": 44,
            "start_column": 1360,
            "end_line": 0,
            "end_column": 2813,
            "start_byte": 1360,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "f you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n         ",
            "start_line": 80,
            "start_column": 2535,
            "end_line": 0,
            "end_column": 4163,
            "start_byte": 2535,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 487,
            "start_column": 17415,
            "end_line": 0,
            "end_column": 17902,
            "start_byte": 17415,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 389,
            "start_column": 13676,
            "end_line": 0,
            "end_column": 15031,
            "start_byte": 13676,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n\n\n# Whether skipped due to skip or skipif marks.\nskipped_by_mark_key = StoreKey[bool]()\n# Saves the xfail mark evaluation. Can be refreshed during call if None.\nxfailed_key = StoreKey[Optional[Xfail]]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if x",
            "start_line": 196,
            "start_column": 7063,
            "end_line": 0,
            "end_column": 9693,
            "start_byte": 7063,
            "end_byte": 0
          },
          {
            "name": "_pytest.main.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for ",
            "start_line": 47,
            "start_column": 1209,
            "end_line": 0,
            "end_column": 5788,
            "start_byte": 1209,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "itions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _py",
            "start_line": 63,
            "start_column": 1994,
            "end_line": 0,
            "end_column": 3613,
            "start_byte": 1994,
            "end_byte": 0
          },
          {
            "name": "_pytest.pastebin.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "est.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = pa",
            "start_line": 17,
            "start_column": 431,
            "end_line": 0,
            "end_column": 775,
            "start_byte": 431,
            "end_byte": 0
          },
          {
            "name": "_pytest.pytester.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "rgs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if",
            "start_line": 57,
            "start_column": 1461,
            "end_line": 0,
            "end_column": 2088,
            "start_byte": 1461,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n  ",
            "start_line": 74,
            "start_column": 2392,
            "end_line": 0,
            "end_column": 3831,
            "start_byte": 2392,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "n not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def no",
            "start_line": 44,
            "start_column": 1120,
            "end_line": 0,
            "end_column": 1447,
            "start_byte": 1120,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "grepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1",
            "start_line": 311,
            "start_column": 10294,
            "end_line": 0,
            "end_column": 10425,
            "start_byte": 10294,
            "end_byte": 0
          },
          {
            "name": "_pytest.setuponly.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "from _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser)",
            "start_line": 13,
            "start_column": 336,
            "end_line": 0,
            "end_column": 751,
            "start_byte": 336,
            "end_byte": 0
          },
          {
            "name": "_pytest.setupplan.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfr",
            "start_line": 11,
            "start_column": 265,
            "end_line": 0,
            "end_column": 558,
            "start_byte": 265,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
            "start_line": 29,
            "start_column": 715,
            "end_line": 0,
            "end_column": 1211,
            "start_byte": 715,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        if \"reason\" in mark.kwargs:\n            reason = mark.kwargs[\"reason\"]\n        elif mark.args:\n            reason = mark.args[0]\n        else:\n            reason = \"unconditional skip\"\n        return Skip(reason)\n\n    return None",
            "start_line": 158,
            "start_column": 5265,
            "end_line": 0,
            "end_column": 6229,
            "start_byte": 5265,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.skipped_by_mark_key",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skipped_by_mark_key = StoreKey[bool]()",
            "start_line": 225,
            "start_column": 7443,
            "end_line": 0,
            "end_column": 7481,
            "start_byte": 7443,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 259,
            "start_column": 8475,
            "end_line": 0,
            "end_column": 10433,
            "start_byte": 8475,
            "end_byte": 0
          },
          {
            "name": "_pytest.stepwise.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "mport TYPE_CHECKING\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest",
            "start_line": 11,
            "start_column": 246,
            "end_line": 0,
            "end_column": 725,
            "start_byte": 246,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "r ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip",
            "start_line": 109,
            "start_column": 2711,
            "end_line": 0,
            "end_column": 5988,
            "start_byte": 2711,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10761,
            "end_line": 0,
            "end_column": 11550,
            "start_byte": 10761,
            "end_byte": 0
          },
          {
            "name": "_pytest.warnings.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "kip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue",
            "start_line": 57,
            "start_column": 1758,
            "end_line": 0,
            "end_column": 2232,
            "start_byte": 1758,
            "end_byte": 0
          },
          {
            "name": "_pytest.resultlog.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "est.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pyt",
            "start_line": 18,
            "start_column": 390,
            "end_line": 0,
            "end_column": 722,
            "start_byte": 390,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": []
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 The issue is that when using `--runxfail` in combination with `@pytest.mark.skip`, the location reported for a skipped test changes from the test item to `_pytest/skipping.py`.\n1.2 This issue is likely caused by the incorrect location being set in the pytest report when `--runxfail` is used.\n1.3 The expected result should show the location of the skipped test as the test item itself, not `_pytest/skipping.py`.\n1.4 If the user did not provide an expected result:\n    The expectation would be that the location always points to the test item regardless of the `--runxfail` option being set.\n1.5 To fix this:\n   - The code responsible for setting the skip location in the pytest report should be adjusted to ensure that `--runxfail` does not alter it.\n\n2. Make the evaluate:\n2.1 Thoroughly analyze each symbol\n2.2 Key elements of interest are related to the `pytest_runtest_makereport` hook in `src/_pytest/skipping.py` since the hint provided in the issue statement points here.\n2.5 I will focus mainly on `pytest_runtest_makereport` to identify if there's more information or changes needed in that specific implementation  regarding the location reporting.\n2.6 Evaluating which symbols relate to the issue:\n   - `_pytest.skipping.pytest_runtest_makereport` is highly relevant because it is pointed out in the issue as where the bug is likely present.\n2.4 Consult all available `pytest_runtest_makereport` implementations to ensure no oversight.\n   - Request more details on `_pytest.skipping.pytest_runtest_makereport` if necessary to understand the logic that affects skip reporting.",
        "symbols": [
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "reason": "The code location needs adjustment because it modifies the longrepr to point to a different file when `--runxfail` is used. It currently incorrectly changes the report location to _pytest/skipping.py instead of keeping the location of the skipped test as the test item."
          }
        ],
        "more_info": [
          {
            "type": "symbol",
            "path": "src/_pytest/skipping.py",
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "reason": "Ensure that the modification of rep.longrepr takes into account that when a test is skipped due to skip marks, the location should not be distorted by `--runxfail`. Verify all decision branches and whether other logic interplays erroneously."
          }
        ],
        "is_done": false,
        "symbols_scratch_pad": [
          "The symbol `_pytest.skipping.pytest_runtest_makereport` likely handles part of the logic making skippings related to the hook implementation. It contains logic modifying the `rep.longrepr`, notably looking at whether `skipped_by_mark_key` is set and adjusting the location then. This comes after logic dealing with xfail and skipped outcomes, suggesting place where location is incorrectly set.",
          "Other `pytest_runtest_makereport` instances or related symbols could interplay, but the main code already seems to indicate handling the skip mark associated rationale explicitly."
        ]
      }
    },
    {
      "name": "SymbolSummary",
      "metadata": {
        "start": "2024-08-19T12:21:52.174237",
        "usage": "2.9821395874023438 ms"
      },
      "request": {
        "symbols": {
          "related_symbols": [
            {
              "name": "_pytest.assertion.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(l",
              "start_line": 23,
              "start_column": 602,
              "end_line": 0,
              "end_column": 1394,
              "start_byte": 602,
              "end_byte": 0
            },
            {
              "name": "_pytest.cacheprovider.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 398,
              "start_column": 14887,
              "end_line": 0,
              "end_column": 16670,
              "start_byte": 14887,
              "end_byte": 0
            },
            {
              "name": "_pytest.capture.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "rom typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        d",
              "start_line": 31,
              "start_column": 690,
              "end_line": 0,
              "end_column": 1171,
              "start_byte": 690,
              "end_byte": 0
            },
            {
              "name": "_pytest.debugging.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips th",
              "start_line": 37,
              "start_column": 1099,
              "end_line": 0,
              "end_column": 1813,
              "start_byte": 1099,
              "end_byte": 0
            },
            {
              "name": "_pytest.doctest.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "le: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reaso",
              "start_line": 63,
              "start_column": 1753,
              "end_line": 0,
              "end_column": 3187,
              "start_byte": 1753,
              "end_byte": 0
            },
            {
              "name": "_pytest.faulthandler.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "fig\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _py",
              "start_line": 16,
              "start_column": 296,
              "end_line": 0,
              "end_column": 529,
              "start_byte": 296,
              "end_byte": 0
            },
            {
              "name": "_pytest.fixtures.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 1402,
              "start_column": 53994,
              "end_line": 0,
              "end_column": 54198,
              "start_byte": 53994,
              "end_byte": 0
            },
            {
              "name": "_pytest.helpconfig.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "l\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/refere",
              "start_line": 44,
              "start_column": 1360,
              "end_line": 0,
              "end_column": 2813,
              "start_byte": 1360,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "f you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n         ",
              "start_line": 80,
              "start_column": 2535,
              "end_line": 0,
              "end_column": 4163,
              "start_byte": 2535,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 487,
              "start_column": 17415,
              "end_line": 0,
              "end_column": 17902,
              "start_byte": 17415,
              "end_byte": 0
            },
            {
              "name": "_pytest.junitxml.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 389,
              "start_column": 13676,
              "end_line": 0,
              "end_column": 15031,
              "start_byte": 13676,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n\n\n# Whether skipped due to skip or skipif marks.\nskipped_by_mark_key = StoreKey[bool]()\n# Saves the xfail mark evaluation. Can be refreshed during call if None.\nxfailed_key = StoreKey[Optional[Xfail]]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if x",
              "start_line": 196,
              "start_column": 7063,
              "end_line": 0,
              "end_column": 9693,
              "start_byte": 7063,
              "end_byte": 0
            },
            {
              "name": "_pytest.main.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for ",
              "start_line": 47,
              "start_column": 1209,
              "end_line": 0,
              "end_column": 5788,
              "start_byte": 1209,
              "end_byte": 0
            },
            {
              "name": "_pytest.mark.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "itions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _py",
              "start_line": 63,
              "start_column": 1994,
              "end_line": 0,
              "end_column": 3613,
              "start_byte": 1994,
              "end_byte": 0
            },
            {
              "name": "_pytest.pastebin.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "est.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = pa",
              "start_line": 17,
              "start_column": 431,
              "end_line": 0,
              "end_column": 775,
              "start_byte": 431,
              "end_byte": 0
            },
            {
              "name": "_pytest.pytester.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "rgs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if",
              "start_line": 57,
              "start_column": 1461,
              "end_line": 0,
              "end_column": 2088,
              "start_byte": 1461,
              "end_byte": 0
            },
            {
              "name": "_pytest.python.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n  ",
              "start_line": 74,
              "start_column": 2392,
              "end_line": 0,
              "end_column": 3831,
              "start_byte": 2392,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "n not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def no",
              "start_line": 44,
              "start_column": 1120,
              "end_line": 0,
              "end_column": 1447,
              "start_byte": 1120,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "grepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1",
              "start_line": 311,
              "start_column": 10294,
              "end_line": 0,
              "end_column": 10425,
              "start_byte": 10294,
              "end_byte": 0
            },
            {
              "name": "_pytest.setuponly.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "from _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser)",
              "start_line": 13,
              "start_column": 336,
              "end_line": 0,
              "end_column": 751,
              "start_byte": 336,
              "end_byte": 0
            },
            {
              "name": "_pytest.setupplan.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfr",
              "start_line": 11,
              "start_column": 265,
              "end_line": 0,
              "end_column": 558,
              "start_byte": 265,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
              "start_line": 29,
              "start_column": 715,
              "end_line": 0,
              "end_column": 1211,
              "start_byte": 715,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.evaluate_skip_marks",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        if \"reason\" in mark.kwargs:\n            reason = mark.kwargs[\"reason\"]\n        elif mark.args:\n            reason = mark.args[0]\n        else:\n            reason = \"unconditional skip\"\n        return Skip(reason)\n\n    return None",
              "start_line": 158,
              "start_column": 5265,
              "end_line": 0,
              "end_column": 6229,
              "start_byte": 5265,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.skipped_by_mark_key",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "skipped_by_mark_key = StoreKey[bool]()",
              "start_line": 225,
              "start_column": 7443,
              "end_line": 0,
              "end_column": 7481,
              "start_byte": 7443,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
              "start_line": 259,
              "start_column": 8475,
              "end_line": 0,
              "end_column": 10433,
              "start_byte": 8475,
              "end_byte": 0
            },
            {
              "name": "_pytest.stepwise.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "mport TYPE_CHECKING\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest",
              "start_line": 11,
              "start_column": 246,
              "end_line": 0,
              "end_column": 725,
              "start_byte": 246,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "r ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip",
              "start_line": 109,
              "start_column": 2711,
              "end_line": 0,
              "end_column": 5988,
              "start_byte": 2711,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 290,
              "start_column": 10761,
              "end_line": 0,
              "end_column": 11550,
              "start_byte": 10761,
              "end_byte": 0
            },
            {
              "name": "_pytest.warnings.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "kip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue",
              "start_line": 57,
              "start_column": 1758,
              "end_line": 0,
              "end_column": 2232,
              "start_byte": 1758,
              "end_byte": 0
            },
            {
              "name": "_pytest.resultlog.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "est.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pyt",
              "start_line": 18,
              "start_column": 390,
              "end_line": 0,
              "end_column": 722,
              "start_byte": 390,
              "end_byte": 0
            }
          ],
          "root_caused_symbols": []
        },
        "symbols_evaluate": {
          "scratch_pad": "1. Analyze User Instructions:\n1.1 The issue is that when using `--runxfail` in combination with `@pytest.mark.skip`, the location reported for a skipped test changes from the test item to `_pytest/skipping.py`.\n1.2 This issue is likely caused by the incorrect location being set in the pytest report when `--runxfail` is used.\n1.3 The expected result should show the location of the skipped test as the test item itself, not `_pytest/skipping.py`.\n1.4 If the user did not provide an expected result:\n    The expectation would be that the location always points to the test item regardless of the `--runxfail` option being set.\n1.5 To fix this:\n   - The code responsible for setting the skip location in the pytest report should be adjusted to ensure that `--runxfail` does not alter it.\n\n2. Make the evaluate:\n2.1 Thoroughly analyze each symbol\n2.2 Key elements of interest are related to the `pytest_runtest_makereport` hook in `src/_pytest/skipping.py` since the hint provided in the issue statement points here.\n2.5 I will focus mainly on `pytest_runtest_makereport` to identify if there's more information or changes needed in that specific implementation  regarding the location reporting.\n2.6 Evaluating which symbols relate to the issue:\n   - `_pytest.skipping.pytest_runtest_makereport` is highly relevant because it is pointed out in the issue as where the bug is likely present.\n2.4 Consult all available `pytest_runtest_makereport` implementations to ensure no oversight.\n   - Request more details on `_pytest.skipping.pytest_runtest_makereport` if necessary to understand the logic that affects skip reporting.",
          "symbols": [
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "reason": "The code location needs adjustment because it modifies the longrepr to point to a different file when `--runxfail` is used. It currently incorrectly changes the report location to _pytest/skipping.py instead of keeping the location of the skipped test as the test item."
            }
          ],
          "more_info": [
            {
              "type": "symbol",
              "path": "src/_pytest/skipping.py",
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "reason": "Ensure that the modification of rep.longrepr takes into account that when a test is skipped due to skip marks, the location should not be distorted by `--runxfail`. Verify all decision branches and whether other logic interplays erroneously."
            }
          ],
          "is_done": false,
          "symbols_scratch_pad": [
            "The symbol `_pytest.skipping.pytest_runtest_makereport` likely handles part of the logic making skippings related to the hook implementation. It contains logic modifying the `rep.longrepr`, notably looking at whether `skipped_by_mark_key` is set and adjusting the location then. This comes after logic dealing with xfail and skipped outcomes, suggesting place where location is incorrectly set.",
            "Other `pytest_runtest_makereport` instances or related symbols could interplay, but the main code already seems to indicate handling the skip mark associated rationale explicitly."
          ]
        }
      },
      "response": {
        "file_paths": [
          "src/_pytest/skipping.py"
        ],
        "symbols": [
          "_pytest.skipping.pytest_runtest_makereport"
        ],
        "code_spans": {
          "_pytest.assertion.pytest_addoption": {
            "name": "_pytest.assertion.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(l",
            "start_line": 23,
            "start_column": 602,
            "end_line": 0,
            "end_column": 1394,
            "start_byte": 602,
            "end_byte": 0
          },
          "_pytest.cacheprovider.pytest_addoption": {
            "name": "_pytest.cacheprovider.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 398,
            "start_column": 14887,
            "end_line": 0,
            "end_column": 16670,
            "start_byte": 14887,
            "end_byte": 0
          },
          "_pytest.capture.pytest_addoption": {
            "name": "_pytest.capture.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "rom typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        d",
            "start_line": 31,
            "start_column": 690,
            "end_line": 0,
            "end_column": 1171,
            "start_byte": 690,
            "end_byte": 0
          },
          "_pytest.debugging.pytest_addoption": {
            "name": "_pytest.debugging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips th",
            "start_line": 37,
            "start_column": 1099,
            "end_line": 0,
            "end_column": 1813,
            "start_byte": 1099,
            "end_byte": 0
          },
          "_pytest.doctest.pytest_addoption": {
            "name": "_pytest.doctest.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "le: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reaso",
            "start_line": 63,
            "start_column": 1753,
            "end_line": 0,
            "end_column": 3187,
            "start_byte": 1753,
            "end_byte": 0
          },
          "_pytest.faulthandler.pytest_addoption": {
            "name": "_pytest.faulthandler.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fig\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _py",
            "start_line": 16,
            "start_column": 296,
            "end_line": 0,
            "end_column": 529,
            "start_byte": 296,
            "end_byte": 0
          },
          "_pytest.fixtures.pytest_addoption": {
            "name": "_pytest.fixtures.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 1402,
            "start_column": 53994,
            "end_line": 0,
            "end_column": 54198,
            "start_byte": 53994,
            "end_byte": 0
          },
          "_pytest.helpconfig.pytest_addoption": {
            "name": "_pytest.helpconfig.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "l\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/refere",
            "start_line": 44,
            "start_column": 1360,
            "end_line": 0,
            "end_column": 2813,
            "start_byte": 1360,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_addoption": {
            "name": "_pytest.hookspec.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "f you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n         ",
            "start_line": 80,
            "start_column": 2535,
            "end_line": 0,
            "end_column": 4163,
            "start_byte": 2535,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_runtest_makereport": {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 487,
            "start_column": 17415,
            "end_line": 0,
            "end_column": 17902,
            "start_byte": 17415,
            "end_byte": 0
          },
          "_pytest.junitxml.pytest_addoption": {
            "name": "_pytest.junitxml.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 389,
            "start_column": 13676,
            "end_line": 0,
            "end_column": 15031,
            "start_byte": 13676,
            "end_byte": 0
          },
          "_pytest.logging.pytest_addoption": {
            "name": "_pytest.logging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n\n\n# Whether skipped due to skip or skipif marks.\nskipped_by_mark_key = StoreKey[bool]()\n# Saves the xfail mark evaluation. Can be refreshed during call if None.\nxfailed_key = StoreKey[Optional[Xfail]]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if x",
            "start_line": 196,
            "start_column": 7063,
            "end_line": 0,
            "end_column": 9693,
            "start_byte": 7063,
            "end_byte": 0
          },
          "_pytest.main.pytest_addoption": {
            "name": "_pytest.main.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for ",
            "start_line": 47,
            "start_column": 1209,
            "end_line": 0,
            "end_column": 5788,
            "start_byte": 1209,
            "end_byte": 0
          },
          "_pytest.mark.pytest_addoption": {
            "name": "_pytest.mark.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "itions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n        \"mark the test function as an expected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _py",
            "start_line": 63,
            "start_column": 1994,
            "end_line": 0,
            "end_column": 3613,
            "start_byte": 1994,
            "end_byte": 0
          },
          "_pytest.pastebin.pytest_addoption": {
            "name": "_pytest.pastebin.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "est.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = pa",
            "start_line": 17,
            "start_column": 431,
            "end_line": 0,
            "end_column": 775,
            "start_byte": 431,
            "end_byte": 0
          },
          "_pytest.pytester.pytest_addoption": {
            "name": "_pytest.pytester.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "rgs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if",
            "start_line": 57,
            "start_column": 1461,
            "end_line": 0,
            "end_column": 2088,
            "start_byte": 1461,
            "end_byte": 0
          },
          "_pytest.python.pytest_addoption": {
            "name": "_pytest.python.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ected failure if any of the conditions \"\n        \"evaluate to True. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n  ",
            "start_line": 74,
            "start_column": 2392,
            "end_line": 0,
            "end_column": 3831,
            "start_byte": 2392,
            "end_byte": 0
          },
          "_pytest.runner.pytest_addoption": {
            "name": "_pytest.runner.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "n not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def no",
            "start_line": 44,
            "start_column": 1120,
            "end_line": 0,
            "end_column": 1447,
            "start_byte": 1120,
            "end_byte": 0
          },
          "_pytest.runner.pytest_runtest_makereport": {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "grepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1",
            "start_line": 311,
            "start_column": 10294,
            "end_line": 0,
            "end_column": 10425,
            "start_byte": 10294,
            "end_byte": 0
          },
          "_pytest.setuponly.pytest_addoption": {
            "name": "_pytest.setuponly.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "from _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest_addoption(parser: Parser)",
            "start_line": 13,
            "start_column": 336,
            "end_line": 0,
            "end_column": 751,
            "start_byte": 336,
            "end_byte": 0
          },
          "_pytest.setupplan.pytest_addoption": {
            "name": "_pytest.setupplan.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfr",
            "start_line": 11,
            "start_column": 265,
            "end_line": 0,
            "end_column": 558,
            "start_byte": 265,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_addoption": {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
            "start_line": 29,
            "start_column": 715,
            "end_line": 0,
            "end_column": 1211,
            "start_byte": 715,
            "end_byte": 0
          },
          "_pytest.skipping.evaluate_skip_marks": {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        if \"reason\" in mark.kwargs:\n            reason = mark.kwargs[\"reason\"]\n        elif mark.args:\n            reason = mark.args[0]\n        else:\n            reason = \"unconditional skip\"\n        return Skip(reason)\n\n    return None",
            "start_line": 158,
            "start_column": 5265,
            "end_line": 0,
            "end_column": 6229,
            "start_byte": 5265,
            "end_byte": 0
          },
          "_pytest.skipping.skipped_by_mark_key": {
            "name": "_pytest.skipping.skipped_by_mark_key",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skipped_by_mark_key = StoreKey[bool]()",
            "start_line": 225,
            "start_column": 7443,
            "end_line": 0,
            "end_column": 7481,
            "start_byte": 7443,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 259,
            "start_column": 8475,
            "end_line": 0,
            "end_column": 10433,
            "start_byte": 8475,
            "end_byte": 0
          },
          "_pytest.stepwise.pytest_addoption": {
            "name": "_pytest.stepwise.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "mport TYPE_CHECKING\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pytest",
            "start_line": 11,
            "start_column": 246,
            "end_line": 0,
            "end_column": 725,
            "start_byte": 246,
            "end_byte": 0
          },
          "_pytest.terminal.pytest_addoption": {
            "name": "_pytest.terminal.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "r ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    condition is bool()'d. If this fails, an appropriately formatted pytest.fail\n    is raised.\n\n    Returns (result, reason). The reason is only relevant if the result is True.\n    \"\"\"\n    # String condition.\n    if isinstance(condition, str):\n        globals_ = {\n            \"os\": os,\n            \"sys\": sys,\n            \"platform\": platform,\n            \"config\": item.config,\n        }\n        if hasattr(item, \"obj\"):\n            globals_.update(item.obj.__globals__)  # type: ignore[attr-defined]\n        try:\n            condition_code = _pytest._code.compile(condition, mode=\"eval\")\n            result = eval(condition_code, globals_)\n        except SyntaxError as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                \"    \" + \" \" * (exc.offset or 0) + \"^\",\n                \"SyntaxError: invalid syntax\",\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition\" % mark.name,\n                \"    \" + condition,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    # Boolean condition.\n    else:\n        try:\n            result = bool(condition)\n        except Exception as exc:\n            msglines = [\n                \"Error evaluating %r condition as a boolean\" % mark.name,\n                *traceback.format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwargs.get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n    reason = attr.ib(type=str)\n\n\ndef evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip",
            "start_line": 109,
            "start_column": 2711,
            "end_line": 0,
            "end_column": 5988,
            "start_byte": 2711,
            "end_byte": 0
          },
          "_pytest.unittest.pytest_runtest_makereport": {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10761,
            "end_line": 0,
            "end_column": 11550,
            "start_byte": 10761,
            "end_byte": 0
          },
          "_pytest.warnings.pytest_addoption": {
            "name": "_pytest.warnings.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "kip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition, ..., *, reason=...): \"\n        \"skip the given test function if any of the conditions evaluate to True. \"\n        \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n        \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n    )\n    config.addinivalue",
            "start_line": 57,
            "start_column": 1758,
            "end_line": 0,
            "end_column": 2232,
            "start_byte": 1758,
            "end_byte": 0
          },
          "_pytest.resultlog.pytest_addoption": {
            "name": "_pytest.resultlog.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "est.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\nif TYPE_CHECKING:\n    from typing import Type\n\n\ndef pyt",
            "start_line": 18,
            "start_column": 390,
            "end_line": 0,
            "end_column": 722,
            "start_byte": 390,
            "end_byte": 0
          }
        }
      }
    }
  ]
}