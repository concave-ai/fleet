{
  "total_cost": 0,
  "task_id": "pytest-dev__pytest-7373",
  "model": "gpt-4o-2024-08-06",
  "spans": [
    {
      "name": "KeywordExtract",
      "metadata": {
        "start": "2024-08-19T12:18:24.656709",
        "usage": "1422.203779220581 ms"
      },
      "request": {
        "repo_tokens": [
          "pytest_collect_directory",
          "add_attribute",
          "_getobj",
          "delattr",
          "_assertion_pass",
          "disabled",
          "compile",
          "fullsource",
          "_ensure_relative_to_basetemp",
          "_teardown_yield_fixture",
          "store_mark",
          "Frame",
          "find_spec",
          "resolve_from_str",
          "MarkGenerator",
          "catch_warnings_for_item",
          "_folded_skips",
          "exitstatus",
          "MODULE_NOT_FOUND_ERROR",
          "reprtraceback",
          "_match_lines",
          "run",
          "not_expr",
          "DEFAULT_ABSOLUTE_TOLERANCE",
          "_should_truncate_item",
          "toterminal",
          "getfailedcollections",
          "Source",
          "STRING_TYPES",
          "_is_setup_py",
          "_visit_filter",
          "_getnextfixturedef",
          "append_skipped",
          "expression",
          "REPORT_COLLECTING_RESOLUTION",
          "runpython",
          "PYC_EXT",
          "_getlines",
          "mkpydir",
          "scopemismatch",
          "reset",
          "print_teardown_sections",
          "_E",
          "_inject_setup_function_fixture",
          "Expression",
          "tmp_path",
          "explanation_param",
          "CLOSE_STDIN",
          "get",
          "get_closest_marker",
          "FunctionDefinition",
          "PytestArg",
          "pytest",
          "FormattedExcinfo",
          "scope",
          "main",
          "_fillfixtures",
          "pytestconfig",
          "maketxtfile",
          "_getconftestmodules",
          "_find_spec",
          "cached_result",
          "ApproxDecimal",
          "_parse_parametrize_args",
          "safeformat",
          "check_interactive_exception",
          "recursionindex",
          "cleanup_numbered_dir",
          "append_failure",
          "_with_exception",
          "applymarker",
          "_check_all_skipped",
          "ApproxNumpy",
          "bin_xml_escape",
          "set_location",
          "exprinfo",
          "RESULT_LOG",
          "_dump_lines",
          "check_xfail_no_run",
          "_yield_comparisons",
          "re_match_lines_random",
          "_recursive_debug",
          "classnamefilter",
          "makepyfile",
          "FUNCARGNAMES",
          "INTERNAL_ERROR",
          "repr_instance",
          "record_tuples",
          "CollectReport",
          "_get_pdb_wrapper_class",
          "setuponly",
          "_import_plugin_specs",
          "pytest_runtest_teardown",
          "getstatementrange",
          "RunResult",
          "UnitTestCase",
          "write_log_entry",
          "DummyRewriteHook",
          "deprecated_call",
          "_write_pyc_fp",
          "get_terminal_writer",
          "_param_ids_from",
          "exec_",
          "visit_Call",
          "inline_genitems",
          "writeorg",
          "no_summary",
          "__eq__",
          "update_testcase_duration",
          "USAGE_MSG",
          "_init_checker_class",
          "collect_one_node",
          "_code",
          "_check_non_top_pytest_plugins",
          "append_pass",
          "_iter_rewritable_modules",
          "teardown",
          "setattr",
          "_determine_main_color",
          "DontReadFromInput",
          "_start",
          "push_format_context",
          "OR",
          "value",
          "consider_preparse",
          "unittest",
          "_param_ids_generated",
          "_PYTEST_DIR",
          "exconly",
          "add_stats",
          "ReprTraceback",
          "exec_module",
          "InvocationParams",
          "_colorama_workaround",
          "get_source",
          "set_level",
          "_pformat_dispatch",
          "consider_pluginarg",
          "FixtureDef",
          "_format_args",
          "add_property",
          "source",
          "NFPlugin",
          "pytest_deselected",
          "_format_assertmsg",
          "from_parent",
          "_exec_lsof",
          "__module__",
          "_set_opt_strings",
          "pytest_fixture_setup",
          "pytest_runtest_call",
          "FIXTURE_POSITIONAL_ARGUMENTS",
          "RaisesContext",
          "_excinfo",
          "get_direct_param_fixture_func",
          "_getcrashline",
          "AssertionRewriter",
          "LsofFdLeakChecker",
          "_recursive_list_map",
          "isattrs",
          "junitxml",
          "_mk_tmp",
          "importlib",
          "_ascii_escaped_by_config",
          "_pyfuncitem",
          "getcall",
          "write_fspath_result",
          "_matches_prefix_or_glob_option",
          "_trace",
          "HelpAction",
          "NODE_USE_FROM_PARENT",
          "pytest_assertion_pass",
          "LEVELNAME_FMT_REGEX",
          "_get_pos",
          "notset",
          "Cache",
          "stop_capturing",
          "get_location",
          "T",
          "V",
          "_log_cli_enabled",
          "on_rm_rf_error",
          "processoption",
          "_config",
          "LFPluginCollSkipfiles",
          "TestCaseFunction",
          "PYTESTER_COPY_EXAMPLE",
          "__contains__",
          "get_timeout_config_value",
          "DOCTEST_REPORT_CHOICES",
          "ParameterSet",
          "hasplugin",
          "capstdout",
          "iscoroutinefunction",
          "pytest_pycollect_makeitem",
          "_compare_eq_sequence",
          "PYTEST_COLLECT_MODULE",
          "location",
          "_report_kwargs_from_json",
          "genitems",
          "isinitpath",
          "stop",
          "_getfailureheadline",
          "getcrashentry",
          "CFG_PYTEST_SECTION",
          "DEFAULT_RELATIVE_TOLERANCE",
          "log_outcome",
          "pytest_runtest_makereport",
          "append_collect_skipped",
          "_FixtureFunc",
          "register",
          "get_default_arg_names",
          "cleanup_candidates",
          "EMPTY_PARAMETERSET_OPTION",
          "_check_record_param_type",
          "format_explanation",
          "makeconftest",
          "ParsedCall",
          "_finalize",
          "_addfinalizer",
          "getworkerinfoline",
          "pytest_fixture_post_finalizer",
          "_get_flag_lookup",
          "resume_fixture",
          "findsource",
          "doctest",
          "fileno",
          "_validate_usepdb_cls",
          "Class",
          "filter_traceback_for_conftest_import_failure",
          "_non_printable_ascii_translate_table",
          "cut",
          "_readline_workaround",
          "helpconfig",
          "getfirstlinesource",
          "DOCTEST_REPORT_CHOICE_NDIFF",
          "prepend",
          "reprentries",
          "old_mark_config_key",
          "create_cleanup_lock",
          "ensure_deletable",
          "rewrite",
          "scopename2class",
          "verbosity",
          "derive_importpath",
          "make_numbered_dir",
          "resolve",
          "_inject_setup_module_fixture",
          "entrysep",
          "DoctestTextfile",
          "RPAREN",
          "families",
          "pytest_terminal_summary",
          "_inject_setup_method_fixture",
          "PyobjMixin",
          "__array_priority__",
          "get_scope_node",
          "TerminalRepr",
          "getmodpath",
          "pastebin",
          "_should_repr_global_name",
          "Junit",
          "fnmatch_lines_random",
          "_get_marks",
          "repr_args",
          "_node_location_to_relpath",
          "skipping",
          "_get_allow_bytes_flag",
          "section",
          "_ensure_unconfigure",
          "_getscopeitem",
          "warning_types",
          "messages",
          "filter_traceback",
          "getimfunc",
          "getcalls",
          "isatty",
          "README_CONTENT",
          "name2fixturedefs",
          "SysModulesSnapshot",
          "get_log_level_for_setting",
          "showlocals",
          "match",
          "PytestUnhandledCoroutineWarning",
          "get_empty_parameterset_mark",
          "tmpdir",
          "_resolve_arg_value_types",
          "call_runtest_hook",
          "done",
          "_get_teardown_reports",
          "add_report_section",
          "PytestPluginManager",
          "getnode",
          "COLLECT_DIRECTORY_HOOK",
          "try_makedirs",
          "suspend",
          "_get_increasing_order",
          "getplugin",
          "ensure_extended_length_path",
          "pos",
          "D",
          "at_level",
          "attrs",
          "popen",
          "_importtestmodule",
          "visit_Attribute",
          "_get_unknown_ini_keys",
          "PrintHelp",
          "timing",
          "reject",
          "MyOptionParser",
          "_gethookproxy",
          "parseconfig",
          "pytest_doctest_prepare_content",
          "scope2index",
          "getbasetemp",
          "_addexcinfo",
          "_width_of_current_line",
          "startTest",
          "_report_keyboardinterrupt",
          "snap",
          "_result",
          "is_true",
          "evalcache_key",
          "_format",
          "format_session_duration",
          "from_item",
          "parseoutcomes",
          "warn",
          "reorder_items_atscope",
          "monkeypatch",
          "variable",
          "resolve_fixture_function",
          "default_plugins",
          "_log_text",
          "resume_capturing",
          "_add_simple",
          "raises",
          "COLLECT_FAKEMODULE_ATTRIBUTES",
          "_assert_state",
          "wcswidth",
          "execute",
          "approx",
          "matchreport",
          "LineComp",
          "add_global_property",
          "FaultHandlerHooks",
          "path_matches_patterns",
          "stop_global_capturing",
          "capstderr",
          "_version",
          "illegal_xml_re",
          "hookspec",
          "wrap_pytest_function_for_tracing",
          "short_test_summary",
          "parse_num",
          "_checkargnotcontained",
          "collect_by_name",
          "parsefactories",
          "getmodulecol",
          "getpathnode",
          "result",
          "__test__",
          "EOF",
          "ColoredLevelFormatter",
          "node_reporter",
          "merge_family",
          "DOCTEST_REPORT_CHOICE_NONE",
          "_rget_with_confmod",
          "pytester",
          "fail",
          "TeeCaptureIO",
          "pytest_unconfigure",
          "listoutcomes",
          "_validate_args",
          "mark_rewrite",
          "showversion",
          "iter_markers_with_node",
          "getvalue",
          "reorder_items",
          "_warn_incompatibility_with_xunit2",
          "_is_marked_for_rewrite",
          "_add_stats",
          "repr",
          "evaluate",
          "SEP",
          "SubRequest",
          "getreportopt",
          "ApproxScalar",
          "PytestUnknownMarkWarning",
          "_ET",
          "get_exconly",
          "get_open_files",
          "CACHEDIR_TAG_CONTENT",
          "hasopt",
          "_get_main_color",
          "check_testcase_implements_trial_reporter",
          "_is_numpy_array",
          "getsource",
          "ids",
          "tmp_path_factory",
          "_disable_output_capturing_for_darwin",
          "is_globally_capturing",
          "LPAREN",
          "display",
          "pytest_collectreport",
          "LOCK_TIMEOUT",
          "DEFAULT_LOG_FORMAT",
          "global_and_fixture_disabled",
          "fixture",
          "_expecting_failure",
          "pytest_cmdline_parse",
          "_enter_pdb",
          "add_marker",
          "cache_key",
          "freeze_includes",
          "node",
          "from_item_and_call",
          "linecomp",
          "makeini",
          "_format_action_invocation",
          "_getglobals",
          "generic_visit",
          "_W",
          "extract_from",
          "pytest_collection_modifyitems",
          "create_new_paste",
          "getinicfg",
          "_report_unserialization_failure",
          "setupplan",
          "issequence",
          "StoreKey",
          "isset",
          "makefile",
          "LoggingPlugin",
          "__hash__",
          "DoctestItem",
          "traceback",
          "_ALLOW_MARKERS",
          "head_line",
          "_assert_expr_to_lineno",
          "autouse",
          "_importconftest",
          "PYTEST_TAG",
          "args",
          "_FileHandler",
          "_checkversion",
          "copy",
          "FastFilesCompleter",
          "PytestDeprecationWarning",
          "set_fixture",
          "visit_Compare",
          "_ensure_removed_sysmodule",
          "showfspath",
          "_parsearg",
          "start_capturing",
          "ArgumentError",
          "_truncate_by_char_count",
          "_prepareconfig",
          "pytest_pycollect_makemodule",
          "_has_param_ids",
          "__array_ufunc__",
          "summary_errors",
          "AlwaysDispatchingPrettyPrinter",
          "_ellipsize",
          "pytest_collection",
          "get_data",
          "pytest_report_collectionfinish",
          "DOCTEST_REPORT_CHOICE_UDIFF",
          "conftest_options",
          "find_prefixed",
          "accept",
          "_check_scope",
          "outcomes",
          "_format_boolop",
          "DropShorterLongHelpFormatter",
          "_is_doctest",
          "HookRecorder",
          "getreports",
          "showhelp",
          "pytest_cmdline_main",
          "__attrs_post_init__",
          "MARK_GEN",
          "fspath",
          "USAGE_ERROR",
          "_opentestcase",
          "_recurse",
          "create_terminal_writer",
          "mark",
          "register_assert_rewrite",
          "MINUS_K_COLON",
          "locate_config",
          "_Markable",
          "syspath_prepend",
          "rm_rf",
          "fixturenames",
          "write_line",
          "MarkDecorator",
          "tb",
          "addUnexpectedSuccess",
          "pytest_collection_finish",
          "WARNING_CAPTURED_HOOK",
          "getstatementrange_ast",
          "ReprFuncArgs",
          "find_suffixes",
          "fill_unfilled",
          "syspathinsert",
          "_S",
          "num_mock_patch_args",
          "UsageError",
          "teardown_nose",
          "_validate_keys",
          "resume_global_capture",
          "IGNORE_PAM",
          "indent",
          "istext",
          "istrue",
          "showlongtestinfo",
          "Code",
          "terminal",
          "_split_explanation",
          "assert_contains",
          "runpython_c",
          "_io",
          "cwd_relative_nodeid",
          "show_fixtures_per_test",
          "NOT",
          "pytest_warning_recorded",
          "_get_plugin_specs_as_list",
          "_strtobool",
          "addsection",
          "deindent",
          "NotSetType",
          "set_when",
          "_perform_collect",
          "delenv",
          "_format_repr_exception",
          "should_do_markup",
          "fromdictargs",
          "__setitem__",
          "_genfunctions",
          "fault_handler_stderr_key",
          "raiseerror",
          "fnmatch_ex",
          "reprfuncargs",
          "Traceback",
          "pytest_runtestloop",
          "_setupstate",
          "item_capture",
          "TestReport",
          "longrepr",
          "capfd",
          "_try_repr_or_str",
          "repr_excinfo",
          "__init__",
          "_truncate_recursive_traceback",
          "emit",
          "_resume",
          "getini",
          "_warn_already_imported",
          "argnames",
          "repr_failure",
          "_get_multicapture",
          "cls",
          "getlocation",
          "parseconfigure",
          "repr_locals",
          "_argcomplete",
          "obj",
          "assert_outcomes",
          "_validate_ids",
          "visit_Name",
          "report_collect",
          "finish",
          "_compilecounter",
          "iter_markers",
          "_patch_unwrap_mock_aware",
          "_get_line_with_reprcrash_message",
          "AssertionState",
          "ihook",
          "call_optional",
          "_highlight",
          "_get_override_ini_value",
          "ExceptionRepr",
          "putaround",
          "deactivate_fixture",
          "set_log_path",
          "wrap_session",
          "pytest_warning_captured",
          "filename_arg",
          "get_plugin_manager",
          "function",
          "ReprExceptionInfo",
          "getfuncargnames",
          "ImportMode",
          "suspend_fixture",
          "SafeRepr",
          "LOGLEVEL_COLOROPTS",
          "__len__",
          "_is_last_item",
          "summary_stats",
          "__next__",
          "getrawcode",
          "_initrequest",
          "_CACHE_PREFIX_DIRS",
          "path",
          "inline_run",
          "pytest_make_parametrize_id",
          "name",
          "catch_log_handler_key",
          "set_trace",
          "parse_known_and_unknown_args",
          "PseudoFixtureDef",
          "_fail",
          "_callfinalizers",
          "CollectErrorRepr",
          "KeywordMatcher",
          "runner",
          "call_fixture_func",
          "teardown_all",
          "errisinstance",
          "hookimpl",
          "capfdbinary",
          "addini",
          "IDENT",
          "runpytest_inprocess",
          "Failed",
          "flow_marker",
          "_AnyPurePath",
          "extract_suffixes",
          "_color_for_type_default",
          "pytest_collectstart",
          "setenv",
          "_schedule_finalizers",
          "_HandlerType",
          "isiterable",
          "FSCollector",
          "BINOP_MAP",
          "style",
          "_log",
          "pytest_internalerror",
          "pytest_enter_pdb",
          "visit_Starred",
          "_update_message",
          "FixtureLookupError",
          "FixtureManager",
          "doctest_namespace",
          "_getentrysource",
          "makedir",
          "_bytes_to_ascii",
          "ensure_newline",
          "saferepr",
          "_basetemp",
          "chdir",
          "_matchfactories",
          "activate_fixture",
          "DOCTEST_REPORT_CHOICE_CDIFF",
          "gethookproxy",
          "_get_assertion_exprs",
          "Exception",
          "mode",
          "CaptureResult",
          "str",
          "isparseable",
          "OK",
          "get_unpacked_marks",
          "tbfilter",
          "pytest_report_teststatus",
          "pytestPDB",
          "_setup_fixtures",
          "_running_on_ci",
          "is_generator",
          "_idval",
          "_write_progress_information_filling_space",
          "getpluginversioninfo",
          "stopTest",
          "_consider_importhook",
          "PdbInvoke",
          "caplog",
          "evalxfail_key",
          "OptionGroup",
          "error",
          "load_config_dict_from_file",
          "LFPluginCollWrapper",
          "NodeMeta",
          "_is_capturing",
          "FixtureRequest",
          "_esctable",
          "_cachedir",
          "_get_global_properties_node",
          "_write_report_lines_from_hooks",
          "_iter_all_modules",
          "_bestrelpath_cache",
          "extraline",
          "ReprFileLocation",
          "format",
          "reprlocals",
          "_ANSI_ESCAPE_SEQ",
          "_create",
          "set",
          "pytest_report_from_serializable",
          "essential_plugins",
          "invocation_dir",
          "_get_first_non_fixture_func",
          "scopeproperty",
          "getitem",
          "_sys_snapshot",
          "names",
          "is_capturing",
          "install_importhook",
          "DEFAULT_MAX_LINES",
          "initialnames",
          "LogXML",
          "__getitem__",
          "count_towards_summary",
          "for_config",
          "get_config",
          "FuncFixtureInfo",
          "Testdir",
          "parse_hookimpl_opts",
          "tracebackcutdir",
          "addExpectedFailure",
          "get_public_names",
          "cacheshow",
          "TracebackEntry",
          "relline",
          "_get_auto_indent",
          "_getconftest_pathlist",
          "get_scope_package",
          "Package",
          "IDENT_PREFIX",
          "patchsysdict",
          "start_global_capturing",
          "record_testsuite_property",
          "warns",
          "_pytest",
          "pytest_itemcollected",
          "compile_",
          "assert_contains_lines",
          "chain",
          "parts",
          "FSHookProxy",
          "_NodeReporter",
          "_locationline",
          "TempdirFactory",
          "helper",
          "and_expr",
          "_write_entry_lines",
          "_teardown_towards",
          "undo",
          "MultipleDoctestFailures",
          "set_session",
          "async_warn_and_skip",
          "lineno",
          "MoreQuietAction",
          "Token",
          "LineMatcher",
          "context",
          "get_extended_length_path_str",
          "fixtures",
          "__call__",
          "wcwidth",
          "countoutcomes",
          "nose",
          "_makeitem",
          "reports",
          "TYPE_CHECKING",
          "_compute_fixture_value",
          "_outrep_summary",
          "debugging",
          "assertrepr_compare",
          "_read_pyc",
          "_approx_scalar",
          "_issue_warning_captured",
          "get_cache_dir",
          "PytestAssertRewriteWarning",
          "scope2props",
          "records",
          "Argument",
          "_getvaluepath",
          "_import_pdb_cls",
          "listchain",
          "get_dirs_from_args",
          "_early_rewrite_bailout",
          "freeze_support",
          "CaptureIO",
          "params",
          "isnosetest",
          "makepyprojecttoml",
          "get_real_func",
          "get_common_ancestor",
          "warnings",
          "skipped_by_mark_key",
          "config",
          "list",
          "newinstance",
          "getfixturemarker",
          "fslocation",
          "wrap_function_to_error_out_if_called_directly",
          "delitem",
          "abspath",
          "_getautousenames",
          "_try_load_conftest",
          "for_later",
          "_plugin_nameversions",
          "repr_traceback",
          "_postmortem_traceback",
          "EncodedFile",
          "TERMINALWRITER_WRITER",
          "mkdir",
          "ishidden",
          "getitems",
          "_addoption_instance",
          "validate_basetemp",
          "import_path",
          "PytestWarning",
          "_processopt",
          "_force_symlink",
          "symlink_or_skip",
          "REGEX_TYPE",
          "assign",
          "_getindent",
          "finish_recording",
          "own_mark_names",
          "_format_lines",
          "plugins",
          "setdefault",
          "FILLFUNCARGS",
          "reportinfo",
          "fnmatch_lines",
          "gethookrecorder",
          "_params_converter",
          "get_terminal_width",
          "MonkeyPatch",
          "NOTSET",
          "_to_json",
          "Mark",
          "_handle_teardown_sections",
          "ExceptionInfo",
          "encoding",
          "annotated_getattr",
          "_remove_ansi_escape_sequences",
          "readlines",
          "console_main",
          "cmdline",
          "__delitem__",
          "inline_runsource",
          "summary_warnings",
          "pytest_exception_interact",
          "typename",
          "CaptureFixture",
          "write_captured_output",
          "CHECKER_CLASS",
          "_warn_about_missing_assertion",
          "SysCaptureBinary",
          "_make_xunit_fixture",
          "get_option_ini",
          "WarningReport",
          "NO_TESTS_COLLECTED",
          "close",
          "register_cleanup_lock_removal",
          "_inject_setup_teardown_fixtures",
          "deprecated",
          "eval",
          "_is_skipped",
          "ExceptionChainRepr",
          "pytest_generate_tests",
          "_testcase",
          "handleError",
          "_show_fixtures_per_test",
          "collect",
          "showfixtures",
          "_traceback",
          "astcache",
          "RUNNER_CLASS",
          "_get_runner",
          "pytest_runtest_logstart",
          "get_user",
          "TimeoutExpired",
          "_given_basetemp",
          "skip",
          "SysCapture",
          "addfinalizer",
          "start",
          "pytest_runtest_protocol",
          "hasnew",
          "from_current",
          "text",
          "MarkEvaluator",
          "ConftestImportFailure",
          "append",
          "assertoutcome",
          "pytest_runtest_logfinish",
          "names_closure",
          "readline",
          "fail_marker",
          "pytest_plugin_registered",
          "pytest_pyfunc_call",
          "cached_eval",
          "writer",
          "prune_dependency_tree",
          "set_repr_style",
          "make_properties_node",
          "__slots__",
          "FixtureFunctionMarker",
          "rex_session_duration",
          "__bool__",
          "_parse_lsof_output",
          "_LiveLoggingStreamHandler",
          "popcall",
          "perform_collect",
          "simple",
          "clear",
          "_get_direct_parametrize_args",
          "_truncate_explanation",
          "Collector",
          "Notset",
          "message",
          "ApproxSequencelike",
          "_update_current_test_var",
          "_compare_eq_iterable",
          "listnames",
          "assertstate_key",
          "kwargs",
          "cache",
          "isdatacls",
          "truncate_if_required",
          "_PluggyPlugin",
          "no_re_match_line",
          "TokenType",
          "restore",
          "MINUS_K_DASH",
          "__exit__",
          "runpytest",
          "is_async_function",
          "_get_allow_unicode_flag",
          "_FixtureFunction",
          "_from_json",
          "pytest_load_initial_conftests",
          "try_cleanup",
          "_no_match_line",
          "_showfixtures_main",
          "_is_mocked",
          "to_xml",
          "instance",
          "repr_traceback_entry",
          "PytestConfigWarning",
          "getrepr",
          "Node",
          "parametrize",
          "is_rewrite_disabled",
          "write_sep",
          "truncate_locals",
          "_names",
          "_write_source",
          "_factorytraceback",
          "WarningsRecorder",
          "resultlog",
          "statement",
          "pytest_assertrepr_compare",
          "get_sections",
          "_NodeType",
          "_makepath",
          "get_lock_path",
          "__repr__",
          "spawn",
          "_legal_chars",
          "pop",
          "determine_setup",
          "fix_cache_order",
          "_parse_filter",
          "ReprEntry",
          "_reprcompare",
          "pytest_runtest_logreport",
          "_makefile",
          "CallSpec2",
          "_compare_eq_any",
          "_fixturemanager",
          "id",
          "_init_runner_class",
          "getfixturevalue",
          "_assert_start_repr",
          "_saferepr",
          "K",
          "StepwisePlugin",
          "when",
          "make_hook_recorder",
          "summary_failures",
          "pastebinfile_key",
          "_report_to_json",
          "CwdSnapshot",
          "_init_pdb",
          "_pluginmanager",
          "getfailures",
          "testdir",
          "expr",
          "_notin_text",
          "visit_BinOp",
          "ApproxMapping",
          "fullwidth",
          "ExitCode",
          "ResultLog",
          "FDCapture",
          "_ensure_supporting_files",
          "_seen",
          "lines",
          "_CACHE_PREFIX_VALUES",
          "MarkMatcher",
          "Interrupted",
          "unset_fixture",
          "assert_never",
          "_repr_failure_py",
          "keywords",
          "TerminalReporter",
          "strip",
          "_validate_plugins",
          "_wrapped_pdb_cls",
          "JUNIT_XML_DEFAULT_FAMILY",
          "pytest_report_header",
          "_saved",
          "ReprFailDoctest",
          "copy_example",
          "getexplanation",
          "_initini",
          "_validate_if_using_arg_names",
          "parse_args",
          "EMPTY_BUFFER",
          "_get_number_flag",
          "recwarn",
          "mktemp",
          "summary_passes",
          "pytest_keyboard_interrupt",
          "addSuccess",
          "File",
          "directory_arg",
          "_T",
          "notify_exception",
          "__getattr__",
          "template",
          "_determine_show_progress_info",
          "_getreprcrash",
          "scopes",
          "_ensure_basetemp",
          "parse_known_args",
          "_PY_DIR",
          "xfail",
          "__ne__",
          "build_summary_stats_line",
          "_check_if_assertion_pass_impl",
          "ImportPathMismatchError",
          "unexpectedsuccess_key",
          "Exit",
          "stream",
          "_legal_xml_re",
          "setup",
          "nodeid",
          "consider_env",
          "normalize_mark_list",
          "_repr_style",
          "exit",
          "mangle_test_address",
          "_should_rewrite",
          "pytest_cmdline_preparse",
          "LineMatcher_fixture",
          "longreprtext",
          "_parse_parametrize_parameters",
          "buffer",
          "safe_getattr",
          "_get_continue_on_failure",
          "_match_lines_random",
          "from_config",
          "handler",
          "no_header",
          "NoCapture",
          "_rewrite_test",
          "PercentStyleMultiline",
          "_check_initialpaths_for_relpath",
          "__take_sys_modules_snapshot",
          "_split_lines",
          "reprfileloc",
          "Store",
          "FakeCollectModule",
          "suspend_capturing",
          "CollectError",
          "_config_for_test",
          "_compare_eq_set",
          "_surrounding_parens_on_own_lines",
          "get_statement_startend2",
          "_addoption",
          "pytest_addoption",
          "Function",
          "_collect",
          "_call_assertion_pass",
          "_compare_eq_dict",
          "pytest_runtest_setup",
          "UnformattedWarning",
          "_get_verbose_word",
          "__dir__",
          "token",
          "lex",
          "pytest_sessionfinish",
          "_getparser",
          "resume",
          "INTERRUPTED",
          "showheader",
          "_call_reprcompare",
          "is_potential_nosetest",
          "pytest_make_collect_report",
          "getfixtureinfo",
          "failed",
          "getfixturedefs",
          "funcargs",
          "_ensure_immutable_ids",
          "_get_report_choice",
          "addError",
          "pytest_collect_file",
          "width_of_current_line",
          "wasvalid",
          "_in_venv",
          "matching_platform",
          "BaseReport",
          "DEPRECATED_EXTERNAL_PLUGINS",
          "write_docstring",
          "add_cleanup",
          "frame",
          "Parser",
          "_getpytestargs",
          "_translate_non_printable",
          "readouterr",
          "__version_tuple__",
          "python_api",
          "param",
          "_for_parametrize",
          "type",
          "sections",
          "__version__",
          "locals",
          "ensure_reset_dir",
          "visit_Assert",
          "stepwise",
          "_markers",
          "record_xml_attribute",
          "getfslineno",
          "_make_plural",
          "co_equal",
          "python",
          "version",
          "pytest_leave_pdb",
          "funcnamefilter",
          "_create_formatter",
          "_F",
          "_preparse",
          "AND",
          "_parse_ini_config",
          "idmaker",
          "FDCaptureBinary",
          "_FixtureCachedResult",
          "funcargnames",
          "cacheprovider",
          "cache_dir_from_config",
          "_non_numeric_type_error",
          "with_args",
          "_get_active_fixturedef",
          "yield_fixture",
          "ascii_escaped",
          "pop_format_context",
          "builtin_plugins",
          "re_match_lines",
          "resolve_package_path",
          "write_ensure_prefix",
          "setitem",
          "skipped",
          "_eval_scope_callable",
          "listextrakeywords",
          "SysPathsSnapshot",
          "PytestCacheWarning",
          "addSkip",
          "get_parametrized_fixture_keys",
          "duration",
          "ApproxBase",
          "_get_stderr_fileno",
          "_py_ext_re",
          "rex_outcome",
          "importorskip",
          "TempPathFactory",
          "pytest_ignore_collect",
          "WarningsChecker",
          "check_strict_xfail",
          "from_exc_info",
          "_pop_and_teardown",
          "isdict",
          "teardown_exact",
          "PYC_TAIL",
          "getoption",
          "PdbTrace",
          "add_funcarg_pseudo_fixture_def",
          "RE_IMPORT_ERROR_NAME",
          "SetupState",
          "KNOWN_TYPES",
          "fillfixtures",
          "markname",
          "append_error",
          "Session",
          "read",
          "maybe_wrap_pytest_function_for_tracing",
          "_state",
          "_in_suspended",
          "_PLUGGY_DIR",
          "_tmppath_factory",
          "getparam",
          "prepare",
          "maybe_delete_a_numbered_dir",
          "LFPlugin",
          "DoctestModule",
          "from_call",
          "NodeKeywords",
          "finalize",
          "_prunetraceback",
          "addinivalue_line",
          "markup",
          "faulthandler",
          "PytestCollectionWarning",
          "append_collect_error",
          "compat",
          "parse",
          "MultiCapture",
          "runpytest_subprocess",
          "pytest_report_to_serializable",
          "runtestprotocol",
          "logging",
          "_suspend",
          "getvalueorskip",
          "pytest_configure",
          "OutcomeException",
          "_inject_setup_class_fixture",
          "excinfo",
          "AssertionRewritingHook",
          "_prepare_content",
          "_Key",
          "runitem",
          "_R",
          "clear_cache",
          "_istrue",
          "PyCollector",
          "record_testreport",
          "_diff_text",
          "_mark_plugins_for_rewrite",
          "consider_module",
          "_REPORTCHARS_DEFAULT",
          "XFailed",
          "LogCaptureFixture",
          "runtest",
          "rewrite_asserts",
          "create_module",
          "FILE_OR_DIR",
          "module",
          "_get_fixturestack",
          "pytest_sessionstart",
          "_show_fixture_action",
          "dir",
          "show_test_item",
          "record_property",
          "tmpdir_factory",
          "CallInfo",
          "get_optionflags",
          "_check_type",
          "make_numbered_dir_with_cleanup",
          "call_and_report",
          "nofuncargs",
          "Config",
          "_runtest_for",
          "get_lines_after",
          "version_tuple",
          "DEFAULT_MAX_CHARS",
          "invalidraise",
          "Module",
          "Item",
          "_idvalset",
          "MatcherAdapter",
          "pytest_addhooks",
          "catch_log_records_key",
          "no_fnmatch_line",
          "__all__",
          "xml_key",
          "matchnodes",
          "read_global_capture",
          "_color_for_type",
          "Scanner",
          "_getini",
          "istestfunction",
          "_LiveLoggingNullHandler",
          "ParseError",
          "warning_record_to_str",
          "DEFAULT_LOG_DATE_FORMAT",
          "_compare_eq_cls",
          "_write_content",
          "assertion",
          "_PytestWrapper",
          "Metafunc",
          "_main",
          "_teardown_with_finalization",
          "_tryconvertpyarg",
          "resultlog_key",
          "get_real_method",
          "BASE_TYPE",
          "prog",
          "nodes",
          "reprcrash",
          "_resolve_arg_ids",
          "_assertion_supported",
          "getparent",
          "combined_with",
          "NoMatch",
          "_get_progress_information_message",
          "filter",
          "parse_setoption",
          "DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE",
          "_do_configure",
          "addoption",
          "getfixtureclosure",
          "TEST_OUTCOME",
          "ReprEntryNative",
          "_warn_or_fail_if_strict",
          "_striptext",
          "_call_with_optional_argument",
          "get_records",
          "getgroup",
          "_get_checker",
          "deselect_by_keyword",
          "_compare_eq_verbose",
          "capsysbinary",
          "capsys",
          "pop_outerr_to_orig",
          "category",
          "_set_initial_conftests",
          "_printcollecteditems",
          "Instance",
          "ReprTracebackNative",
          "pathlib",
          "nextitem",
          "parse_hookspec_opts",
          "formatrepr",
          "flush",
          "FixtureLookupErrorRepr",
          "_splitnode",
          "fail_fixturefunc",
          "visit_UnaryOp",
          "CaptureManager",
          "PytestExperimentalApiWarning",
          "getargs",
          "__enter__",
          "suspend_global_capture",
          "LogCaptureHandler",
          "__str__",
          "ischildnode",
          "_legal_ranges",
          "consider_conftest",
          "line",
          "write",
          "UNARY_MAP",
          "get_fslocation_from_item",
          "session",
          "store",
          "deselect_by_mark",
          "TESTS_FAILED",
          "spawn_pytest",
          "scopenum_function",
          "_WithException",
          "_matchnodes",
          "import_plugin",
          "TerminalWriter",
          "_FixtureValue",
          "istestclass",
          "_py36_windowsconsoleio_workaround",
          "__missing__",
          "hasinit",
          "visit_BoolOp",
          "post_mortem",
          "__iter__",
          "passed",
          "istestfunc",
          "ReprLocals",
          "getstatement",
          "_find_parametrized_scope",
          "_set_main_color",
          "builtin",
          "get_last_failed_paths",
          "_collectfile",
          "sep",
          "catching_logs",
          "addFailure",
          "setmulti2",
          "Skipped",
          "capture",
          "safe_isclass",
          "_typ_map",
          "tolerance"
        ]
      },
      "response": {
        "keywords": [
          "cached_eval",
          "MarkEvaluator",
          "_istrue",
          "skipif",
          "xfail",
          "mark",
          "evaluate"
        ]
      }
    },
    {
      "name": "FilesKeywordSearch",
      "metadata": {
        "start": "2024-08-19T12:18:26.078942",
        "usage": "231.52709007263184 ms"
      },
      "request": {
        "tokens": [
          "cached_eval",
          "MarkEvaluator",
          "_istrue",
          "skipif",
          "xfail",
          "mark",
          "evaluate"
        ]
      },
      "response": {
        "results": {
          "mark": [
            "src/_pytest/mark/__init__.py",
            "src/pytest/__init__.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/mark/evaluate.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/warnings.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/skipping.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/python_api.py",
            "src/_pytest/python.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/nodes.py"
          ],
          "cached_eval": [
            "src/_pytest/mark/evaluate.py"
          ],
          "MarkEvaluator": [
            "src/_pytest/mark/evaluate.py",
            "src/_pytest/skipping.py"
          ],
          "_istrue": [
            "src/_pytest/mark/evaluate.py"
          ],
          "xfail": [
            "src/_pytest/unittest.py",
            "src/_pytest/mark/__init__.py",
            "src/pytest/__init__.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/junitxml.py"
          ],
          "evaluate": [
            "src/_pytest/mark/__init__.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/skipping.py"
          ],
          "skipif": [
            "src/_pytest/mark/structures.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/skipping.py"
          ]
        }
      }
    },
    {
      "name": "FilesFilter",
      "metadata": {
        "start": "2024-08-19T12:18:26.310492",
        "usage": "3969.226121902466 ms"
      },
      "request": {
        "keyword_search_results": {
          "mark": [
            "src/_pytest/mark/__init__.py",
            "src/pytest/__init__.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/mark/evaluate.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/warnings.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/skipping.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/python_api.py",
            "src/_pytest/python.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/nodes.py"
          ],
          "cached_eval": [
            "src/_pytest/mark/evaluate.py"
          ],
          "MarkEvaluator": [
            "src/_pytest/mark/evaluate.py",
            "src/_pytest/skipping.py"
          ],
          "_istrue": [
            "src/_pytest/mark/evaluate.py"
          ],
          "xfail": [
            "src/_pytest/unittest.py",
            "src/_pytest/mark/__init__.py",
            "src/pytest/__init__.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/junitxml.py"
          ],
          "evaluate": [
            "src/_pytest/mark/__init__.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/skipping.py"
          ],
          "skipif": [
            "src/_pytest/mark/structures.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/python.py",
            "src/_pytest/skipping.py"
          ]
        }
      },
      "response": {
        "scratch_pad": "The issue is about incorrect caching when evaluating the conditions for skipping tests in pytest. Specifically, the function `cached_eval` used in the file `src/_pytest/mark/evaluate.py` caches these evaluations and incorrectly uses only the string condition as the key, ignoring other scope factors which could lead to a bug.\n\nTo identify relevant files, we need to focus on references to `cached_eval`, `MarkEvaluator`, and the method `_istrue`. Moreover, anything related to `skipif` might also be relevant, but to a lesser extent since the core issue lies in the evaluation caching rather than the specific markers.\n\n1. **`src/_pytest/mark/evaluate.py`**: This file contains `cached_eval`, `MarkEvaluator`, and `_istrue`, all of which are directly mentioned in the issue. It's almost certainly where the core logic of the caching behavior is found.\n2. **`src/_pytest/mark/structures.py`**: Contains `skipif` marker logic, which might tie into how these are processed and evaluated. But it\u2019s less likely to contain the core bug logic.\n3. **`src/_pytest/skipping.py`**: Also might have some relevance to how skipping logic is integrated, containing references to `MarkEvaluator` and `skipif`.\n\nThe rest of the files or keyword hits seem less relevant since they don't directly intersect the core problem described.",
        "file_paths": [
          "src/_pytest/mark/evaluate.py",
          "src/_pytest/mark/structures.py",
          "src/_pytest/skipping.py"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T12:18:30.279734",
        "usage": "6757.328748703003 ms"
      },
      "request": {
        "file_path": "src/_pytest/mark/evaluate.py"
      },
      "response": {
        "scratch_pad": "There is a caching mechanism in the `cached_eval` function that caches the evaluation results of expressions based on the expression string. However, this caching mechanism does not account for variations in the global namespace (`d`) that can change the outcome of the evaluation. As the reported issue states, this leads to incorrect results when the same string expression is used across different test modules where the global variables might differ.\n\nAnalyzing the provided code, especially the `cached_eval` function, indeed confirms that it uses only the expression string as the cache key and ignores the dynamics of the globals (`d`). The `MarkEvaluator._istrue()` method calls `cached_eval` to evaluate condition expressions.\n\nTo resolve the issue, removing the caching logic from `cached_eval` and integrating the evaluation directly into the `MarkEvaluator._istrue()` function without caching would ensure that the globals are respected during each evaluation. The proposed solution in the issue description aligns with this idea by suggesting to inline `cached_eval` into the `MarkEvaluator._istrue().\n\n- Relevant symbols: `cached_eval` is directly tied to the caching problem. `MarkEvaluator._istrue()` involves the crucial point where the condition expression evaluation takes place.\n- The root cause of the issue is definitely in the way `cached_eval` handles caching based solely on expression strings, thus leading to incorrect evaluations when different contexts (globals) are used.",
        "relevant": true,
        "root_cause": true,
        "relevant_files": [],
        "relevant_symbols": [
          "MarkEvaluator._istrue",
          "cached_eval"
        ],
        "root_cause_symbols": [
          "cached_eval"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T12:18:37.037111",
        "usage": "6650.852918624878 ms"
      },
      "request": {
        "file_path": "src/_pytest/mark/structures.py"
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 The user reports an issue regarding the caching behavior of pytest for skipif/xfail string condition evaluations.\n1.2 By running `pytest test_module_1.py test_module_2.py`, both tests are skipped when only one should be. This happens because the evaluation result of the string condition is cached without considering globals scope per test module.\n1.3 The issue is caused by caching the evaluation of string conditions based solely on the condition string and not the context in which it's evaluated.\n1.4 The user's expectation is that caching shouldn't interfere with correct evaluation as per each test module's condition. Since globals are not considered in caching, both tests get skipped due to the cached result.\n1.5 Removing caching (or modifying it to include globals) should resolve this.\n\n2. Make file evaluation:\n2.1 The file is `src/_pytest/mark/structures.py`.\n2.2 This file does not deal with the caching logic for skipif/xfail; it handles MarkDecorators and other mark-related utility functions and classes.\n2.3 The issue described is related to caching logic, which is likely not managed in this file but perhaps in a file dealing with evaluation or caching mechanisms, possibly '_pytest/mark/evaluate.py', as hinted by the user.\n\nConclusion: \n- This file does not appear to directly address the root cause of the caching issue described.\n- Likely relevant files would include '_pytest/mark/evaluate.py'.",
        "relevant": false,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/mark/evaluate.py"
        ],
        "relevant_symbols": [],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T12:18:43.687996",
        "usage": "14077.940940856934 ms"
      },
      "request": {
        "file_path": "src/_pytest/skipping.py"
      },
      "response": {
        "scratch_pad": "### Understanding the Issue\n1. **Issue Description**:\n   The issue is about incorrect caching behavior in the evaluation of skipif/xfail string conditions in pytest. Specifically, the caching key is only the condition string itself, which fails to account for differences in the module's global context. This can lead to incorrect behavior when tests rely on conditions involving global variables.\n\n2. **Reproduction Steps**:\n   The user provided two test modules:\n   - `test_module_1.py` which should be skipped and\n   - `test_module_2.py` which shouldn't be skipped.\n\n   When running these modules together, due to caching, both tests are skipped. This behavior is incorrect.\n\n3. **Root Cause**:\n   The problem is caused by using a caching mechanism for the evaluation of strings in `@pytest.mark.skipif` which only considers the condition string, ignoring differences in the global context of the modules where these strings are evaluated.\n\n4. **User Expectation**:\n   The user expects `test_module_1.py` to be skipped and `test_module_2.py` to be executed normally, as their respective global 'skip' variables determine.\n\n5. **Suggested Solution**:\n   The user suggests removing the caching and inlining the evaluation logic within `MarkEvaluator._istrue` to better handle the contextual differences across modules.\n\n### Evaluating the File\n1. **File Context**:\n   The file `src/_pytest/skipping.py` contains functions and hooks for handling skip and xfail markers in pytest. The central point here seems to be how pytest evaluates conditions for these markers and caches their results.\n\n2. **Symbol Relevance**:\n   - The relevant symbol in this context appears to be `MarkEvaluator.istrue()` within the `pytest_runtest_setup` function where this caching behavior might be evidenced. This method is responsible for determining if a test should be skipped by evaluating the skipif marker.\n   - `_pytest.mark.evaluate.MarkEvaluator` is initialized and used in `pytest_runtest_setup` for evaluating the 'skipif' condition.\n\n3. **Analysis**:\n   - The `pytest_runtest_setup` function clearly utilizes `MarkEvaluator`, creating instances for 'skipif' and 'xfail'. This connects to the issue as the conditions for skipping are evaluated here.\n   - There's mention of `MarkEvaluator.is_true()`, which suggests this ties back to where string conditions would be evaluated in conjunction with potentially problematic caching.\n\n4. **Conclusion**:\n   The file is relevant as it includes direct references to how conditions in markers like skipif/xfail are evaluated, which is intrinsic to the issue at hand.\n\n### Decision\n- Considering the issue is directly about the evaluation and caching behavior of conditions. The `MarkEvaluator.istrue()` usage in `pytest_runtest_setup` is highly relevant and pointed out by the user as a place to refactor.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/mark/evaluate.py"
        ],
        "relevant_symbols": [
          "pytest_runtest_setup",
          "MarkEvaluator",
          "pytest_runtest_makereport",
          "check_xfail_no_run",
          "check_strict_xfail"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileSymbolSearch",
      "metadata": {
        "start": "2024-08-19T12:18:57.766031",
        "usage": "146.61884307861328 ms"
      },
      "request": {
        "files": [
          {
            "file_path": "src/_pytest/mark/evaluate.py",
            "relevant_symbol_keys": [
              "MarkEvaluator._istrue",
              "cached_eval"
            ],
            "root_cause_symbol_keys": [
              "cached_eval"
            ]
          },
          {
            "file_path": "src/_pytest/mark/structures.py",
            "relevant_symbol_keys": [],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/_pytest/skipping.py",
            "relevant_symbol_keys": [
              "pytest_runtest_setup",
              "MarkEvaluator",
              "pytest_runtest_makereport",
              "check_xfail_no_run",
              "check_strict_xfail"
            ],
            "root_cause_symbol_keys": []
          }
        ],
        "with_content": true
      },
      "response": {
        "related_symbols": [
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "unxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
            "start_line": 33,
            "start_column": 794,
            "end_line": 0,
            "end_column": 4578,
            "start_byte": 794,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.MarkEvaluator.__init__",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the",
            "start_line": 34,
            "start_column": 819,
            "end_line": 4,
            "end_column": 1032,
            "start_byte": 819,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.__bool__",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n      ",
            "start_line": 40,
            "start_column": 1038,
            "end_line": 4,
            "end_column": 1152,
            "start_byte": 1038,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.wasvalid",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    ",
            "start_line": 44,
            "start_column": 1158,
            "end_line": 4,
            "end_column": 1225,
            "start_byte": 1158,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._get_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fig.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        c",
            "start_line": 47,
            "start_column": 1231,
            "end_line": 4,
            "end_column": 1332,
            "start_byte": 1231,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.invalidraise",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "_cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-def",
            "start_line": 50,
            "start_column": 1338,
            "end_line": 4,
            "end_column": 1510,
            "start_byte": 1338,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ne, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. ",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "    \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check ",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.get",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = ",
            "start_line": 122,
            "start_column": 4149,
            "end_line": 4,
            "end_column": 4293,
            "start_byte": 4149,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.getexplanation",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "item.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
            "start_line": 127,
            "start_column": 4299,
            "end_line": 4,
            "end_column": 4578,
            "start_byte": 4299,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
          }
        ]
      }
    },
    {
      "name": "SymbolsEvaluate",
      "metadata": {
        "start": "2024-08-19T12:18:57.912668",
        "usage": "140725.11410713196 ms"
      },
      "request": {
        "related_symbols": [
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "unxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
            "start_line": 33,
            "start_column": 794,
            "end_line": 0,
            "end_column": 4578,
            "start_byte": 794,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.MarkEvaluator.__init__",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the",
            "start_line": 34,
            "start_column": 819,
            "end_line": 4,
            "end_column": 1032,
            "start_byte": 819,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.__bool__",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n      ",
            "start_line": 40,
            "start_column": 1038,
            "end_line": 4,
            "end_column": 1152,
            "start_byte": 1038,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.wasvalid",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    ",
            "start_line": 44,
            "start_column": 1158,
            "end_line": 4,
            "end_column": 1225,
            "start_byte": 1158,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._get_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fig.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        c",
            "start_line": 47,
            "start_column": 1231,
            "end_line": 4,
            "end_column": 1332,
            "start_byte": 1231,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.invalidraise",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "_cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-def",
            "start_line": 50,
            "start_column": 1338,
            "end_line": 4,
            "end_column": 1510,
            "start_byte": 1338,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ne, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. ",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "    \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check ",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.get",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = ",
            "start_line": 122,
            "start_column": 4149,
            "end_line": 4,
            "end_column": 4293,
            "start_byte": 4149,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.getexplanation",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "item.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
            "start_line": 127,
            "start_column": 4299,
            "end_line": 4,
            "end_column": 4578,
            "start_byte": 4299,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
          }
        ]
      },
      "response": {
        "scratch_pad": "1. The issue described mentions a problem with caching the evaluation of 'skipif' conditions causing incorrect behavior. \n2. This is likely caused by the caching mechanism in 'cached_eval'.\n3. Expected result is tests should skip correctly without being affected by previous cache entries.\n4. Expected behavior is specific tests are either skippable or executable as per their own conditions without interference.\n5. Fix: Looks like removing the cache use in 'MarkEvaluator._istrue' and 'cached_eval' as their recommendation.\n\nI'm presuming that 'cached_eval' is directly involved since it's mentioned and recommended to change, needing to inline its code instead of calling it to avoid shared state issues with the cache.\n\nMarkEvaluator._istrue is where this is being consumed so it would need modification to remove the use of 'cached_eval'.\n\nOther provided symbols don't appear directly relevant to the caching problem.\n\nNext steps are to review _istrue and cached_eval in particular for this matter.",
        "symbols": [
          {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "reason": "The use of caching in this function is not ideal for 'skipif' as it can let previous evaluations interfere with current ones. Suggested to inline this to avoid cache side effects."
          },
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "reason": "The function uses 'cached_eval' which can result in incorrect skip/xfail evaluations. Modifying this to incorporate inline evaluation without caching will fix the misleading checks."
          }
        ],
        "more_info": [],
        "is_done": false,
        "symbols_scratch_pad": [
          "The function 'cached_eval' relies on a cached result dict to evaluate expressions and uses items globals. Here the caching is inappropriate for 'skipif', since different global contexts affect results; removing caching solves this issue.",
          "This function determines if a condition is true for 'MarkEvaluator', calling 'cached_eval' to determine truth for a string expr. Removing cache-dependence prevents erroneous evaluations from prior cached results.",
          "The function is not relevant to caching evaluation errors directly, but manages which tests are skipped hence impact skipping logic in the tool chain action list and process.",
          "This symbol identifies a pytest hook but doesn't point to the problem origin for caching in evaluation, skipping our direct attention for caching misbehavior.",
          "Like previous func has similar indications but no direct impact on the erroneously cached context the issue describes.",
          "Another pytest hook possibly relating but doesn't direct address the described problem context.",
          "Involvement in pytest hook systems is apparent, not directly linked with the caching issue as described in the broader context.",
          "General MarkEvaluator functions, defines marker behavior but doesn't seek our attention for correcting evaluated context due to caching.",
          "A constructor function lacking direct implemented context on the caching evaluation, but possibly affects that area programmatically setup.",
          "Doesn't comprehensively influence the fix or emanate the problems resulting from the cached_eval concerns.",
          "Auxiliary retrieval functions that possibly interact with others influencing skipif behavior; not the root cause.",
          "Evaluation defines but itself doesn\u2019t pinpoint caching trouble.",
          "Ongoing expressions handler aiding logic elsewhere, not the central problematic area.",
          "Explanations given for xfail/skip checks at-runtest. It settles checked conditions but doesn't resolve the issue with bad caching behavior.",
          "Setup cursors through skip/run phases observing conditional truths inline, partook in traditional formats like skipif.",
          "Capturing expressions likely involved to extent but-may precision fallback specifics to getexplan understandings settled elsewhere.",
          "Validation checks for conditions quite like others, doesn't contest incorrect execution context.",
          "Registration or retrieval methods of relevance indirectly towards skipping sys simulations, redundant info in them."
        ]
      }
    },
    {
      "name": "SymbolSummary",
      "metadata": {
        "start": "2024-08-19T12:21:18.637837",
        "usage": "2.9058456420898438 ms"
      },
      "request": {
        "symbols": {
          "related_symbols": [
            {
              "name": "_pytest.mark.MarkEvaluator._istrue",
              "file_path": "src/_pytest/mark/evaluate.py",
              "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
              "start_line": 81,
              "start_column": 2564,
              "end_line": 4,
              "end_column": 4143,
              "start_byte": 2564,
              "end_byte": 4
            },
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 735,
              "start_column": 23643,
              "end_line": 4,
              "end_column": 23821,
              "start_byte": 23643,
              "end_byte": 4
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 420,
              "start_column": 14531,
              "end_line": 0,
              "end_column": 14635,
              "start_byte": 14531,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 438,
              "start_column": 15172,
              "end_line": 0,
              "end_column": 15521,
              "start_byte": 15172,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 675,
              "start_column": 24746,
              "end_line": 4,
              "end_column": 25079,
              "start_byte": 24746,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "unxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
              "start_line": 33,
              "start_column": 794,
              "end_line": 0,
              "end_column": 4578,
              "start_byte": 794,
              "end_byte": 0
            },
            {
              "name": "_pytest.mark.MarkEvaluator.__init__",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the",
              "start_line": 34,
              "start_column": 819,
              "end_line": 4,
              "end_column": 1032,
              "start_byte": 819,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.__bool__",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "t parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n      ",
              "start_line": 40,
              "start_column": 1038,
              "end_line": 4,
              "end_column": 1152,
              "start_byte": 1038,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.wasvalid",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    ",
              "start_line": 44,
              "start_column": 1158,
              "end_line": 4,
              "end_column": 1225,
              "start_byte": 1158,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator._get_marks",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "fig.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        c",
              "start_line": 47,
              "start_column": 1231,
              "end_line": 4,
              "end_column": 1332,
              "start_byte": 1231,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.invalidraise",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "_cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-def",
              "start_line": 50,
              "start_column": 1338,
              "end_line": 4,
              "end_column": 1510,
              "start_byte": 1338,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.istrue",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
              "start_line": 56,
              "start_column": 1516,
              "end_line": 4,
              "end_column": 2278,
              "start_byte": 1516,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator._getglobals",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ne, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. ",
              "start_line": 75,
              "start_column": 2284,
              "end_line": 4,
              "end_column": 2558,
              "start_byte": 2284,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator._istrue",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "    \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check ",
              "start_line": 81,
              "start_column": 2564,
              "end_line": 4,
              "end_column": 4143,
              "start_byte": 2564,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.get",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = ",
              "start_line": 122,
              "start_column": 4149,
              "end_line": 4,
              "end_column": 4293,
              "start_byte": 4149,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.getexplanation",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "item.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
              "start_line": 127,
              "start_column": 4299,
              "end_line": 4,
              "end_column": 4578,
              "start_byte": 4299,
              "end_byte": 4
            },
            {
              "name": "_pytest.runner.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
              "start_line": 137,
              "start_column": 4365,
              "end_line": 0,
              "end_column": 4497,
              "start_byte": 4365,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 310,
              "start_column": 10281,
              "end_line": 0,
              "end_column": 10412,
              "start_byte": 10281,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
              "start_line": 82,
              "start_column": 2802,
              "end_line": 0,
              "end_column": 3543,
              "start_byte": 2802,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.check_xfail_no_run",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
              "start_line": 113,
              "start_column": 3768,
              "end_line": 0,
              "end_column": 4073,
              "start_byte": 3768,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.check_strict_xfail",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
              "start_line": 122,
              "start_column": 4076,
              "end_line": 0,
              "end_column": 4595,
              "start_byte": 4076,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
              "start_line": 134,
              "start_column": 4598,
              "end_line": 0,
              "end_column": 6754,
              "start_byte": 4598,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 290,
              "start_column": 10753,
              "end_line": 0,
              "end_column": 11542,
              "start_byte": 10753,
              "end_byte": 0
            },
            {
              "name": "_pytest.nose.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
              "start_line": 7,
              "start_column": 167,
              "end_line": 0,
              "end_column": 579,
              "start_byte": 167,
              "end_byte": 0
            }
          ],
          "root_caused_symbols": [
            {
              "name": "_pytest.mark.cached_eval",
              "file_path": "src/_pytest/mark/evaluate.py",
              "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
              "start_line": 20,
              "start_column": 394,
              "end_line": 0,
              "end_column": 791,
              "start_byte": 394,
              "end_byte": 0
            }
          ]
        },
        "symbols_evaluate": {
          "scratch_pad": "1. The issue described mentions a problem with caching the evaluation of 'skipif' conditions causing incorrect behavior. \n2. This is likely caused by the caching mechanism in 'cached_eval'.\n3. Expected result is tests should skip correctly without being affected by previous cache entries.\n4. Expected behavior is specific tests are either skippable or executable as per their own conditions without interference.\n5. Fix: Looks like removing the cache use in 'MarkEvaluator._istrue' and 'cached_eval' as their recommendation.\n\nI'm presuming that 'cached_eval' is directly involved since it's mentioned and recommended to change, needing to inline its code instead of calling it to avoid shared state issues with the cache.\n\nMarkEvaluator._istrue is where this is being consumed so it would need modification to remove the use of 'cached_eval'.\n\nOther provided symbols don't appear directly relevant to the caching problem.\n\nNext steps are to review _istrue and cached_eval in particular for this matter.",
          "symbols": [
            {
              "name": "_pytest.mark.cached_eval",
              "file_path": "src/_pytest/mark/evaluate.py",
              "reason": "The use of caching in this function is not ideal for 'skipif' as it can let previous evaluations interfere with current ones. Suggested to inline this to avoid cache side effects."
            },
            {
              "name": "_pytest.mark.MarkEvaluator._istrue",
              "file_path": "src/_pytest/mark/evaluate.py",
              "reason": "The function uses 'cached_eval' which can result in incorrect skip/xfail evaluations. Modifying this to incorporate inline evaluation without caching will fix the misleading checks."
            }
          ],
          "more_info": [],
          "is_done": false,
          "symbols_scratch_pad": [
            "The function 'cached_eval' relies on a cached result dict to evaluate expressions and uses items globals. Here the caching is inappropriate for 'skipif', since different global contexts affect results; removing caching solves this issue.",
            "This function determines if a condition is true for 'MarkEvaluator', calling 'cached_eval' to determine truth for a string expr. Removing cache-dependence prevents erroneous evaluations from prior cached results.",
            "The function is not relevant to caching evaluation errors directly, but manages which tests are skipped hence impact skipping logic in the tool chain action list and process.",
            "This symbol identifies a pytest hook but doesn't point to the problem origin for caching in evaluation, skipping our direct attention for caching misbehavior.",
            "Like previous func has similar indications but no direct impact on the erroneously cached context the issue describes.",
            "Another pytest hook possibly relating but doesn't direct address the described problem context.",
            "Involvement in pytest hook systems is apparent, not directly linked with the caching issue as described in the broader context.",
            "General MarkEvaluator functions, defines marker behavior but doesn't seek our attention for correcting evaluated context due to caching.",
            "A constructor function lacking direct implemented context on the caching evaluation, but possibly affects that area programmatically setup.",
            "Doesn't comprehensively influence the fix or emanate the problems resulting from the cached_eval concerns.",
            "Auxiliary retrieval functions that possibly interact with others influencing skipif behavior; not the root cause.",
            "Evaluation defines but itself doesn\u2019t pinpoint caching trouble.",
            "Ongoing expressions handler aiding logic elsewhere, not the central problematic area.",
            "Explanations given for xfail/skip checks at-runtest. It settles checked conditions but doesn't resolve the issue with bad caching behavior.",
            "Setup cursors through skip/run phases observing conditional truths inline, partook in traditional formats like skipif.",
            "Capturing expressions likely involved to extent but-may precision fallback specifics to getexplan understandings settled elsewhere.",
            "Validation checks for conditions quite like others, doesn't contest incorrect execution context.",
            "Registration or retrieval methods of relevance indirectly towards skipping sys simulations, redundant info in them."
          ]
        }
      },
      "response": {
        "file_paths": [
          "src/_pytest/mark/evaluate.py"
        ],
        "symbols": [
          "_pytest.mark.cached_eval",
          "_pytest.mark.MarkEvaluator._istrue"
        ],
        "code_spans": {
          "_pytest.mark.MarkEvaluator._istrue": {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "    \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check ",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
          },
          "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_runtest_makereport": {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator": {
            "name": "_pytest.mark.MarkEvaluator",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "unxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem: Function):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
            "start_line": 33,
            "start_column": 794,
            "end_line": 0,
            "end_column": 4578,
            "start_byte": 794,
            "end_byte": 0
          },
          "_pytest.mark.MarkEvaluator.__init__": {
            "name": "_pytest.mark.MarkEvaluator.__init__",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the",
            "start_line": 34,
            "start_column": 819,
            "end_line": 4,
            "end_column": 1032,
            "start_byte": 819,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.__bool__": {
            "name": "_pytest.mark.MarkEvaluator.__bool__",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n      ",
            "start_line": 40,
            "start_column": 1038,
            "end_line": 4,
            "end_column": 1152,
            "start_byte": 1038,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.wasvalid": {
            "name": "_pytest.mark.MarkEvaluator.wasvalid",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "=\"bool\",\n    )\n\n\ndef pytest_configure(config: Config) -> None:\n    ",
            "start_line": 44,
            "start_column": 1158,
            "end_line": 4,
            "end_column": 1225,
            "start_byte": 1158,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator._get_marks": {
            "name": "_pytest.mark.MarkEvaluator._get_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fig.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        c",
            "start_line": 47,
            "start_column": 1231,
            "end_line": 4,
            "end_column": 1332,
            "start_byte": 1231,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.invalidraise": {
            "name": "_pytest.mark.MarkEvaluator.invalidraise",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "_cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception  # type: ignore[attr-def",
            "start_line": 50,
            "start_column": 1338,
            "end_line": 4,
            "end_column": 1510,
            "start_byte": 1338,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.istrue": {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator._getglobals": {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ne, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. ",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.get": {
            "name": "_pytest.mark.MarkEvaluator.get",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = ",
            "start_line": 122,
            "start_column": 4149,
            "end_line": 4,
            "end_column": 4293,
            "start_byte": 4149,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.getexplanation": {
            "name": "_pytest.mark.MarkEvaluator.getexplanation",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "item.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanatio",
            "start_line": 127,
            "start_column": 4299,
            "end_line": 4,
            "end_column": 4578,
            "start_byte": 4299,
            "end_byte": 4
          },
          "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
          },
          "_pytest.runner.pytest_runtest_makereport": {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
          },
          "_pytest.skipping.check_xfail_no_run": {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
          },
          "_pytest.skipping.check_strict_xfail": {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
          },
          "_pytest.unittest.pytest_runtest_makereport": {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
          },
          "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
          },
          "_pytest.mark.cached_eval": {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
          }
        }
      }
    }
  ]
}