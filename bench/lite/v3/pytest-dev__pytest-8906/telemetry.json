{
  "total_cost": 0,
  "task_id": "pytest-dev__pytest-8906",
  "model": "gpt-4o-2024-08-06",
  "spans": [
    {
      "name": "KeywordExtract",
      "metadata": {
        "start": "2024-08-19T13:46:21.074232",
        "usage": "1207.9739570617676 ms"
      },
      "request": {
        "repo_tokens": [
          "showversion",
          "try_makedirs",
          "setenv",
          "_IGNORED_WINERRORS",
          "CollectError",
          "FILLFUNCARGS",
          "maketxtfile",
          "duration",
          "get_default_arg_names",
          "pytest_runtest_makereport",
          "Module",
          "pytest_make_collect_report",
          "Cache",
          "LogXML",
          "_is_numpy_array",
          "PytestUnhandledThreadExceptionWarning",
          "_create_formatter",
          "_with_exception",
          "get_unpacked_marks",
          "pytest_runtest_setup",
          "ensure_newline",
          "_is_marked_for_rewrite",
          "wrap_pytest_function_for_tracing",
          "_find_spec",
          "popcall",
          "_CACHE_PREFIX_VALUES",
          "_compare_eq_verbose",
          "evaluate_condition",
          "keywords",
          "get_lock_path",
          "PRIVATE",
          "cls",
          "makeconftest",
          "recursionindex",
          "resume_fixture",
          "pytest_addoption",
          "mkpydir",
          "write",
          "StepwisePlugin",
          "Store",
          "StoreKey",
          "UNITTEST_SKIP_DURING_COLLECTION",
          "_compare_approx",
          "visit_BoolOp",
          "fillfixtures",
          "notset",
          "helpconfig",
          "assert_outcomes",
          "_locationline",
          "copytree",
          "FakeCollectModule",
          "firstlineno",
          "ReprExceptionInfo",
          "getworkerinfoline",
          "__call__",
          "_get_direct_parametrize_args",
          "_rewrite_test",
          "pytest_exception_interact",
          "merge_family",
          "_for_parametrize",
          "lex",
          "annotated_getattr",
          "essential_plugins",
          "get_closest_marker",
          "pytest_warning_captured",
          "_factorytraceback",
          "setupplan",
          "caplog_handler_key",
          "hasplugin",
          "no_re_match_line",
          "debugging",
          "_as_numpy_array",
          "TestCaseFunction",
          "ConftestImportFailure",
          "resume_global_capture",
          "_config_for_test",
          "activate_fixture",
          "warning_types",
          "_argcomplete",
          "assertoutcome",
          "_getautousenames",
          "getmodulecol",
          "makedir",
          "_import_plugin_specs",
          "_get_teardown_reports",
          "fullsource",
          "_initini",
          "_get_fixturestack",
          "_init_runner_class",
          "explanation_param",
          "PytestCacheWarning",
          "version",
          "_validate_ids",
          "listextrakeywords",
          "write_sep",
          "set",
          "push_format_context",
          "line",
          "terminal",
          "addExpectedFailure",
          "reset",
          "ReprFileLocation",
          "exit",
          "pytest_deselected",
          "warn",
          "dir",
          "_ellipsize",
          "notify_exception",
          "_getconftest_pathlist",
          "runpython_c",
          "Code",
          "disabled",
          "capsys",
          "compat",
          "doctest_namespace",
          "prog",
          "_PytestWrapper",
          "mode",
          "repr_locals",
          "_parse_ini_config",
          "pytest_report_to_serializable",
          "try_cleanup",
          "FixtureManager",
          "fnmatch_lines_random",
          "NodeKeywords",
          "ApproxNumpy",
          "inline_runsource",
          "names_closure",
          "writeorg",
          "getpluginversioninfo",
          "Node",
          "no_header",
          "get_source",
          "build_summary_stats_line",
          "getlocation",
          "tmpdir_factory",
          "reprlocals",
          "fslocation",
          "_get_pdb_wrapper_class",
          "ImportPathMismatchError",
          "_ALLOW_MARKERS",
          "_validate_if_using_arg_names",
          "stop_global_capturing",
          "_write_source",
          "showfixtures",
          "accept",
          "classnamefilter",
          "Skipped",
          "_get_unknown_ini_keys",
          "unset_fixture",
          "fault_handler_stderr_key",
          "ExceptionChainRepr",
          "_update_current_test_var",
          "session",
          "repr_traceback_entry",
          "SysModulesSnapshot",
          "instance",
          "evaluate_skip_marks",
          "type",
          "_inject_setup_teardown_fixtures",
          "DEPRECATED_EXTERNAL_PLUGINS",
          "AND",
          "FormattedExcinfo",
          "COLLECT_FAKEMODULE_ATTRIBUTES",
          "ReprFailDoctest",
          "syspathinsert",
          "CallSpec2",
          "_is_started",
          "_checkversion",
          "_match_lines",
          "istext",
          "_remove_ansi_escape_sequences",
          "xml_key",
          "resolve_fixture_function",
          "from_item",
          "getvalueorskip",
          "visit_Attribute",
          "chdir",
          "_initrequest",
          "_should_rewrite",
          "add_funcarg_pseudo_fixture_def",
          "deprecated",
          "_resume",
          "_warn_incompatibility_with_xunit2",
          "ReprTracebackNative",
          "_check_initialpaths_for_relpath",
          "pos",
          "_log_cli_enabled",
          "register",
          "_getparser",
          "deactivate_fixture",
          "getitems",
          "pytest_assertion_pass",
          "_from_json",
          "getplugin",
          "failed",
          "_handle_teardown_sections",
          "is_generator",
          "location",
          "UnitTestCase",
          "_try_repr_or_str",
          "_main",
          "FuncFixtureInfo",
          "_saferepr",
          "get_config",
          "get_scope_package",
          "FixtureLookupErrorRepr",
          "assign",
          "_truncate_explanation",
          "tmp_path_factory",
          "ArgumentError",
          "_args_converter",
          "_idvalset",
          "getfixtureclosure",
          "params",
          "record_testsuite_property",
          "makeini",
          "freeze_support",
          "_W",
          "monkeypatch",
          "_strtobool",
          "_ensure_supporting_files",
          "summary_stats",
          "get_direct_param_fixture_func",
          "re_match_lines",
          "__contains__",
          "pytest_collectreport",
          "register_assert_rewrite",
          "name2fixturedefs",
          "ApproxSequencelike",
          "get_stderr_fileno",
          "isnosetest",
          "parse_warning_filter",
          "_get_plugin_specs_as_list",
          "install_importhook",
          "async_warn_and_skip",
          "addini",
          "faulthandler",
          "get_lines_after",
          "resolve_collection_argument",
          "__take_sys_modules_snapshot",
          "getsource",
          "getcall",
          "builtin",
          "CHECKER_CLASS",
          "importlib",
          "fail_fixturefunc",
          "pytest_ignore_collect",
          "_opentestcase",
          "scope2index",
          "PytestDeprecationWarning",
          "consider_module",
          "hookimpl",
          "summary_warnings",
          "_report_unserialization_failure",
          "safe_isclass",
          "__init__",
          "__array_priority__",
          "__all__",
          "KNOWN_TYPES",
          "_try_load_conftest",
          "_NodeType",
          "_fail",
          "fault_handler_originally_enabled_key",
          "TempPathFactory",
          "deselect_by_keyword",
          "_FixtureCachedResult",
          "showhelp",
          "isdict",
          "summary_failures",
          "spawn_pytest",
          "Failed",
          "__enter__",
          "istestfunction",
          "filename_arg",
          "recwarn",
          "assertion",
          "write_fspath_result",
          "unraisableexception",
          "code",
          "FunctionDefinition",
          "BaseReport",
          "get_exconly",
          "message",
          "_get_verbose_word",
          "getini",
          "_call_with_optional_argument",
          "check_ispytest",
          "matching_platform",
          "_write_content",
          "get_parametrized_fixture_keys",
          "readouterr",
          "processoption",
          "_early_rewrite_bailout",
          "fspath",
          "getcrashentry",
          "delattr",
          "fileno",
          "addSuccess",
          "DOCTEST_REPORT_CHOICE_CDIFF",
          "parseconfigure",
          "_PY_DIR",
          "skip",
          "fnmatch_lines",
          "Collector",
          "saferepr",
          "add_cleanup",
          "_getobj",
          "get_terminal_width",
          "_typ_map",
          "PytestAssertRewriteWarning",
          "startpath",
          "_is_capturing",
          "normalize_mark_list",
          "_wrapped_pdb_cls",
          "importorskip",
          "source",
          "reprtraceback",
          "CACHEDIR_TAG_CONTENT",
          "_esctable",
          "match",
          "MultipleDoctestFailures",
          "not_expr",
          "MarkDecorator",
          "ColoredLevelFormatter",
          "_report_to_json",
          "_compare_eq_cls",
          "done",
          "CallInfo",
          "pytest_plugins",
          "WarningsChecker",
          "fixture",
          "__attrs_post_init__",
          "_get_raw_skip_reason",
          "MonkeyPatch",
          "getfixturedefs",
          "legacy_path",
          "genitems",
          "timing",
          "_start",
          "fail",
          "filter_traceback",
          "get_data",
          "_get_allow_bytes_flag",
          "entrysep",
          "_set_main_color",
          "from_item_and_call",
          "_show_fixtures_per_test",
          "fix_cache_order",
          "Interrupted",
          "TResult",
          "getstatementrange",
          "newinstance",
          "_params_converter",
          "extraline",
          "iscoroutinefunction",
          "collect",
          "cmdline",
          "variable",
          "run",
          "_compare_eq_sequence",
          "DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE",
          "pytest_fixture_post_finalizer",
          "_validate_config_options",
          "collect_by_name",
          "getparent",
          "_set_opt_strings",
          "pytest_unconfigure",
          "get_records",
          "_get_increasing_order",
          "fixturenames",
          "Item",
          "DontReadFromInput",
          "text",
          "matchreport",
          "hasopt",
          "TESTS_FAILED",
          "verbosity",
          "_check_scope",
          "absolutepath",
          "_update_message",
          "ParameterSet",
          "raises",
          "from_parent",
          "format",
          "summary_errors",
          "_determine_show_progress_info",
          "getimfunc",
          "warnings",
          "caplog",
          "Markable",
          "_saved",
          "tmpdir",
          "_inject_setup_class_fixture",
          "CollectReport",
          "from_exc_info",
          "cacheprovider",
          "_fillfixtures",
          "when",
          "__slots__",
          "PYC_EXT",
          "reportinfo",
          "_format_trimmed",
          "_enter_pdb",
          "conftest_options",
          "_get_checker",
          "finish",
          "ExceptionRepr",
          "pytest_pyfunc_call",
          "_non_printable_ascii_translate_table",
          "rex_outcome",
          "pytest_addhooks",
          "tb",
          "DEFAULT_LOG_DATE_FORMAT",
          "update_testcase_duration",
          "python_api",
          "linecomp",
          "index",
          "repr_failure",
          "__array_ufunc__",
          "showheader",
          "pytest_markeval_namespace",
          "get_last_failed_paths",
          "wcswidth",
          "pytestPDB",
          "record_testreport",
          "_import_pdb_cls",
          "_outrep_summary",
          "pastebinfile_key",
          "MatcherAdapter",
          "listoutcomes",
          "getinicfg",
          "_approx_scalar",
          "_yield_comparisons",
          "DOCTEST_REPORT_CHOICES",
          "junitxml",
          "ReprTraceback",
          "set_fixture",
          "ARGUMENT_TYPE_STR",
          "TYPE_CHECKING",
          "pytest_configure",
          "CollectErrorRepr",
          "stepwise",
          "make_hook_recorder",
          "restore",
          "finish_recording",
          "RE_IMPORT_ERROR_NAME",
          "istestfunc",
          "IGNORE_PAM",
          "expression",
          "MARK_GEN",
          "finalize",
          "Pytester",
          "directory_arg",
          "locals",
          "_get_progress_information_message",
          "findsource",
          "Expression",
          "pytest_runtest_protocol",
          "CaptureResult",
          "T",
          "typename",
          "perform_collect",
          "OR",
          "traceback",
          "is_capturing",
          "MarkGenerator",
          "LogCaptureFixture",
          "pytest_collection_finish",
          "LogCaptureHandler",
          "pytest_collectstart",
          "Exception",
          "argnames",
          "CaptureManager",
          "isinitpath",
          "pytest",
          "INTERNAL_ERROR",
          "visit_Assert",
          "resume",
          "NOT",
          "TeeCaptureIO",
          "_do_configure",
          "on_rm_rf_error",
          "exec_module",
          "get_fslocation_from_item",
          "_FileHandler",
          "DummyRewriteHook",
          "_FixtureFunc",
          "parse_num",
          "_is_last_item",
          "unraisable_exception_runtest_hook",
          "UnformattedWarning",
          "_validate_args",
          "_warn_about_missing_assertion",
          "getnode",
          "_showfixtures_main",
          "_add_stats",
          "parse_hookspec_opts",
          "iter_markers",
          "getstatement",
          "item_capture",
          "LoggingPlugin",
          "reason",
          "_ignore_error",
          "lines",
          "is_async_function",
          "import_plugin",
          "_assert_expr_to_lineno",
          "pytest_pycollect_makemodule",
          "stop_capturing",
          "pytest_report_header",
          "_notin_text",
          "_no_match_line",
          "append_pass",
          "record_xml_attribute",
          "prepend",
          "E",
          "DEFAULT_MAX_CHARS",
          "tolerance",
          "_fill_fixtures_impl",
          "_getreprcrash",
          "_get_override_ini_value",
          "get_terminal_writer",
          "_find_parametrized_scope",
          "_LiveLoggingNullHandler",
          "getrepr",
          "SysCapture",
          "scopenum_function",
          "FixtureLookupError",
          "frame",
          "imply_paths_hooks",
          "names",
          "own_mark_names",
          "EncodedFile",
          "Mark",
          "mark_rewrite",
          "LEVELNAME_FMT_REGEX",
          "__exit__",
          "get_empty_parameterset_mark",
          "LineMatcher",
          "_resolve_arg_value_types",
          "_recursive_debug",
          "parseoutcomes",
          "isattrs",
          "encoding",
          "DOCTEST_REPORT_CHOICE_NDIFF",
          "maybe_wrap_pytest_function_for_tracing",
          "pytest_enter_pdb",
          "store_mark",
          "test_tmproot",
          "_EmptyClass",
          "_bytes_to_ascii",
          "consider_preparse",
          "_get_report_choice",
          "LOGLEVEL_COLOROPTS",
          "runpytest",
          "check_testcase_implements_trial_reporter",
          "MyOptionParser",
          "assertrepr_compare",
          "_assert_state",
          "_show_fixture_action",
          "RaisesContext",
          "__init_subclass__",
          "store",
          "hookspec",
          "list",
          "_matches_prefix_or_glob_option",
          "_format_assertmsg",
          "sep",
          "add_marker",
          "PytestExperimentalApiWarning",
          "generic_visit",
          "make_properties_node",
          "mkdir",
          "issue_config_time_warning",
          "get_sections",
          "_getini",
          "make_numbered_dir",
          "PytestUnraisableExceptionWarning",
          "ParseError",
          "setattr",
          "exconly",
          "_repr_failure_py",
          "_recursive_list_map",
          "_report_keyboardinterrupt",
          "Parser",
          "FDCaptureBinary",
          "_ensure_unconfigure",
          "check_interactive_exception",
          "WARNING_CAPTURED_HOOK",
          "DEFAULT_LOG_FORMAT",
          "passed",
          "repr",
          "_check_if_assertion_pass_impl",
          "_assertion_supported",
          "_force_symlink",
          "__len__",
          "catching_logs",
          "style",
          "eval",
          "_read_pyc",
          "print_teardown_sections",
          "NFPlugin",
          "_format",
          "_Key",
          "DOCTEST_REPORT_CHOICE_UDIFF",
          "idmaker",
          "show_test_item",
          "rewrite",
          "toterminal",
          "nodes",
          "_in_suspended",
          "LFPluginCollSkipfiles",
          "_get_flag_lookup",
          "_io",
          "pytester_assertions",
          "builtin_plugins",
          "_parse_parametrize_args",
          "getargs",
          "commonpath",
          "old_mark_config_key",
          "addSkip",
          "symlink_or_skip",
          "assert_contains",
          "_get_auto_indent",
          "capsysbinary",
          "reject",
          "fullwidth",
          "FastFilesCompleter",
          "RPAREN",
          "append_collect_error",
          "logging",
          "ParsedCall",
          "IDENT_PREFIX",
          "parametrize",
          "__str__",
          "_traceback",
          "_get_pos",
          "_sys_snapshot",
          "issequence",
          "suspend_fixture",
          "runtestprotocol",
          "f_globals",
          "_should_truncate_item",
          "indent",
          "parse_args",
          "skipped",
          "_hook",
          "id",
          "getgroup",
          "num_mock_patch_args",
          "cwd_relative_nodeid",
          "setuponly",
          "_prepare_content",
          "ishidden",
          "pytest_warning_recorded",
          "_config",
          "REPORT_COLLECTING_RESOLUTION",
          "ascii_escaped",
          "_printcollecteditems",
          "str",
          "LFPluginCollWrapper",
          "expr",
          "__version_tuple__",
          "set_session",
          "showlocals",
          "error",
          "ids",
          "_replace",
          "warns",
          "__version__",
          "raiseerror",
          "FixtureRequest",
          "no_summary",
          "pathlib",
          "_code",
          "__module__",
          "astcache",
          "_iter_rewritable_modules",
          "SafeRepr",
          "HookRecorder",
          "README_CONTENT",
          "visit_Compare",
          "record_property",
          "resolve",
          "simple",
          "reorder_items_atscope",
          "read",
          "_validate_usepdb_cls",
          "_write_pyc_fp",
          "_getscopeitem",
          "show_fixtures_per_test",
          "runpython",
          "visit_BinOp",
          "suspend_capturing",
          "_get_maxsize_for_saferepr",
          "Class",
          "set_log_path",
          "_postmortem_traceback",
          "_getpytestargs",
          "_set_initial_conftests",
          "re_match_lines_random",
          "excinfo",
          "CaptureFixture",
          "PrintHelp",
          "cache",
          "messages",
          "capstdout",
          "WarningReport",
          "_parse_parametrize_parameters",
          "fromdictargs",
          "PYC_TAIL",
          "runpytest_inprocess",
          "DEFAULT_MAX_LINES",
          "summary_passes",
          "_seen",
          "SubRequest",
          "setup",
          "OK",
          "showfspath",
          "getfixturevalue",
          "EMPTY_BUFFER",
          "_addexcinfo",
          "Metafunc",
          "fnmatch_ex",
          "ApproxMapping",
          "_get_active_fixturedef",
          "visit_Call",
          "_determine_main_color",
          "_R",
          "LineComp",
          "_striptext",
          "isdatacls",
          "SysCaptureBinary",
          "find_suffixes",
          "LFPlugin",
          "count_towards_summary",
          "autouse",
          "CLOSE_STDIN",
          "cached_result",
          "PytestConfigWarning",
          "add_report_section",
          "flow_marker",
          "pop",
          "DEFAULT_RELATIVE_TOLERANCE",
          "_imply_path",
          "clear_cache",
          "deindent",
          "_check_non_top_pytest_plugins",
          "capfdbinary",
          "Function",
          "section",
          "ARGUMENT_PERCENT_DEFAULT",
          "addUnexpectedSuccess",
          "_pformat_dispatch",
          "repr_instance",
          "_compare_eq_any",
          "ReprEntryNative",
          "doctest",
          "_get_continue_on_failure",
          "stop",
          "_REPORTCHARS_DEFAULT",
          "MultiCapture",
          "post_mortem",
          "countoutcomes",
          "suspend_global_capture",
          "helper",
          "_report_kwargs_from_json",
          "getoption",
          "append_failure",
          "add_property",
          "write_ensure_prefix",
          "inline_run",
          "_ensure_immutable_ids",
          "teardown_exact",
          "delenv",
          "NotSetType",
          "copy_example",
          "assert_contains_lines",
          "_build_normal_summary_stats_line",
          "truncate_locals",
          "append",
          "FSCollector",
          "mangle_test_address",
          "value",
          "NodeMeta",
          "parse_known_args",
          "pytest_generate_tests",
          "PYTEST_COLLECT_MODULE",
          "FSHookProxy",
          "kwargs",
          "cache_dir_from_config",
          "Frame",
          "catch_threading_exception",
          "makepyprojecttoml",
          "_is_skipped",
          "IGNORED_ATTRIBUTES",
          "ImportMode",
          "assert_never",
          "head_line",
          "pytest_sessionstart",
          "find_prefixed",
          "Testdir",
          "PathAwareHookProxy",
          "get_real_func",
          "get_plugin_manager",
          "_basetemp",
          "repr_args",
          "_cachedir",
          "_patch_unwrap_mock_aware",
          "make_numbered_dir_with_cleanup",
          "_match_lines_random",
          "relline",
          "startTest",
          "_IGNORED_ERRORS",
          "inifile",
          "getcalls",
          "errisinstance",
          "_check_type",
          "_state",
          "USAGE_ERROR",
          "_inject_setup_method_fixture",
          "get",
          "_mark_plugins_for_rewrite",
          "reprfuncargs",
          "add_stats",
          "fill_unfilled",
          "_plugin_nameversions",
          "ensure_deletable",
          "_compare_eq_iterable",
          "NO_TESTS_COLLECTED",
          "getfuncargnames",
          "getreportopt",
          "evaluate_xfail_marks",
          "MoreQuietAction",
          "pytest_collection_modifyitems",
          "AssertionState",
          "showlongtestinfo",
          "FixtureFunctionMarker",
          "_get_assertion_exprs",
          "_non_numeric_type_error",
          "_F",
          "console_main",
          "startdir",
          "iter_markers_with_node",
          "PYTEST_TAG",
          "consider_pluginarg",
          "determine_setup",
          "Traceback",
          "threadexception",
          "scopes",
          "_suspend",
          "_node_location_to_relpath",
          "safe_getattr",
          "abspath",
          "PytestCollectionWarning",
          "_LiveLoggingStreamHandler",
          "extract_from",
          "YIELD_FIXTURE",
          "_rget_with_confmod",
          "setmulti2",
          "tracebackcutdir",
          "_consider_importhook",
          "filter_traceback_for_conftest_import_failure",
          "import_path",
          "global_and_fixture_disabled",
          "statement",
          "_AnyPurePath",
          "clear",
          "TokenType",
          "parsefactories",
          "get_extended_length_path_str",
          "BINOP_MAP",
          "create_cleanup_lock",
          "_NodeReporter",
          "from_function",
          "__delitem__",
          "version_tuple",
          "longrepr",
          "bin_xml_escape",
          "_names",
          "_build_collect_only_summary_stats_line",
          "context",
          "short_test_summary",
          "__next__",
          "format_session_duration",
          "FDCapture",
          "name2pseudofixturedef_key",
          "get_option_ini",
          "runtest",
          "D",
          "DoctestTextfile",
          "report_collect",
          "addsection",
          "write_captured_output",
          "fail_marker",
          "is_globally_capturing",
          "funcargs",
          "V",
          "get_public_names",
          "_compare_eq_set",
          "_make_xunit_fixture",
          "to_xml",
          "suspend",
          "_version",
          "get_location",
          "Argument",
          "path",
          "default_plugins",
          "USAGE_MSG",
          "_get_line_with_reprcrash_message",
          "_ensure_basetemp",
          "addError",
          "call_optional",
          "_PluggyPlugin",
          "hasinit",
          "pluralize",
          "_warn_already_imported",
          "pytest_report_from_serializable",
          "SEP",
          "start_global_capturing",
          "_addfinalizer",
          "ReprEntry",
          "funcnamefilter",
          "safeformat",
          "find_spec",
          "_makepath",
          "_format_action_invocation",
          "_getvaluepath",
          "reprfileloc",
          "compile",
          "getfailures",
          "SysPathsSnapshot",
          "_T",
          "_get_global_properties_node",
          "__dir__",
          "getpathnode",
          "_runtest_for",
          "Instance",
          "warning_record_to_str",
          "_warn_or_fail_if_strict",
          "token",
          "pytest_report_teststatus",
          "ApproxBase",
          "skipping",
          "Session",
          "addinivalue_line",
          "FixtureValue",
          "Exit",
          "FixtureFunction",
          "path_matches_patterns",
          "makepyfile",
          "Source",
          "_get_runner",
          "getparam",
          "PercentStyleMultiline",
          "PytestPluginManager",
          "pytest_collection",
          "__setitem__",
          "_preparse",
          "_running_on_ci",
          "pop_outerr_to_orig",
          "create_module",
          "_assertion_pass",
          "iterparentnodeids",
          "_teardown_yield_fixture",
          "_get_multicapture",
          "stream",
          "close",
          "at_level",
          "inipath",
          "_prepareconfig",
          "PyCollector",
          "DoctestModule",
          "_init_pdb",
          "_inject_setup_function_fixture",
          "and_expr",
          "start",
          "outcome",
          "_ANSI_ESCAPE_SEQ",
          "patchsysdict",
          "_processopt",
          "longreprtext",
          "TempdirFactory",
          "getfslineno",
          "_ET",
          "wrap_session",
          "maybe_delete_a_numbered_dir",
          "_has_param_ids",
          "pytest_fixture_setup",
          "visit_UnaryOp",
          "pytest_runtest_logreport",
          "ihook",
          "rm_rf",
          "_WithException",
          "emit",
          "_log",
          "reprentries",
          "pytest_runtest_teardown",
          "add_global_property",
          "_split_lines",
          "ApproxDecimal",
          "getvalue",
          "wcwidth",
          "pytest_load_initial_conftests",
          "pytest_terminal_summary",
          "spawn",
          "no_fnmatch_line",
          "markup",
          "outcomes",
          "sections",
          "addoption",
          "result",
          "_compute_fixture_value",
          "ApproxScalar",
          "from_call",
          "getmodpath",
          "_CACHE_PREFIX_DIRS",
          "_importconftest",
          "resume_capturing",
          "_tmppath_factory",
          "reprcrash",
          "module",
          "_prunetraceback",
          "reorder_items",
          "rootdir",
          "_ascii_escaped_by_config",
          "start_capturing",
          "freeze_includes",
          "pastebin",
          "invocation_dir",
          "_fillfuncargs",
          "_get_first_non_fixture_func",
          "PytestArg",
          "xfail",
          "K",
          "node_reporter",
          "tbfilter",
          "_inject_setup_module_fixture",
          "write_line",
          "locate_config",
          "parseconfig",
          "gethookproxy",
          "STEPWISE_CACHE_DIR",
          "name",
          "pytest_itemcollected",
          "_add_simple",
          "deprecated_call",
          "append_collect_skipped",
          "_collectfile",
          "_color_for_type",
          "register_cleanup_lock_removal",
          "pytest_plugin_registered",
          "pytest_runtest_call",
          "for_config",
          "PseudoFixtureDef",
          "_reprcompare",
          "pytest_cmdline_parse",
          "_getcrashline",
          "handler",
          "_get_number_flag",
          "_S",
          "INTERRUPTED",
          "_readline_workaround",
          "WARNS_NONE_ARG",
          "exitstatus",
          "_ensure_relative_to_basetemp",
          "_check_record_param_type",
          "_PLUGGY_DIR",
          "_getentrysource",
          "_py36_windowsconsoleio_workaround",
          "_HandlerType",
          "create_new_paste",
          "prune_dependency_tree",
          "from_config",
          "pytest_assertrepr_compare",
          "NoCapture",
          "ExceptionInfo",
          "_truncate_recursive_traceback",
          "_param_ids_generated",
          "_format_args",
          "should_do_markup",
          "Config",
          "pytest_runtestloop",
          "_get_main_color",
          "yield_fixture",
          "_setupstate",
          "_warn_about_skipped_plugins",
          "_resolve_arg_ids",
          "_colorama_workaround",
          "RunResult",
          "_log_text",
          "width_of_current_line",
          "pytest_runtest_logfinish",
          "_write_progress_information_filling_space",
          "isnamedtuple",
          "resolve_from_str",
          "readlines",
          "__getitem__",
          "parse_hookimpl_opts",
          "_get_allow_unicode_flag",
          "Token",
          "_mk_tmp",
          "_importtestmodule",
          "write_docstring",
          "HOOK_LEGACY_PATH_ARG",
          "addFailure",
          "setdefault",
          "PyobjMixin",
          "combined_with",
          "_makefile",
          "unittest",
          "lineno",
          "__lt__",
          "getstatementrange_ast",
          "pytest_sessionfinish",
          "getfirstlinesource",
          "PytestUnknownMarkWarning",
          "scope",
          "AlwaysDispatchingPrettyPrinter",
          "repr_traceback",
          "TEST_OUTCOME",
          "parse_known_and_unknown_args",
          "bestrelpath",
          "runner",
          "Xfail",
          "applymarker",
          "_pytest",
          "PdbInvoke",
          "_diff_text",
          "DropShorterLongHelpFormatter",
          "pytest_cmdline_main",
          "_pluginmanager",
          "pytest_cmdline_preparse",
          "formatrepr",
          "_create",
          "isiterable",
          "deselect_by_mark",
          "pytest_make_parametrize_id",
          "pytest_internalerror",
          "LPAREN",
          "get_open_files",
          "pytest_report_collectionfinish",
          "families",
          "resolve_package_path",
          "_PYTEST_DIR",
          "main",
          "TracebackEntry",
          "_param_ids_from",
          "_getnextfixturedef",
          "getfailedcollections",
          "KeywordMatcher",
          "chain",
          "get_timeout_config_value",
          "add_attribute",
          "_repr_compare",
          "DEFAULT_ABSOLUTE_TOLERANCE",
          "set_level",
          "TerminalWriter",
          "call_and_report",
          "visit_Starred",
          "catch_warnings_for_item",
          "DOCTEST_REPORT_CHOICE_NONE",
          "get_dirs_from_args",
          "mktemp",
          "catch_unraisable_exception",
          "__ne__",
          "InvocationParams",
          "copy",
          "__test__",
          "OutcomeException",
          "pytestconfig",
          "capstderr",
          "_get_reports_to_display",
          "_ensure_removed_sysmodule",
          "from_current",
          "isset",
          "stardir",
          "caplog_records_key",
          "markname",
          "_given_basetemp",
          "popen",
          "module_name_from_path",
          "_split_explanation",
          "_assert_start_repr",
          "HelpAction",
          "consider_conftest",
          "execute",
          "UNARY_MAP",
          "nextitem",
          "approx",
          "_addoption_instance",
          "_color_for_type_default",
          "LineMatcher_fixture",
          "plugins",
          "_init_checker_class",
          "_excinfo",
          "assertstate_key",
          "derive_importpath",
          "_testcase",
          "_iter_all_modules",
          "category",
          "isatty",
          "_to_json",
          "LsofFdLeakChecker",
          "__eq__",
          "_recurse",
          "getrawcode",
          "args",
          "handleError",
          "pop_format_context",
          "CFG_PYTEST_SECTION",
          "request",
          "REGEX_TYPE",
          "listchain",
          "_format_boolop",
          "nofuncargs",
          "_in_venv",
          "AssertionRewriter",
          "TimeoutExpired",
          "_call_reprcompare",
          "_format_lines",
          "__missing__",
          "_finalize",
          "File",
          "_call_assertion_pass",
          "_matchfactories",
          "addfinalizer",
          "_getconftestmodules",
          "getfixturemarker",
          "PytestWarning",
          "for_later",
          "_genfunctions",
          "_idval",
          "template",
          "parse_setoption",
          "get_statement_startend2",
          "listnames",
          "_result",
          "getbasetemp",
          "FILE_OR_DIR",
          "_trace",
          "EMPTY_PARAMETERSET_OPTION",
          "readline",
          "getreports",
          "visit_Name",
          "_setup_fixtures",
          "TerminalReporter",
          "__hash__",
          "consider_env",
          "is_rewrite_disabled",
          "call_fixture_func",
          "thread_exception_runtest_hook",
          "gethookrecorder",
          "get_log_level_for_setting",
          "strict",
          "repr_excinfo",
          "ExitCode",
          "syspath_prepend",
          "cache_key",
          "insert_missing_modules",
          "is_started",
          "rootpath",
          "set_trace",
          "__iter__",
          "XFailed",
          "mark",
          "pytest_pycollect_makeitem",
          "EOF",
          "MarkMatcher",
          "initialnames",
          "UsageError",
          "_getfailureheadline",
          "fixtures",
          "python",
          "rex_session_duration",
          "PdbTrace",
          "record_tuples",
          "_dump_lines",
          "AssertionRewritingHook",
          "STRICT_OPTION",
          "call_runtest_hook",
          "Notset",
          "visit",
          "setitem",
          "LOCK_TIMEOUT",
          "_schedule_finalizers",
          "_getlines",
          "cacheshow",
          "TerminalRepr",
          "_fixturemanager",
          "attrs",
          "snap",
          "undo",
          "ARGUMENT_TYPE_STR_CHOICE",
          "_check_all_skipped",
          "_validate_plugins",
          "_is_mocked",
          "istestclass",
          "f_locals",
          "buffer",
          "validate_basetemp",
          "function",
          "set_repr_style",
          "_addoption",
          "LEGACY_PATH",
          "CaptureIO",
          "__getattr__",
          "ReprFuncArgs",
          "MINUS_K_COLON",
          "FSCOLLECTOR_GETHOOKPROXY_ISINITPATH",
          "append_skipped",
          "wrap_function_to_error_out_if_called_directly",
          "Skip",
          "OptionGroup",
          "runitem",
          "collect_one_node",
          "search_pypath",
          "format_explanation",
          "ReprLocals",
          "count",
          "get_real_method",
          "rewrite_asserts",
          "evaluate",
          "runpytest_subprocess",
          "PytestUnhandledCoroutineWarning",
          "get_cache_dir",
          "append_error",
          "testdir",
          "pytest_keyboard_interrupt",
          "_width_of_current_line",
          "display",
          "_bestrelpath_cache",
          "_truncate_by_char_count",
          "create_terminal_writer",
          "pytester",
          "cut",
          "pytest_collect_file",
          "set_when",
          "STRING_TYPES",
          "FixtureDef",
          "capture",
          "Package",
          "_write_report_lines_from_hooks",
          "xfailed_key",
          "_translate_non_printable",
          "apply_warning_filters",
          "_highlight",
          "records",
          "node",
          "_disable_output_capturing_for_darwin",
          "truncate_if_required",
          "strip",
          "_getindent",
          "scopemismatch",
          "filter",
          "makefile",
          "get_user",
          "read_global_capture",
          "RUNNER_CLASS",
          "reports",
          "MINUS_K_DASH",
          "load_config_dict_from_file",
          "TestReport",
          "SetupState",
          "_surrounding_parens_on_own_lines",
          "flush",
          "pytest_runtest_logstart",
          "cleanup_numbered_dir",
          "config",
          "extract_suffixes",
          "_folded_skips",
          "tmp_path",
          "getfixtureinfo",
          "obj",
          "WarningsRecorder",
          "parse",
          "parse_summary_nouns",
          "_should_repr_global_name",
          "NOTSET",
          "inline_genitems",
          "set_location",
          "Scanner",
          "_compare_eq_dict",
          "with_args",
          "stopTest",
          "IDENT",
          "_eval_scope_callable",
          "delitem",
          "param",
          "get_scope_node",
          "__repr__",
          "getitem",
          "_format_repr_exception",
          "nose",
          "capfd",
          "ensure_extended_length_path",
          "CwdSnapshot",
          "DEFAULT_REPR_MAX_SIZE",
          "get_optionflags",
          "_is_doctest",
          "DoctestItem",
          "pytest_leave_pdb",
          "_pyfuncitem",
          "_is_setup_py",
          "_write_entry_lines",
          "hasnew",
          "parts",
          "get_common_ancestor",
          "teardown",
          "cleanup_candidates",
          "nodeid"
        ]
      },
      "response": {
        "keywords": [
          "skip",
          "session",
          "pytest",
          "raises",
          "Module",
          "evaluate_skip_marks",
          "mark",
          "pytest_configure",
          "skip_module",
          "setup"
        ]
      }
    },
    {
      "name": "FilesKeywordSearch",
      "metadata": {
        "start": "2024-08-19T13:46:22.282229",
        "usage": "259.51313972473145 ms"
      },
      "request": {
        "tokens": [
          "skip",
          "session",
          "pytest",
          "raises",
          "Module",
          "evaluate_skip_marks",
          "mark",
          "pytest_configure",
          "skip_module",
          "setup"
        ]
      },
      "response": {
        "results": {
          "pytest": [
            "src/_pytest/deprecated.py",
            "src/_pytest/faulthandler.py",
            "src/_pytest/logging.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/pastebin.py",
            "src/pytest/__init__.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/pytester_assertions.py",
            "src/_pytest/recwarn.py",
            "src/_pytest/skipping.py",
            "src/pytest/collect.py",
            "src/_pytest/pytester.py",
            "src/_pytest/monkeypatch.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/main.py",
            "src/_pytest/timing.py",
            "src/_pytest/warnings.py",
            "src/_pytest/unraisableexception.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/capture.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/unittest.py",
            "src/_pytest/nodes.py",
            "src/pytest/__main__.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/runner.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/threadexception.py",
            "src/_pytest/compat.py",
            "src/_pytest/debugging.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/config/argparsing.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/python_api.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/freeze_support.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/mark/__init__.py"
          ],
          "skip": [
            "src/_pytest/deprecated.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/skipping.py",
            "src/_pytest/pytester.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/unittest.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/config/exceptions.py"
          ],
          "session": [
            "src/_pytest/deprecated.py",
            "src/_pytest/logging.py",
            "src/_pytest/pastebin.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/pytester.py",
            "src/_pytest/main.py",
            "src/_pytest/warnings.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/nodes.py",
            "src/_pytest/unittest.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/runner.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/fixtures.py"
          ],
          "setup": [
            "src/_pytest/logging.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/python.py",
            "src/_pytest/nose.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/pytester.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/capture.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/nodes.py",
            "src/_pytest/unittest.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/_argcomplete.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/runner.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/debugging.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/fixtures.py"
          ],
          "pytest_configure": [
            "src/_pytest/skipping.py",
            "src/_pytest/pytester.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/faulthandler.py",
            "src/_pytest/logging.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/pastebin.py",
            "src/_pytest/terminal.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/warnings.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/debugging.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/mark/__init__.py"
          ],
          "mark": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/python_api.py",
            "src/_pytest/warnings.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/nodes.py",
            "src/_pytest/mark/__init__.py"
          ],
          "raises": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/python_api.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/nodes.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/threadexception.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/debugging.py",
            "src/_pytest/recwarn.py"
          ],
          "Module": [
            "src/_pytest/store.py",
            "src/pytest/collect.py",
            "src/_pytest/doctest.py",
            "src/_pytest/main.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/runner.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/fixtures.py"
          ],
          "evaluate_skip_marks": [
            "src/_pytest/skipping.py"
          ]
        }
      }
    },
    {
      "name": "FilesFilter",
      "metadata": {
        "start": "2024-08-19T13:46:22.541753",
        "usage": "5300.379991531372 ms"
      },
      "request": {
        "keyword_search_results": {
          "pytest": [
            "src/_pytest/deprecated.py",
            "src/_pytest/faulthandler.py",
            "src/_pytest/logging.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/pastebin.py",
            "src/pytest/__init__.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/pytester_assertions.py",
            "src/_pytest/recwarn.py",
            "src/_pytest/skipping.py",
            "src/pytest/collect.py",
            "src/_pytest/pytester.py",
            "src/_pytest/monkeypatch.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/main.py",
            "src/_pytest/timing.py",
            "src/_pytest/warnings.py",
            "src/_pytest/unraisableexception.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/capture.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/unittest.py",
            "src/_pytest/nodes.py",
            "src/pytest/__main__.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/runner.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/threadexception.py",
            "src/_pytest/compat.py",
            "src/_pytest/debugging.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/config/argparsing.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/python_api.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/freeze_support.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/mark/__init__.py"
          ],
          "skip": [
            "src/_pytest/deprecated.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/skipping.py",
            "src/_pytest/pytester.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/unittest.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/config/exceptions.py"
          ],
          "session": [
            "src/_pytest/deprecated.py",
            "src/_pytest/logging.py",
            "src/_pytest/pastebin.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/pytester.py",
            "src/_pytest/main.py",
            "src/_pytest/warnings.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/nodes.py",
            "src/_pytest/unittest.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/runner.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/fixtures.py"
          ],
          "setup": [
            "src/_pytest/logging.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/python.py",
            "src/_pytest/nose.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/pytester.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/capture.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/nodes.py",
            "src/_pytest/unittest.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/_argcomplete.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/runner.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/debugging.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/terminal.py",
            "src/_pytest/reports.py",
            "src/_pytest/fixtures.py"
          ],
          "pytest_configure": [
            "src/_pytest/skipping.py",
            "src/_pytest/pytester.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/faulthandler.py",
            "src/_pytest/logging.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/pastebin.py",
            "src/_pytest/terminal.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/warnings.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/debugging.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/mark/__init__.py"
          ],
          "mark": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/python_api.py",
            "src/_pytest/warnings.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/nodes.py",
            "src/_pytest/mark/__init__.py"
          ],
          "raises": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/python_api.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/nodes.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/threadexception.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/debugging.py",
            "src/_pytest/recwarn.py"
          ],
          "Module": [
            "src/_pytest/store.py",
            "src/pytest/collect.py",
            "src/_pytest/doctest.py",
            "src/_pytest/main.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/runner.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/fixtures.py"
          ],
          "evaluate_skip_marks": [
            "src/_pytest/skipping.py"
          ]
        }
      },
      "response": {
        "scratch_pad": "The issue is about improving the handling of module-level skips in pytest when using syntax permissible in Python 3.8 but not in previous versions. The skip functionality is related to the usage of \"skip\" and related keywords such as \"raises\", \"pytestmark\", and the handling of module-level functions.\n\nBased on the issue description, starting points for finding relevant files can involve the keywords: \n- 'pytest' which will likely involve core modules for functionality including skipping\n- 'skip' as it is directly related to the described issue where skipping modules by Python version is concerned.\n\nLet's match these with the file paths:\n1. 'src/_pytest/skipping.py' would be where the skip logic is handled as it directly matches the 'skip' keyword and relates to skipping tests.\n2. 'src/_pytest/outcomes.py' could involve outcomes related to skipping and is worth checking as it matches with 'skip' and 'raises'.\n3. 'src/_pytest/mark/structures.py' relates to `@pytest.mark.skip` annotations which are mentioned as the alternate feature that users are suggested to use.\n4. 'src/pytest/__init__.py' potentially covers global initializations or settings that might relate to default behaviors around skipping.\n5. 'src/_pytest/logging.py' may capture logging improvements for better error message clarity.\n6. 'src/_pytest/config/__init__.py' might include configuration for module-level settings.\n\nThe most relevant files will likely be those directly managing skipping and error handling for clear documentation or alternate API suggestions.",
        "file_paths": [
          "src/_pytest/skipping.py",
          "src/_pytest/outcomes.py",
          "src/_pytest/mark/structures.py",
          "src/pytest/__init__.py",
          "src/_pytest/config/__init__.py",
          "src/_pytest/logging.py"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:27.842148",
        "usage": "7230.880975723267 ms"
      },
      "request": {
        "file_path": "src/_pytest/skipping.py"
      },
      "response": {
        "scratch_pad": "1. **Understanding the Issue:**\n   - The user is facing a problem with handling module-level skips in pytest. They want to skip an entire module when the Python version is older than 3.8 due to syntax incompatibilities.\n   - The issue arises because placing a `skip()` directly in the module code results in errors since pytest does not allow skipping outside of a test, and `pytestmark` does not work because the Python parser processes the code before pytest gets to evaluate the version.\n   - They suggest adding clarity in error messages or possibly a new API `skip_module()`.\n\n2. **Reproducing the Issue:**\n   - Using `skip()` directly in the module leads to \"skipping outside a test\" error.\n   - Placing `pytestmark` fails since the module is parsed by an older Python version before pytest determines to skip due to `skipif`.\n\n3. **Cause of Issue:**\n   - The problem is that pytest's current skipping mechanisms (`pytestmark`, `skip()`, `skipif`) do not suit scenarios where module-level code is incompatible with earlier Python versions as parsing happens before any test-level decorators or markers are evaluated.\n\n4. **User Expectation:**\n   - The user wishes to skip entire modules based on version condition without running into evaluation issues caused by Python parsing prior to pytest execution.\n\n5. **Possible Solutions:**\n   - Improve error messaging to guide users correctly to use `allow_module_level=True` when necessary.\n   - Introduce a new API to handle module-level skips more gracefully, e.g., `skip_module(\"reason\")`.\n\n---\n\nEvaluating the `src/_pytest/skipping.py` file:\n\n- The file contains functions relating to the handling of skip and xfail markers in pytest.\n- Functions like `evaluate_skip_marks`, `evaluate_xfail_marks`, and `pytest_runtest_setup` are relevant as they deal directly with the evaluation and setup of skipped tests and marks.\n- However, nothing in this file directly supports or denies module-level skips with Python syntax-level issues as described in the issue because pytest's markdown and options configurations and not the module parsing mechanics themselves.\n\nTherefore, this file contains relevant code but isn't the root cause. Potential solutions would need to involve new API structures or error message updates.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [],
        "relevant_symbols": [
          "evaluate_skip_marks",
          "evaluate_xfail_marks",
          "pytest_runtest_setup"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:35.073044",
        "usage": "15384.867906570435 ms"
      },
      "request": {
        "file_path": "src/_pytest/outcomes.py"
      },
      "response": {
        "scratch_pad": "1. **Analyze User Instructions:**\n   - **1.1 Issue Description:** The user is attempting to skip tests at the module level when using features specific to Python 3.8 in an environment potentially using older Python versions (like 3.6, 3.7).\n   - **1.2 Steps Provided:**\n     - User tries to use `skip()` function directly at module level as a naive solution and encounters an error.\n     - They propose improvements for better handling in such scenarios.\n   - **1.3 How was this issue caused?** \n     - This issue is likely caused because the error message indicates using `skip` outside of a test context is not allowed without `allow_module_level=True`. The error handling message discourages the way that allows skipping at the module level.\n   - **1.4 User Expectation & Cause:**\n     - The user expects clear guidance or an alternative to skip tests in modules leveraging 3.8 features on older Python versions. Misleading error messages or lack of an adequate API makes this confusing.\n   - **1.5 Possible Solutions: Proposed by user**\n     - Suggest using `allow_module_level` for clarity in error messages.\n     - Introduce a new API `skip_module(\"reason\")` to handle use cases better.\n\n2. **File Evaluation:**\n   - The file content provided is for `src/_pytest/outcomes.py`.\n   - The file contains classes and methods related to exception handling for test outcomes.\n   - **Relevant Symbols/Analysis:**\n     - The 'skip' function is directly related to the user's issue as it allows skipping tests. This function already supports `allow_module_level`, which is what the user proposes to improve on or clarify.\n     - `Skipped` class: Implemented to handle skip exceptions and includes support for `allow_module_level`.\n     - The current implementation seems to already support skipping at module level but lacks clarity in error messages, as noted.\n\n   - **Overall Relevance to Issue:**\n     - The file is relevant regarding handling the skip logic and exceptions. The user's issue largely points out documentation or messaging improvements from `skip()` and potentially extending functionality to better support module-level skipping with an explicit API.\n     - Proposed changes would likely involve updating error messages or introducing the suggested API within this file.\n\n3. **Conclusion:**\n   - The file is related to the issue but not necessarily the root cause as it contains mechanisms that the user can utilize but perhaps not effectively conveyed.\n   - Addressing the user's concerns might include:\n     - Enhancing documentation/error messaging within `skip`.\n     - Extending functionality for easier use via an alternate API if deemed necessary.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [],
        "relevant_symbols": [
          "skip",
          "Skipped.__init__"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:50.457924",
        "usage": "8108.085870742798 ms"
      },
      "request": {
        "file_path": "src/_pytest/mark/structures.py"
      },
      "response": {
        "scratch_pad": "1. Analyze the Issue:\n\n1.1 The issue describes complications when trying to skip tests in older Python versions (specifically before 3.8) using pytest. A proper way could involve using a 'skip' with a `allow_module_level=True` flag.\n\n1.2 The user provided a scenario where they encountered an error when trying to use `pytest.skip` outside of a test. It's noted that `pytestmark = pytest.mark.{skip,skipif}` does not work as expected with Python version syntax beyond 3.7.\n\n1.3 The problem appears to stem from how pytest processes files and how it's skipping tests at the module level. The user suggests adding an alternative simplified API like `skip_module(\"reason\")` for convenience.\n\n1.4 The expectation from the user is to have a seamless and clear way to skip modules when certain Python versions are being used. They face issues with `pytestmark` in processing file failures with older pre-3.8 Python versions.\n\n1.5 The core suggestion includes better error messages or a more user-friendly API.\n\n2. Evaluate the provided file:\n\n2.1 Analyze the `structures.py` file.\n\n2.2 The file content majorly revolves around decorators for marking test functions/classes. Symbols like `Mark`, `MarkDecorator`, `MarkGenerator`, and functions related to manipulating and applying marks are present.\n\n2.3 None of the symbols directly relate to implementing a mechanism for skipping tests or modules based on Python version checks or user suggestions (like `skip_module` function).\n\n2.4 This seems to be more about structuring the mark system rather than handling skip functionality directly, so none of the presented methods or classes seem directly aligned with the issue.\n\n2.5 Although some mechanisms for skipping (`_SkipMarkDecorator`) are present, the file is more about handling decorators rather than facilitating the exact feature request for skipping modules based on Python feature availability.\n\n2.6 This file doesn't appear to contain the root cause or a suitable place for implementing the feature requested in the issue scenario, as it doesn't engage directly with the pytest test execution flow.",
        "relevant": false,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/main.py",
          "src/_pytest/config/__init__.py",
          "src/_pytest/nodes.py"
        ],
        "relevant_symbols": [],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:58.566102",
        "usage": "9148.044109344482 ms"
      },
      "request": {
        "file_path": "src/pytest/__init__.py"
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n\n1.1 The issue involves improving the handling of module-level skips in pytest, specifically for code that uses syntax or features only available in newer Python versions (like 3.8), which shouldn't be tested in older versions like 3.6 or 3.7. The current message shown when trying to skip improperly conflicts with a discouraged use case that actually works.\n\n1.2 The user provides a scenario that causes an error: attempting to use 'pytest.skip' at the module level without 'allow_module_level=True', which raises an error because it's not designed for use as such according to the current guidance.\n\n1.3 The user is trying to manage code that should only be run in Python 3.8+, but finds the error messaging and guidance around using module-level skips in pytest lacking. The error messages suggest alternatives that don't actually work in the user's context, leading to confusion.\n\n1.4 The user expects a cleaner and clearer way to handle scenarios where a module should only be processed in certain Python versions without hitting syntax errors.\n\n1.6 Possible solutions include improving error messages to explain using 'allow_module_level' correctly, or creating a new API function like 'skip_module' to streamline this process.\n\n2. Make file evaluation:\n\n2.1 The file 'src/pytest/__init__.py' appears to be primarily for imports and versioning as it collects various components from the '_pytest' module and assigns them to named objects in '__all__'.\n\n2.2 The presence of 'skip' in the imports from '_pytest.outcomes' is relevant as it relates directly to the skipping functionality the user is having issues with.\n\n2.3 This file does not seem to contain the function definitions but acts as an aggregator for accessible components in pytest. Therefore, the root causes or attempts to address the issue should focus on files that implement `skip` and skipping logic.\n\n2.5 Relevant functions would likely be implemented in outcome-based files, possibly '_pytest/outcomes.py', based on typical Python project structures.\n\n2.6 The file acts as a relay/loading file and not as a cause for the reported issue. However, understanding where 'skip' is imported from is useful for delving deeper into the issue.",
        "relevant": false,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/outcomes.py",
          "src/_pytest/mark.py",
          "src/_pytest/config.py"
        ],
        "relevant_symbols": [],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:47:07.714158",
        "usage": "9385.21695137024 ms"
      },
      "request": {
        "file_path": "src/_pytest/config/__init__.py"
      },
      "response": {
        "scratch_pad": "## Analysis of the Issue\n1. **Understanding the Reported Issue**: \n   - The issue is about handling module-level skips in pytest for specific Python versions, particularly for Python versions less than 3.8 where new syntax might break tests.\n   - The error message using `pytest.skip` without allowing module-level usage adds confusion as the intended solution is discouraged.\n   - The root cause is the inability for pytest to skip modules correctly, leading to execution and failure when encountering newer syntax in older interpreters.\n\n2. **User-provided Reproduction and Steps**:\n   - Demonstrates using `skip` to avoid testing under older Python versions.\n   - Shows that the typical suggestion by pytest (using `pytestmark`) does not work when decorators or parameters enforce Python 3.8+\n   - Provides an example of the expected behavior using `allow_module_level=True` and discusses a more intuitive API proposal.\n\n3. **Cause of Issue**:\n   - The user encounters an error message unless they invoke `skip` with `allow_module_level=True`.\n   - Error messages and lack of intuitive APIs for module-level skips complicated proper handling.\n\n4. **User Expectation**:\n   - Users would expect an obvious and easy way to skip entire modules based on certain conditions (e.g., when syntax is not compatible with older Python versions).\n   - Requires intuitive error messages or API to facilitate this need.\n\n5. **Potential Solutions**:\n   - Update error messages to better guide users on the use of `allow_module_level=True`.\n   - Introducing a more straightforward API, such as `skip_module(reason)`.\n\n## Analysis of the File Content\n1. **Symbols in Scope**:\n   - Inspect functions and classes related to pytest's handling of configuration and plugins.\n   - Evaluate usage or implementation details of skipping and skipping warnings.\n\n2. **Functionality of Interest**:\n   - `main(args, plugins)` and `console_main()` are involved in command-line argument parsing and execution.\n   - `Config` and `PyTestPluginManager` seem central to configuration processing, yet current content does not reflect direct usage or checks for module version compatibility skipping.\n\n3. **Potential Relevance**:\n   - No direct evidence that this file deals with handling skips or specific error message creation.\n   - Principles used in importing or handling configurations might shed light on how skips could be better handled, though not directly related to the `allow_module_level` skip issue.\n\n4. **Overall Assessment**:\n   - This file appears primarily intended for setup/initialization configurations rather than handling specific scenarios like the module-level skips directly.\n\nGiven these observations, let's outline our determination next.",
        "relevant": false,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/skipping.py",
          "src/_pytest/outcomes.py",
          "src/_pytest/mark.py"
        ],
        "relevant_symbols": [],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:47:17.099391",
        "usage": "9012.619018554688 ms"
      },
      "request": {
        "file_path": "src/_pytest/logging.py"
      },
      "response": {
        "scratch_pad": "1. Analyzing User Instructions:\n\n1.1 The user is encountering an issue with skipping tests at the module level based on the Python version.\n\n1.2 The reproduce steps involve writing a script using Python 3.8 syntax and attempting to skip executing the module under older Python versions (e.g., 3.6, 3.7) using incorrect methods.\n\n1.3 The issue seems to result from a misunderstanding of how to properly skip test modules that rely on newer syntax or functionality not present in older Python versions. When they use `skip` incorrectly, it results in an error message because skipping outside a function in an improper way isn't allowed.\n\n1.4 The root cause appears to be an insufficient explanation, or the lack of an easy API to properly skip modules at runtime. The error message generated doesn't guide the user properly towards using `allow_module_level=True` with `skip`, creating confusion.\n\n1.5 The user's expectation is to have a simple and effective means of skipping tests at a module level for incompatible Python versions, which the current API does not clearly convey.\n\n1.6 Potential solutions might involve improving documentation to better reflect how `allow_module_level=True` should be used, or creating a new API like `skip_module(\"reason\")` which inherently acts at module level.\n\n2. Making File Evaluation:\n\n2.1 `src/_pytest/logging.py` seems to focus on log capturing and logging configuration for pytest.\n\n2.2 The file includes classes and functions related to formatting and capturing logs (`ColoredLevelFormatter`, `LogCaptureHandler`, `get_option_ini`, etc.), but it doesn't seem directly related to the issue of test skipping based on version control.\n\n2.3 No current symbols in `src/_pytest/logging.py` are related to or responsible for the skip functionality described in the issue (e.g., skipping tests based on Python version).\n\n2.6 This file is not the root cause of the issue as it's unrelated to the `skip` functionality in pytest.",
        "relevant": false,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/skipping.py",
          "src/_pytest/core.py"
        ],
        "relevant_symbols": [],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileSymbolSearch",
      "metadata": {
        "start": "2024-08-19T13:47:26.112052",
        "usage": "113.80815505981445 ms"
      },
      "request": {
        "files": [
          {
            "file_path": "src/_pytest/skipping.py",
            "relevant_symbol_keys": [
              "evaluate_skip_marks",
              "evaluate_xfail_marks",
              "pytest_runtest_setup"
            ],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/_pytest/outcomes.py",
            "relevant_symbol_keys": [
              "skip",
              "Skipped.__init__"
            ],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/_pytest/mark/structures.py",
            "relevant_symbol_keys": [],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/pytest/__init__.py",
            "relevant_symbol_keys": [],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/_pytest/config/__init__.py",
            "relevant_symbol_keys": [],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/_pytest/logging.py",
            "relevant_symbol_keys": [],
            "root_cause_symbol_keys": []
          }
        ],
        "with_content": true
      },
      "response": {
        "related_symbols": [
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
          },
          {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
          },
          {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
          },
          {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
          },
          {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
          },
          {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
          },
          {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": []
      }
    },
    {
      "name": "SymbolsEvaluate",
      "metadata": {
        "start": "2024-08-19T13:47:26.225867",
        "usage": "13167.666912078857 ms"
      },
      "request": {
        "related_symbols": [
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
          },
          {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
          },
          {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
          },
          {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
          },
          {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
          },
          {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
          },
          {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": []
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 The user is encountering an issue with skipping tests in pytest that use Python 3.8 specific syntax when running in older environments like Python 3.6 or 3.7. Naive use of `pytest.skip` leads to an error because it doesn't support module-level skipping without a specific parameter. The user suggests improving documentation, error messages, or introducing a new API to handle this case better.\n\n1.2 The issue is caused because pytest's API does not directly support module-level skips using `skip` without setting `allow_module_level=True`. This behavior is not clearly explained in error messages or documentation, leading to confusion for users.\n\n1.3 The expected result is that users should be able to easily skip entire modules if the Python version is below a certain threshold, without encountering confusing errors. Ideally, a simple function like `skip_module(\"reason\")` would provide this functionality.\n\n1.4 If not specified, the expectation is simple handling of module-level test skipping based on conditions without confusion.\n\n1.5 The issue could be addressed by either clarifying existing error messages to include details about the `allow_module_level` parameter usage or by introducing a new function/API for module-level skipping.\n\n2. Evaluate Symbols:\n2.1 The symbols include various functions and classes related to skipping functionality in pytest.\n2.2 We need to focus on symbols that handle skip logic or exceptions related to skipping tests.\n2.3 Checking for parts in code that might be responsible for evaluating skip conditions or deciding behavior based on certain python versions.\n\n2.6 The most relevant symbols will be those directly preventing module-level skips or not properly documenting the usage of allow_module_level.\n\n- `_pytest.outcomes.skip`: could be crucial because it directly relates to the skipping mechanism.\n- `evaluate_skip_marks`: seems responsible for interpreting skip markers, crucial for implementing any logic related to `allow_module_level`.\n\n2.4 I need to determine where modifications or additional explanations would best fit to solve the issue, potentially modify or add comments/examples in the `skip` implementation.",
        "symbols": [
          {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "reason": "This symbol directly implements the skip function, which is central to the issue. Adjusting this to provide clearer error messages or documentation regarding the `allow_module_level` parameter could resolve the user's problem."
          }
        ],
        "more_info": [],
        "is_done": true,
        "symbols_scratch_pad": [
          "Not relevant as this appears to refer more to the setup configuration of capturing sys.output rather than skipping logic.",
          "Not relevant, likely related to the setup specific to hook but not skipping logic.",
          "Not relevant, related more to logging rather than skipping logic.",
          "Not relevant, handling fails during running tests, not skipping.",
          "This function evaluates skip marks, potentially crucial as it might be adjusted to handle allow_module_level logic or condition messages better.",
          "This symbol evaluates xfail marks, not related to skipping or the issue described, so not immediately relevant.",
          "Handles setup for evaluating skip and xfail, potentially linked to evaluating skip logic could relate to when evaluate_skip_marks function is called so could be involved indirectly.",
          "Not directly related to skipping logic for module-level issues with version checks, appears more related to thread exception handling during setup.",
          "Not directly related to skipping logic, focuses on issues raised through unraisable exceptions.",
          "Not related to skipping logic, refers to NOSE plugin hooks which seem off-topic here.",
          "Not directly related either, these config settings like getvalueorskip refer to plugin warnings or settings configurations.",
          "Related to warnings rather than functional changes needed for skipping logic.",
          "Check for all skips may relate indirectly but not central to proposed solution as no processing logic here evident.",
          "Reportings storage of skipped status in XML unlikely linked to processing logic.",
          "Again, reporting related concerns unlikely to address processing or documentation needed.",
          "Constructor of Skipped holding core logic, particularly the allow_module_level flag, highly relevant.",
          "Directly implements skip functionality, needs adjustment to clearly allow module level skips when conditionally needed as user issues here illustrate.",
          "Imports missing version check would throw Skipped exception at module level, relevant here extensively due to functionality similar to what is asked.",
          "Symlink or skip unrelated, no influence on logic discussed.",
          "No direct relation to issue, focus remains on skipping logic or processing, suggests possible concepts overlap only."
        ]
      }
    },
    {
      "name": "SymbolSummary",
      "metadata": {
        "start": "2024-08-19T13:47:39.393553",
        "usage": "1.7421245574951172 ms"
      },
      "request": {
        "symbols": {
          "related_symbols": [
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 802,
              "start_column": 26043,
              "end_line": 4,
              "end_column": 26214,
              "start_byte": 26043,
              "end_byte": 4
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 481,
              "start_column": 16683,
              "end_line": 0,
              "end_column": 17027,
              "start_byte": 16683,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 704,
              "start_column": 25635,
              "end_line": 4,
              "end_column": 25950,
              "start_byte": 25635,
              "end_byte": 4
            },
            {
              "name": "_pytest.runner.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
              "start_line": 153,
              "start_column": 4972,
              "end_line": 0,
              "end_column": 5102,
              "start_byte": 4972,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.evaluate_skip_marks",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
              "start_line": 166,
              "start_column": 5700,
              "end_line": 0,
              "end_column": 6627,
              "start_byte": 5700,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.evaluate_xfail_marks",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None",
              "start_line": 204,
              "start_column": 6888,
              "end_line": 0,
              "end_column": 7789,
              "start_byte": 6888,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
              "start_line": 233,
              "start_column": 7909,
              "end_line": 0,
              "end_column": 8290,
              "start_byte": 7909,
              "end_byte": 0
            },
            {
              "name": "_pytest.threadexception.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
              "start_line": 75,
              "start_column": 2442,
              "end_line": 0,
              "end_column": 2596,
              "start_byte": 2442,
              "end_byte": 0
            },
            {
              "name": "_pytest.unraisableexception.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
              "start_line": 80,
              "start_column": 2705,
              "end_line": 0,
              "end_column": 2864,
              "start_byte": 2705,
              "end_byte": 0
            },
            {
              "name": "_pytest.nose.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
              "start_line": 8,
              "start_column": 236,
              "end_line": 0,
              "end_column": 1174,
              "start_byte": 236,
              "end_byte": 0
            },
            {
              "name": "_pytest.config.Config.getvalueorskip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1522,
              "start_column": 53901,
              "end_line": 4,
              "end_column": 54055,
              "start_byte": 53901,
              "end_byte": 4
            },
            {
              "name": "_pytest.config.Config._warn_about_skipped_plugins",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1547,
              "start_column": 54888,
              "end_line": 4,
              "end_column": 55169,
              "start_byte": 54888,
              "end_byte": 4
            },
            {
              "name": "_pytest.doctest._check_all_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 440,
              "start_column": 14834,
              "end_line": 0,
              "end_column": 15165,
              "start_byte": 14834,
              "end_byte": 0
            },
            {
              "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 216,
              "start_column": 7806,
              "end_line": 4,
              "end_column": 7947,
              "start_byte": 7806,
              "end_byte": 4
            },
            {
              "name": "_pytest.junitxml._NodeReporter.append_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 235,
              "start_column": 8519,
              "end_line": 4,
              "end_column": 9421,
              "start_byte": 8519,
              "end_byte": 4
            },
            {
              "name": "_pytest.outcomes.Skipped.__init__",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
              "start_line": 55,
              "start_column": 1779,
              "end_line": 4,
              "end_column": 2297,
              "start_byte": 1779,
              "end_byte": 4
            },
            {
              "name": "_pytest.outcomes.skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
              "start_line": 122,
              "start_column": 3684,
              "end_line": 0,
              "end_column": 4711,
              "start_byte": 3684,
              "end_byte": 0
            },
            {
              "name": "_pytest.outcomes.importorskip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
              "start_line": 179,
              "start_column": 5761,
              "end_line": 0,
              "end_column": 7676,
              "start_byte": 5761,
              "end_byte": 0
            },
            {
              "name": "_pytest.pathlib.symlink_or_skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 428,
              "start_column": 13871,
              "end_line": 0,
              "end_column": 14118,
              "start_byte": 13871,
              "end_byte": 0
            },
            {
              "name": "_pytest.python.async_warn_and_skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
              "start_line": 163,
              "start_column": 5601,
              "end_line": 0,
              "end_column": 6147,
              "start_byte": 5601,
              "end_byte": 0
            },
            {
              "name": "_pytest.reports.BaseReport.skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
              "start_line": 151,
              "start_column": 4247,
              "end_line": 4,
              "end_column": 4328,
              "start_byte": 4247,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.pytest_addoption",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
              "start_line": 26,
              "start_column": 664,
              "end_line": 0,
              "end_column": 1160,
              "start_byte": 664,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_configure",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
              "start_line": 45,
              "start_column": 1163,
              "end_line": 0,
              "end_column": 2803,
              "start_byte": 1163,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.evaluate_condition",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
              "start_line": 84,
              "start_column": 2806,
              "end_line": 0,
              "end_column": 5541,
              "start_byte": 2806,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
              "start_line": 159,
              "start_column": 5544,
              "end_line": 0,
              "end_column": 5697,
              "start_byte": 5544,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Skip.reason",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "onditions\n        like known bugs or missing features.\n ",
              "start_line": 163,
              "start_column": 5641,
              "end_line": 4,
              "end_column": 5697,
              "start_byte": 5641,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.evaluate_skip_marks",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
              "start_line": 166,
              "start_column": 5700,
              "end_line": 0,
              "end_column": 6627,
              "start_byte": 5700,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Xfail",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
              "start_line": 194,
              "start_column": 6630,
              "end_line": 0,
              "end_column": 6885,
              "start_byte": 6630,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Xfail.reason",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "n because\n        # of exi",
              "start_line": 198,
              "start_column": 6729,
              "end_line": 4,
              "end_column": 6755,
              "start_byte": 6729,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.Xfail.run",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": " directories with the sa",
              "start_line": 199,
              "start_column": 6760,
              "end_line": 4,
              "end_column": 6784,
              "start_byte": 6760,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.Xfail.strict",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "me we're trying to\n        ",
              "start_line": 200,
              "start_column": 6789,
              "end_line": 4,
              "end_column": 6816,
              "start_byte": 6789,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.Xfail.raises",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
              "start_line": 201,
              "start_column": 6821,
              "end_line": 4,
              "end_column": 6885,
              "start_byte": 6821,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.evaluate_xfail_marks",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
              "start_line": 204,
              "start_column": 6888,
              "end_line": 0,
              "end_column": 7789,
              "start_byte": 6888,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.xfailed_key",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 230,
              "start_column": 7865,
              "end_line": 0,
              "end_column": 7906,
              "start_byte": 7865,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 233,
              "start_column": 7909,
              "end_line": 0,
              "end_column": 8290,
              "start_byte": 7909,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_call",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 244,
              "start_column": 8293,
              "end_line": 0,
              "end_column": 8868,
              "start_byte": 8293,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 261,
              "start_column": 8871,
              "end_line": 0,
              "end_column": 9945,
              "start_byte": 8871,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_report_teststatus",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 289,
              "start_column": 9948,
              "end_line": 0,
              "end_column": 10224,
              "start_byte": 9948,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal._folded_skips",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1318,
              "start_column": 47439,
              "end_line": 0,
              "end_column": 48734,
              "start_byte": 47439,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal._get_raw_skip_reason",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1394,
              "start_column": 50198,
              "end_line": 0,
              "end_column": 50865,
              "start_byte": 50198,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest._is_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 403,
              "start_column": 13805,
              "end_line": 0,
              "end_column": 13971,
              "start_byte": 13805,
              "end_byte": 0
            }
          ],
          "root_caused_symbols": []
        },
        "symbols_evaluate": {
          "scratch_pad": "1. Analyze User Instructions:\n1.1 The user is encountering an issue with skipping tests in pytest that use Python 3.8 specific syntax when running in older environments like Python 3.6 or 3.7. Naive use of `pytest.skip` leads to an error because it doesn't support module-level skipping without a specific parameter. The user suggests improving documentation, error messages, or introducing a new API to handle this case better.\n\n1.2 The issue is caused because pytest's API does not directly support module-level skips using `skip` without setting `allow_module_level=True`. This behavior is not clearly explained in error messages or documentation, leading to confusion for users.\n\n1.3 The expected result is that users should be able to easily skip entire modules if the Python version is below a certain threshold, without encountering confusing errors. Ideally, a simple function like `skip_module(\"reason\")` would provide this functionality.\n\n1.4 If not specified, the expectation is simple handling of module-level test skipping based on conditions without confusion.\n\n1.5 The issue could be addressed by either clarifying existing error messages to include details about the `allow_module_level` parameter usage or by introducing a new function/API for module-level skipping.\n\n2. Evaluate Symbols:\n2.1 The symbols include various functions and classes related to skipping functionality in pytest.\n2.2 We need to focus on symbols that handle skip logic or exceptions related to skipping tests.\n2.3 Checking for parts in code that might be responsible for evaluating skip conditions or deciding behavior based on certain python versions.\n\n2.6 The most relevant symbols will be those directly preventing module-level skips or not properly documenting the usage of allow_module_level.\n\n- `_pytest.outcomes.skip`: could be crucial because it directly relates to the skipping mechanism.\n- `evaluate_skip_marks`: seems responsible for interpreting skip markers, crucial for implementing any logic related to `allow_module_level`.\n\n2.4 I need to determine where modifications or additional explanations would best fit to solve the issue, potentially modify or add comments/examples in the `skip` implementation.",
          "symbols": [
            {
              "name": "_pytest.outcomes.skip",
              "file_path": "src/_pytest/outcomes.py",
              "reason": "This symbol directly implements the skip function, which is central to the issue. Adjusting this to provide clearer error messages or documentation regarding the `allow_module_level` parameter could resolve the user's problem."
            }
          ],
          "more_info": [],
          "is_done": true,
          "symbols_scratch_pad": [
            "Not relevant as this appears to refer more to the setup configuration of capturing sys.output rather than skipping logic.",
            "Not relevant, likely related to the setup specific to hook but not skipping logic.",
            "Not relevant, related more to logging rather than skipping logic.",
            "Not relevant, handling fails during running tests, not skipping.",
            "This function evaluates skip marks, potentially crucial as it might be adjusted to handle allow_module_level logic or condition messages better.",
            "This symbol evaluates xfail marks, not related to skipping or the issue described, so not immediately relevant.",
            "Handles setup for evaluating skip and xfail, potentially linked to evaluating skip logic could relate to when evaluate_skip_marks function is called so could be involved indirectly.",
            "Not directly related to skipping logic for module-level issues with version checks, appears more related to thread exception handling during setup.",
            "Not directly related to skipping logic, focuses on issues raised through unraisable exceptions.",
            "Not related to skipping logic, refers to NOSE plugin hooks which seem off-topic here.",
            "Not directly related either, these config settings like getvalueorskip refer to plugin warnings or settings configurations.",
            "Related to warnings rather than functional changes needed for skipping logic.",
            "Check for all skips may relate indirectly but not central to proposed solution as no processing logic here evident.",
            "Reportings storage of skipped status in XML unlikely linked to processing logic.",
            "Again, reporting related concerns unlikely to address processing or documentation needed.",
            "Constructor of Skipped holding core logic, particularly the allow_module_level flag, highly relevant.",
            "Directly implements skip functionality, needs adjustment to clearly allow module level skips when conditionally needed as user issues here illustrate.",
            "Imports missing version check would throw Skipped exception at module level, relevant here extensively due to functionality similar to what is asked.",
            "Symlink or skip unrelated, no influence on logic discussed.",
            "No direct relation to issue, focus remains on skipping logic or processing, suggests possible concepts overlap only."
          ]
        }
      },
      "response": {
        "file_paths": [
          "src/_pytest/outcomes.py"
        ],
        "symbols": [
          "_pytest.outcomes.skip"
        ],
        "code_spans": {
          "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
          },
          "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
          },
          "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
          },
          "_pytest.skipping.evaluate_skip_marks": {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          "_pytest.skipping.evaluate_xfail_marks": {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          "_pytest.threadexception.pytest_runtest_setup": {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
          },
          "_pytest.unraisableexception.pytest_runtest_setup": {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
          },
          "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
          },
          "_pytest.config.Config.getvalueorskip": {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
          },
          "_pytest.config.Config._warn_about_skipped_plugins": {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
          },
          "_pytest.doctest._check_all_skipped": {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
          },
          "_pytest.junitxml._NodeReporter.append_collect_skipped": {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
          },
          "_pytest.junitxml._NodeReporter.append_skipped": {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
          },
          "_pytest.outcomes.Skipped.__init__": {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
          },
          "_pytest.outcomes.skip": {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
          },
          "_pytest.outcomes.importorskip": {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
          },
          "_pytest.pathlib.symlink_or_skip": {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
          },
          "_pytest.python.async_warn_and_skip": {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
          },
          "_pytest.reports.BaseReport.skipped": {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
          },
          "_pytest.skipping.pytest_addoption": {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_configure": {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
          },
          "_pytest.skipping.evaluate_condition": {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
          },
          "_pytest.skipping.Skip": {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
          },
          "_pytest.skipping.Skip.reason": {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail": {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
          },
          "_pytest.skipping.Xfail.reason": {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail.run": {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail.strict": {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail.raises": {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
          },
          "_pytest.skipping.xfailed_key": {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_call": {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_report_teststatus": {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
          },
          "_pytest.terminal._folded_skips": {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
          },
          "_pytest.terminal._get_raw_skip_reason": {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
          },
          "_pytest.unittest._is_skipped": {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
          }
        }
      }
    }
  ]
}