{
    "file_paths": [
        "src/_pytest/outcomes.py"
    ],
    "symbols": [
        "_pytest.outcomes.skip"
    ],
    "code_spans": {
        "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
        },
        "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
        },
        "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
        },
        "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
        },
        "_pytest.skipping.evaluate_skip_marks": {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
        },
        "_pytest.skipping.evaluate_xfail_marks": {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
        },
        "_pytest.threadexception.pytest_runtest_setup": {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
        },
        "_pytest.unraisableexception.pytest_runtest_setup": {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
        },
        "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
        },
        "_pytest.config.Config.getvalueorskip": {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
        },
        "_pytest.config.Config._warn_about_skipped_plugins": {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
        },
        "_pytest.doctest._check_all_skipped": {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
        },
        "_pytest.junitxml._NodeReporter.append_collect_skipped": {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
        },
        "_pytest.junitxml._NodeReporter.append_skipped": {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
        },
        "_pytest.outcomes.Skipped.__init__": {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
        },
        "_pytest.outcomes.skip": {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
        },
        "_pytest.outcomes.importorskip": {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
        },
        "_pytest.pathlib.symlink_or_skip": {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
        },
        "_pytest.python.async_warn_and_skip": {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
        },
        "_pytest.reports.BaseReport.skipped": {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
        },
        "_pytest.skipping.pytest_addoption": {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_configure": {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
        },
        "_pytest.skipping.evaluate_condition": {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
        },
        "_pytest.skipping.Skip": {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
        },
        "_pytest.skipping.Skip.reason": {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
        },
        "_pytest.skipping.Xfail": {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
        },
        "_pytest.skipping.Xfail.reason": {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
        },
        "_pytest.skipping.Xfail.run": {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
        },
        "_pytest.skipping.Xfail.strict": {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
        },
        "_pytest.skipping.Xfail.raises": {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
        },
        "_pytest.skipping.xfailed_key": {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_runtest_call": {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_report_teststatus": {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
        },
        "_pytest.terminal._folded_skips": {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
        },
        "_pytest.terminal._get_raw_skip_reason": {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
        },
        "_pytest.unittest._is_skipped": {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
        }
    }
}