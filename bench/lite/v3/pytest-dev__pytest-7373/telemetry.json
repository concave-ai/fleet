{
  "total_cost": 0,
  "task_id": "pytest-dev__pytest-7373",
  "model": "gpt-4o-2024-08-06",
  "spans": [
    {
      "name": "KeywordExtract",
      "metadata": {
        "start": "2024-08-19T13:46:09.285470",
        "usage": "1508.863925933838 ms"
      },
      "request": {
        "repo_tokens": [
          "showversion",
          "try_makedirs",
          "putaround",
          "setenv",
          "CollectError",
          "FILLFUNCARGS",
          "maketxtfile",
          "duration",
          "get_default_arg_names",
          "pytest_runtest_makereport",
          "Module",
          "pytest_make_collect_report",
          "Cache",
          "LogXML",
          "_is_numpy_array",
          "_create_formatter",
          "_with_exception",
          "get_unpacked_marks",
          "pytest_runtest_setup",
          "ensure_newline",
          "_is_marked_for_rewrite",
          "wrap_pytest_function_for_tracing",
          "_find_spec",
          "popcall",
          "pytest_collect_directory",
          "_CACHE_PREFIX_VALUES",
          "_compare_eq_verbose",
          "keywords",
          "_get_marks",
          "get_lock_path",
          "cls",
          "makeconftest",
          "recursionindex",
          "resume_fixture",
          "pytest_addoption",
          "NoMatch",
          "mkpydir",
          "write",
          "StepwisePlugin",
          "Store",
          "StoreKey",
          "visit_BoolOp",
          "check_strict_xfail",
          "fillfixtures",
          "notset",
          "helpconfig",
          "assert_outcomes",
          "funcargnames",
          "_locationline",
          "FakeCollectModule",
          "ReprExceptionInfo",
          "getworkerinfoline",
          "__call__",
          "_get_direct_parametrize_args",
          "_rewrite_test",
          "pytest_exception_interact",
          "merge_family",
          "_for_parametrize",
          "lex",
          "annotated_getattr",
          "essential_plugins",
          "get_closest_marker",
          "pytest_warning_captured",
          "_factorytraceback",
          "setupplan",
          "hasplugin",
          "no_re_match_line",
          "debugging",
          "TestCaseFunction",
          "ConftestImportFailure",
          "resume_global_capture",
          "_config_for_test",
          "activate_fixture",
          "warning_types",
          "_argcomplete",
          "assertoutcome",
          "_getautousenames",
          "getmodulecol",
          "makedir",
          "_import_plugin_specs",
          "_get_teardown_reports",
          "fullsource",
          "_initini",
          "_get_fixturestack",
          "_init_runner_class",
          "explanation_param",
          "PytestCacheWarning",
          "version",
          "_validate_ids",
          "listextrakeywords",
          "write_sep",
          "set",
          "push_format_context",
          "line",
          "terminal",
          "addExpectedFailure",
          "reset",
          "ReprFileLocation",
          "exit",
          "pytest_deselected",
          "warn",
          "dir",
          "_ellipsize",
          "notify_exception",
          "_getconftest_pathlist",
          "runpython_c",
          "Code",
          "disabled",
          "capsys",
          "compat",
          "doctest_namespace",
          "prog",
          "_PytestWrapper",
          "mode",
          "repr_locals",
          "_parse_ini_config",
          "pytest_report_to_serializable",
          "scope2props",
          "try_cleanup",
          "FixtureManager",
          "fnmatch_lines_random",
          "NodeKeywords",
          "ApproxNumpy",
          "inline_runsource",
          "names_closure",
          "writeorg",
          "getpluginversioninfo",
          "Node",
          "no_header",
          "get_source",
          "build_summary_stats_line",
          "getlocation",
          "tmpdir_factory",
          "reprlocals",
          "fslocation",
          "_get_pdb_wrapper_class",
          "ImportPathMismatchError",
          "_ALLOW_MARKERS",
          "_validate_if_using_arg_names",
          "stop_global_capturing",
          "_write_source",
          "showfixtures",
          "accept",
          "classnamefilter",
          "Skipped",
          "_get_unknown_ini_keys",
          "unset_fixture",
          "fault_handler_stderr_key",
          "ExceptionChainRepr",
          "_update_current_test_var",
          "session",
          "repr_traceback_entry",
          "SysModulesSnapshot",
          "instance",
          "type",
          "_inject_setup_teardown_fixtures",
          "DEPRECATED_EXTERNAL_PLUGINS",
          "AND",
          "FormattedExcinfo",
          "COLLECT_FAKEMODULE_ATTRIBUTES",
          "ReprFailDoctest",
          "syspathinsert",
          "CallSpec2",
          "_checkversion",
          "_match_lines",
          "istext",
          "_remove_ansi_escape_sequences",
          "xml_key",
          "resolve_fixture_function",
          "from_item",
          "getvalueorskip",
          "visit_Attribute",
          "chdir",
          "_initrequest",
          "_should_rewrite",
          "is_true",
          "add_funcarg_pseudo_fixture_def",
          "deprecated",
          "_resume",
          "_warn_incompatibility_with_xunit2",
          "ReprTracebackNative",
          "_check_initialpaths_for_relpath",
          "pos",
          "_log_cli_enabled",
          "register",
          "_getparser",
          "exec_",
          "deactivate_fixture",
          "getitems",
          "pytest_assertion_pass",
          "_from_json",
          "getplugin",
          "failed",
          "_handle_teardown_sections",
          "scopeproperty",
          "is_generator",
          "location",
          "UnitTestCase",
          "_try_repr_or_str",
          "_main",
          "FuncFixtureInfo",
          "_saferepr",
          "get_config",
          "get_scope_package",
          "FixtureLookupErrorRepr",
          "assign",
          "_truncate_explanation",
          "tmp_path_factory",
          "ArgumentError",
          "_idvalset",
          "getfixtureclosure",
          "params",
          "record_testsuite_property",
          "makeini",
          "freeze_support",
          "_W",
          "monkeypatch",
          "__bool__",
          "_strtobool",
          "_ensure_supporting_files",
          "summary_stats",
          "get_direct_param_fixture_func",
          "re_match_lines",
          "__contains__",
          "pytest_collectreport",
          "register_assert_rewrite",
          "name2fixturedefs",
          "ApproxSequencelike",
          "isnosetest",
          "_get_plugin_specs_as_list",
          "install_importhook",
          "async_warn_and_skip",
          "addini",
          "faulthandler",
          "get_lines_after",
          "__take_sys_modules_snapshot",
          "getsource",
          "getcall",
          "builtin",
          "CHECKER_CLASS",
          "importlib",
          "fail_fixturefunc",
          "pytest_ignore_collect",
          "_opentestcase",
          "scope2index",
          "PytestDeprecationWarning",
          "consider_module",
          "_visit_filter",
          "_parse_lsof_output",
          "hookimpl",
          "summary_warnings",
          "_report_unserialization_failure",
          "safe_isclass",
          "__init__",
          "__array_priority__",
          "__all__",
          "KNOWN_TYPES",
          "_try_load_conftest",
          "_NodeType",
          "_fail",
          "TempPathFactory",
          "deselect_by_keyword",
          "illegal_xml_re",
          "_FixtureCachedResult",
          "PYTESTER_COPY_EXAMPLE",
          "matchnodes",
          "showhelp",
          "isdict",
          "summary_failures",
          "spawn_pytest",
          "Failed",
          "__enter__",
          "istestfunction",
          "filename_arg",
          "recwarn",
          "assertion",
          "write_fspath_result",
          "FunctionDefinition",
          "BaseReport",
          "get_exconly",
          "message",
          "write_log_entry",
          "_get_verbose_word",
          "getini",
          "_call_with_optional_argument",
          "catch_log_records_key",
          "_issue_warning_captured",
          "matching_platform",
          "_write_content",
          "get_parametrized_fixture_keys",
          "readouterr",
          "processoption",
          "_early_rewrite_bailout",
          "fspath",
          "getcrashentry",
          "delattr",
          "fileno",
          "addSuccess",
          "DOCTEST_REPORT_CHOICE_CDIFF",
          "_expecting_failure",
          "parseconfigure",
          "_PY_DIR",
          "skip",
          "fnmatch_lines",
          "Collector",
          "saferepr",
          "add_cleanup",
          "_getobj",
          "get_terminal_width",
          "_typ_map",
          "PytestAssertRewriteWarning",
          "_perform_collect",
          "_is_capturing",
          "normalize_mark_list",
          "_wrapped_pdb_cls",
          "importorskip",
          "source",
          "reprtraceback",
          "CACHEDIR_TAG_CONTENT",
          "_esctable",
          "_repr_style",
          "match",
          "MultipleDoctestFailures",
          "not_expr",
          "MarkDecorator",
          "teardown_nose",
          "ColoredLevelFormatter",
          "_report_to_json",
          "_compare_eq_cls",
          "done",
          "CallInfo",
          "WarningsChecker",
          "fixture",
          "__attrs_post_init__",
          "MonkeyPatch",
          "getfixturedefs",
          "genitems",
          "timing",
          "_start",
          "fail",
          "filter_traceback",
          "get_data",
          "_get_allow_bytes_flag",
          "entrysep",
          "_make_plural",
          "_set_main_color",
          "invalidraise",
          "from_item_and_call",
          "_show_fixtures_per_test",
          "fix_cache_order",
          "Interrupted",
          "getstatementrange",
          "newinstance",
          "_params_converter",
          "extraline",
          "iscoroutinefunction",
          "collect",
          "cmdline",
          "variable",
          "run",
          "_compare_eq_sequence",
          "DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE",
          "pytest_fixture_post_finalizer",
          "collect_by_name",
          "writer",
          "getparent",
          "_set_opt_strings",
          "pytest_unconfigure",
          "get_records",
          "_get_increasing_order",
          "fixturenames",
          "isparseable",
          "Item",
          "DontReadFromInput",
          "text",
          "matchreport",
          "hasopt",
          "TESTS_FAILED",
          "verbosity",
          "_check_scope",
          "_update_message",
          "ParameterSet",
          "raises",
          "from_parent",
          "Junit",
          "_FixtureValue",
          "format",
          "summary_errors",
          "_determine_show_progress_info",
          "getimfunc",
          "warnings",
          "caplog",
          "_saved",
          "tmpdir",
          "_inject_setup_class_fixture",
          "pytest_doctest_prepare_content",
          "CollectReport",
          "_tryconvertpyarg",
          "FIXTURE_POSITIONAL_ARGUMENTS",
          "from_exc_info",
          "cacheprovider",
          "_fillfixtures",
          "when",
          "_legal_ranges",
          "PYC_EXT",
          "__slots__",
          "reportinfo",
          "ensure_reset_dir",
          "_enter_pdb",
          "conftest_options",
          "_get_checker",
          "_parse_filter",
          "log_outcome",
          "finish",
          "ExceptionRepr",
          "pytest_pyfunc_call",
          "_non_printable_ascii_translate_table",
          "rex_outcome",
          "pytest_addhooks",
          "tb",
          "DEFAULT_LOG_DATE_FORMAT",
          "update_testcase_duration",
          "_makeitem",
          "python_api",
          "linecomp",
          "repr_failure",
          "__array_ufunc__",
          "showheader",
          "get_last_failed_paths",
          "wcswidth",
          "pytestPDB",
          "record_testreport",
          "_import_pdb_cls",
          "_outrep_summary",
          "pastebinfile_key",
          "MatcherAdapter",
          "listoutcomes",
          "getinicfg",
          "_Markable",
          "_approx_scalar",
          "_yield_comparisons",
          "DOCTEST_REPORT_CHOICES",
          "junitxml",
          "ReprTraceback",
          "set_fixture",
          "stepwise",
          "TYPE_CHECKING",
          "pytest_configure",
          "CollectErrorRepr",
          "make_hook_recorder",
          "restore",
          "finish_recording",
          "RE_IMPORT_ERROR_NAME",
          "_exec_lsof",
          "istestfunc",
          "IGNORE_PAM",
          "expression",
          "MARK_GEN",
          "finalize",
          "_parsearg",
          "directory_arg",
          "locals",
          "_get_progress_information_message",
          "findsource",
          "Expression",
          "pytest_runtest_protocol",
          "CaptureResult",
          "T",
          "typename",
          "perform_collect",
          "OR",
          "traceback",
          "is_capturing",
          "MarkGenerator",
          "LogCaptureFixture",
          "pytest_collection_finish",
          "LogCaptureHandler",
          "pytest_collectstart",
          "Exception",
          "argnames",
          "CaptureManager",
          "isinitpath",
          "pytest",
          "INTERNAL_ERROR",
          "visit_Assert",
          "resume",
          "NOT",
          "_matchnodes",
          "TeeCaptureIO",
          "_do_configure",
          "on_rm_rf_error",
          "exec_module",
          "get_fslocation_from_item",
          "_FileHandler",
          "DummyRewriteHook",
          "_FixtureFunc",
          "parse_num",
          "_is_last_item",
          "UnformattedWarning",
          "_validate_args",
          "_warn_about_missing_assertion",
          "skipped_by_mark_key",
          "getnode",
          "_showfixtures_main",
          "_add_stats",
          "parse_hookspec_opts",
          "iter_markers",
          "getstatement",
          "item_capture",
          "LoggingPlugin",
          "lines",
          "is_async_function",
          "import_plugin",
          "_assert_expr_to_lineno",
          "pytest_pycollect_makemodule",
          "stop_capturing",
          "pytest_report_header",
          "_notin_text",
          "_no_match_line",
          "append_pass",
          "record_xml_attribute",
          "prepend",
          "DEFAULT_MAX_CHARS",
          "tolerance",
          "_getreprcrash",
          "_get_override_ini_value",
          "get_terminal_writer",
          "_find_parametrized_scope",
          "_LiveLoggingNullHandler",
          "getrepr",
          "SysCapture",
          "exprinfo",
          "scopenum_function",
          "FixtureLookupError",
          "frame",
          "names",
          "own_mark_names",
          "EncodedFile",
          "Mark",
          "_splitnode",
          "mark_rewrite",
          "LEVELNAME_FMT_REGEX",
          "__exit__",
          "get_empty_parameterset_mark",
          "LineMatcher",
          "_resolve_arg_value_types",
          "_recursive_debug",
          "parseoutcomes",
          "isattrs",
          "encoding",
          "DOCTEST_REPORT_CHOICE_NDIFF",
          "maybe_wrap_pytest_function_for_tracing",
          "pytest_enter_pdb",
          "store_mark",
          "_compilecounter",
          "_bytes_to_ascii",
          "_validate_keys",
          "consider_preparse",
          "_get_report_choice",
          "LOGLEVEL_COLOROPTS",
          "runpytest",
          "check_testcase_implements_trial_reporter",
          "MyOptionParser",
          "assertrepr_compare",
          "_assert_state",
          "_show_fixture_action",
          "RaisesContext",
          "store",
          "hookspec",
          "list",
          "_matches_prefix_or_glob_option",
          "_format_assertmsg",
          "_py_ext_re",
          "sep",
          "add_marker",
          "PytestExperimentalApiWarning",
          "generic_visit",
          "make_properties_node",
          "BASE_TYPE",
          "mkdir",
          "RESULT_LOG",
          "get_sections",
          "_getini",
          "make_numbered_dir",
          "ParseError",
          "setattr",
          "exconly",
          "_repr_failure_py",
          "_recursive_list_map",
          "_report_keyboardinterrupt",
          "Parser",
          "FDCaptureBinary",
          "_ensure_unconfigure",
          "check_interactive_exception",
          "WARNING_CAPTURED_HOOK",
          "DEFAULT_LOG_FORMAT",
          "passed",
          "repr",
          "_check_if_assertion_pass_impl",
          "_assertion_supported",
          "_force_symlink",
          "__len__",
          "catching_logs",
          "style",
          "eval",
          "_read_pyc",
          "print_teardown_sections",
          "NFPlugin",
          "_format",
          "_Key",
          "idmaker",
          "DOCTEST_REPORT_CHOICE_UDIFF",
          "show_test_item",
          "rewrite",
          "toterminal",
          "nodes",
          "_in_suspended",
          "LFPluginCollSkipfiles",
          "_get_flag_lookup",
          "_io",
          "builtin_plugins",
          "_parse_parametrize_args",
          "getargs",
          "old_mark_config_key",
          "addSkip",
          "symlink_or_skip",
          "assert_contains",
          "_get_auto_indent",
          "capsysbinary",
          "reject",
          "fullwidth",
          "FastFilesCompleter",
          "RPAREN",
          "append_collect_error",
          "logging",
          "ParsedCall",
          "catch_log_handler_key",
          "IDENT_PREFIX",
          "parametrize",
          "__str__",
          "_traceback",
          "_get_pos",
          "_sys_snapshot",
          "issequence",
          "suspend_fixture",
          "runtestprotocol",
          "_should_truncate_item",
          "indent",
          "parse_args",
          "FaultHandlerHooks",
          "skipped",
          "id",
          "getgroup",
          "num_mock_patch_args",
          "cwd_relative_nodeid",
          "MODULE_NOT_FOUND_ERROR",
          "setuponly",
          "_prepare_content",
          "ishidden",
          "pytest_warning_recorded",
          "_config",
          "REPORT_COLLECTING_RESOLUTION",
          "ascii_escaped",
          "_printcollecteditems",
          "str",
          "LFPluginCollWrapper",
          "expr",
          "__version_tuple__",
          "set_session",
          "showlocals",
          "error",
          "ids",
          "warns",
          "__version__",
          "raiseerror",
          "FixtureRequest",
          "no_summary",
          "pathlib",
          "_code",
          "__module__",
          "astcache",
          "_iter_rewritable_modules",
          "SafeRepr",
          "HookRecorder",
          "README_CONTENT",
          "visit_Compare",
          "record_property",
          "resolve",
          "simple",
          "reorder_items_atscope",
          "read",
          "_validate_usepdb_cls",
          "_write_pyc_fp",
          "_getscopeitem",
          "show_fixtures_per_test",
          "runpython",
          "visit_BinOp",
          "_pop_and_teardown",
          "suspend_capturing",
          "Class",
          "set_log_path",
          "_postmortem_traceback",
          "_getpytestargs",
          "_set_initial_conftests",
          "re_match_lines_random",
          "excinfo",
          "CaptureFixture",
          "PrintHelp",
          "cache",
          "messages",
          "capstdout",
          "WarningReport",
          "_parse_parametrize_parameters",
          "fromdictargs",
          "PYC_TAIL",
          "runpytest_inprocess",
          "DEFAULT_MAX_LINES",
          "summary_passes",
          "_seen",
          "SubRequest",
          "setup",
          "OK",
          "TERMINALWRITER_WRITER",
          "COLLECT_DIRECTORY_HOOK",
          "showfspath",
          "getfixturevalue",
          "EMPTY_BUFFER",
          "_addexcinfo",
          "Metafunc",
          "fnmatch_ex",
          "ApproxMapping",
          "_get_active_fixturedef",
          "visit_Call",
          "_determine_main_color",
          "_R",
          "LineComp",
          "_striptext",
          "isdatacls",
          "SysCaptureBinary",
          "find_suffixes",
          "LFPlugin",
          "count_towards_summary",
          "autouse",
          "CLOSE_STDIN",
          "cached_result",
          "_FixtureFunction",
          "PytestConfigWarning",
          "add_report_section",
          "flow_marker",
          "pop",
          "DEFAULT_RELATIVE_TOLERANCE",
          "clear_cache",
          "deindent",
          "_check_non_top_pytest_plugins",
          "_callfinalizers",
          "capfdbinary",
          "Function",
          "section",
          "_markers",
          "addUnexpectedSuccess",
          "_pformat_dispatch",
          "repr_instance",
          "scopename2class",
          "_compare_eq_any",
          "ReprEntryNative",
          "doctest",
          "_get_continue_on_failure",
          "stop",
          "_REPORTCHARS_DEFAULT",
          "resultlog",
          "MultiCapture",
          "post_mortem",
          "evalxfail_key",
          "countoutcomes",
          "suspend_global_capture",
          "helper",
          "_report_kwargs_from_json",
          "getoption",
          "append_failure",
          "add_property",
          "write_ensure_prefix",
          "inline_run",
          "_ensure_immutable_ids",
          "teardown_exact",
          "delenv",
          "NotSetType",
          "copy_example",
          "assert_contains_lines",
          "truncate_locals",
          "_collect",
          "append",
          "FSCollector",
          "mangle_test_address",
          "value",
          "NodeMeta",
          "parse_known_args",
          "pytest_generate_tests",
          "PYTEST_COLLECT_MODULE",
          "kwargs",
          "FSHookProxy",
          "cache_dir_from_config",
          "Frame",
          "makepyprojecttoml",
          "_is_skipped",
          "ImportMode",
          "assert_never",
          "head_line",
          "pytest_sessionstart",
          "find_prefixed",
          "Testdir",
          "get_real_func",
          "get_plugin_manager",
          "_basetemp",
          "repr_args",
          "_cachedir",
          "_patch_unwrap_mock_aware",
          "make_numbered_dir_with_cleanup",
          "_match_lines_random",
          "relline",
          "startTest",
          "getcalls",
          "errisinstance",
          "_check_type",
          "_state",
          "USAGE_ERROR",
          "_inject_setup_method_fixture",
          "get",
          "_mark_plugins_for_rewrite",
          "reprfuncargs",
          "add_stats",
          "fill_unfilled",
          "_plugin_nameversions",
          "ensure_deletable",
          "_compare_eq_iterable",
          "NO_TESTS_COLLECTED",
          "getfuncargnames",
          "getreportopt",
          "MoreQuietAction",
          "pytest_collection_modifyitems",
          "AssertionState",
          "showlongtestinfo",
          "FixtureFunctionMarker",
          "_get_assertion_exprs",
          "_non_numeric_type_error",
          "_F",
          "console_main",
          "iter_markers_with_node",
          "PYTEST_TAG",
          "consider_pluginarg",
          "determine_setup",
          "Traceback",
          "NODE_USE_FROM_PARENT",
          "scopes",
          "_suspend",
          "_node_location_to_relpath",
          "safe_getattr",
          "abspath",
          "PytestCollectionWarning",
          "_LiveLoggingStreamHandler",
          "extract_from",
          "ischildnode",
          "_rget_with_confmod",
          "setmulti2",
          "tracebackcutdir",
          "_consider_importhook",
          "filter_traceback_for_conftest_import_failure",
          "import_path",
          "global_and_fixture_disabled",
          "statement",
          "_AnyPurePath",
          "clear",
          "TokenType",
          "parsefactories",
          "get_extended_length_path_str",
          "BINOP_MAP",
          "create_cleanup_lock",
          "_NodeReporter",
          "__delitem__",
          "version_tuple",
          "longrepr",
          "bin_xml_escape",
          "_names",
          "context",
          "short_test_summary",
          "__next__",
          "format_session_duration",
          "FDCapture",
          "get_option_ini",
          "runtest",
          "D",
          "DoctestTextfile",
          "report_collect",
          "addsection",
          "write_captured_output",
          "fail_marker",
          "is_globally_capturing",
          "funcargs",
          "V",
          "get_public_names",
          "_compare_eq_set",
          "_make_xunit_fixture",
          "to_xml",
          "getexplanation",
          "suspend",
          "_version",
          "get_location",
          "Argument",
          "path",
          "default_plugins",
          "USAGE_MSG",
          "_get_line_with_reprcrash_message",
          "_ensure_basetemp",
          "addError",
          "call_optional",
          "_PluggyPlugin",
          "_getglobals",
          "hasinit",
          "_warn_already_imported",
          "pytest_report_from_serializable",
          "SEP",
          "start_global_capturing",
          "_addfinalizer",
          "ReprEntry",
          "funcnamefilter",
          "safeformat",
          "find_spec",
          "_makepath",
          "_format_action_invocation",
          "_getvaluepath",
          "reprfileloc",
          "compile",
          "getfailures",
          "SysPathsSnapshot",
          "_T",
          "_get_global_properties_node",
          "getpathnode",
          "__dir__",
          "_runtest_for",
          "Instance",
          "warning_record_to_str",
          "_warn_or_fail_if_strict",
          "token",
          "pytest_report_teststatus",
          "ApproxBase",
          "skipping",
          "Session",
          "addinivalue_line",
          "Exit",
          "path_matches_patterns",
          "makepyfile",
          "Source",
          "_get_runner",
          "getparam",
          "PercentStyleMultiline",
          "PytestPluginManager",
          "pytest_collection",
          "__setitem__",
          "_preparse",
          "_running_on_ci",
          "pop_outerr_to_orig",
          "create_module",
          "_assertion_pass",
          "_teardown_yield_fixture",
          "_get_multicapture",
          "stream",
          "close",
          "at_level",
          "_prepareconfig",
          "PyCollector",
          "DoctestModule",
          "_init_pdb",
          "_inject_setup_function_fixture",
          "and_expr",
          "start",
          "_ANSI_ESCAPE_SEQ",
          "patchsysdict",
          "_processopt",
          "longreprtext",
          "TempdirFactory",
          "getfslineno",
          "_ET",
          "wrap_session",
          "maybe_delete_a_numbered_dir",
          "_has_param_ids",
          "pytest_fixture_setup",
          "visit_UnaryOp",
          "pytest_runtest_logreport",
          "ihook",
          "rm_rf",
          "_WithException",
          "emit",
          "_log",
          "reprentries",
          "pytest_runtest_teardown",
          "add_global_property",
          "prepare",
          "_split_lines",
          "ApproxDecimal",
          "getvalue",
          "wcwidth",
          "pytest_load_initial_conftests",
          "pytest_terminal_summary",
          "spawn",
          "no_fnmatch_line",
          "markup",
          "outcomes",
          "sections",
          "addoption",
          "result",
          "_compute_fixture_value",
          "ApproxScalar",
          "from_call",
          "getmodpath",
          "wasvalid",
          "_CACHE_PREFIX_DIRS",
          "_importconftest",
          "resume_capturing",
          "_tmppath_factory",
          "reprcrash",
          "module",
          "_prunetraceback",
          "reorder_items",
          "_ascii_escaped_by_config",
          "start_capturing",
          "freeze_includes",
          "pastebin",
          "invocation_dir",
          "_get_first_non_fixture_func",
          "PytestArg",
          "xfail",
          "K",
          "node_reporter",
          "tbfilter",
          "_inject_setup_module_fixture",
          "write_line",
          "locate_config",
          "parseconfig",
          "gethookproxy",
          "name",
          "pytest_itemcollected",
          "_add_simple",
          "deprecated_call",
          "append_collect_skipped",
          "_collectfile",
          "_gethookproxy",
          "_color_for_type",
          "register_cleanup_lock_removal",
          "pytest_plugin_registered",
          "pytest_runtest_call",
          "for_config",
          "PseudoFixtureDef",
          "_reprcompare",
          "unexpectedsuccess_key",
          "pytest_cmdline_parse",
          "_getcrashline",
          "handler",
          "_get_number_flag",
          "_S",
          "INTERRUPTED",
          "_readline_workaround",
          "_teardown_towards",
          "exitstatus",
          "_ensure_relative_to_basetemp",
          "_check_record_param_type",
          "_PLUGGY_DIR",
          "_getentrysource",
          "_py36_windowsconsoleio_workaround",
          "resultlog_key",
          "_HandlerType",
          "create_new_paste",
          "prune_dependency_tree",
          "from_config",
          "pytest_assertrepr_compare",
          "NoCapture",
          "ExceptionInfo",
          "_truncate_recursive_traceback",
          "_param_ids_generated",
          "_format_args",
          "should_do_markup",
          "Config",
          "pytest_runtestloop",
          "_get_main_color",
          "yield_fixture",
          "_setupstate",
          "_resolve_arg_ids",
          "_colorama_workaround",
          "RunResult",
          "_log_text",
          "width_of_current_line",
          "pytest_runtest_logfinish",
          "_write_progress_information_filling_space",
          "resolve_from_str",
          "readlines",
          "__getitem__",
          "parse_hookimpl_opts",
          "_get_allow_unicode_flag",
          "teardown_all",
          "Token",
          "_mk_tmp",
          "_importtestmodule",
          "write_docstring",
          "addFailure",
          "setdefault",
          "PyobjMixin",
          "combined_with",
          "_makefile",
          "unittest",
          "lineno",
          "getstatementrange_ast",
          "pytest_sessionfinish",
          "getfirstlinesource",
          "PytestUnknownMarkWarning",
          "scope",
          "AlwaysDispatchingPrettyPrinter",
          "repr_traceback",
          "TEST_OUTCOME",
          "parse_known_and_unknown_args",
          "runner",
          "applymarker",
          "_pytest",
          "_istrue",
          "PdbInvoke",
          "_diff_text",
          "co_equal",
          "DropShorterLongHelpFormatter",
          "pytest_cmdline_main",
          "_pluginmanager",
          "evalcache_key",
          "pytest_cmdline_preparse",
          "formatrepr",
          "_E",
          "_create",
          "isiterable",
          "deselect_by_mark",
          "pytest_make_parametrize_id",
          "pytest_internalerror",
          "LPAREN",
          "get_open_files",
          "pytest_report_collectionfinish",
          "families",
          "resolve_package_path",
          "_PYTEST_DIR",
          "main",
          "TracebackEntry",
          "_param_ids_from",
          "_getnextfixturedef",
          "getfailedcollections",
          "KeywordMatcher",
          "chain",
          "get_timeout_config_value",
          "add_attribute",
          "DEFAULT_ABSOLUTE_TOLERANCE",
          "set_level",
          "TerminalWriter",
          "call_and_report",
          "visit_Starred",
          "catch_warnings_for_item",
          "DOCTEST_REPORT_CHOICE_NONE",
          "get_dirs_from_args",
          "mktemp",
          "__ne__",
          "InvocationParams",
          "copy",
          "__test__",
          "OutcomeException",
          "pytestconfig",
          "capstderr",
          "_ensure_removed_sysmodule",
          "JUNIT_XML_DEFAULT_FAMILY",
          "from_current",
          "isset",
          "markname",
          "_given_basetemp",
          "popen",
          "_split_explanation",
          "_assert_start_repr",
          "HelpAction",
          "consider_conftest",
          "execute",
          "UNARY_MAP",
          "nextitem",
          "approx",
          "_addoption_instance",
          "_color_for_type_default",
          "LineMatcher_fixture",
          "plugins",
          "_init_checker_class",
          "_excinfo",
          "assertstate_key",
          "derive_importpath",
          "_testcase",
          "_iter_all_modules",
          "category",
          "isatty",
          "_to_json",
          "LsofFdLeakChecker",
          "__eq__",
          "_recurse",
          "getrawcode",
          "args",
          "_teardown_with_finalization",
          "handleError",
          "pop_format_context",
          "CFG_PYTEST_SECTION",
          "REGEX_TYPE",
          "listchain",
          "_format_boolop",
          "nofuncargs",
          "_in_venv",
          "AssertionRewriter",
          "TimeoutExpired",
          "_call_reprcompare",
          "_format_lines",
          "__missing__",
          "_finalize",
          "File",
          "_call_assertion_pass",
          "_matchfactories",
          "addfinalizer",
          "_getconftestmodules",
          "getfixturemarker",
          "PytestWarning",
          "for_later",
          "_genfunctions",
          "_idval",
          "template",
          "parse_setoption",
          "get_statement_startend2",
          "listnames",
          "_result",
          "getbasetemp",
          "FILE_OR_DIR",
          "_trace",
          "EMPTY_PARAMETERSET_OPTION",
          "readline",
          "getreports",
          "visit_Name",
          "_checkargnotcontained",
          "_setup_fixtures",
          "TerminalReporter",
          "__hash__",
          "consider_env",
          "is_rewrite_disabled",
          "call_fixture_func",
          "gethookrecorder",
          "get_log_level_for_setting",
          "repr_excinfo",
          "ExitCode",
          "syspath_prepend",
          "cache_key",
          "set_trace",
          "__iter__",
          "XFailed",
          "mark",
          "pytest_pycollect_makeitem",
          "EOF",
          "MarkMatcher",
          "initialnames",
          "UsageError",
          "_getfailureheadline",
          "fixtures",
          "python",
          "rex_session_duration",
          "istrue",
          "PdbTrace",
          "record_tuples",
          "_dump_lines",
          "AssertionRewritingHook",
          "call_runtest_hook",
          "Notset",
          "cached_eval",
          "setitem",
          "LOCK_TIMEOUT",
          "_schedule_finalizers",
          "_getlines",
          "cacheshow",
          "TerminalRepr",
          "_fixturemanager",
          "attrs",
          "snap",
          "undo",
          "_check_all_skipped",
          "_validate_plugins",
          "_is_mocked",
          "istestclass",
          "_legal_xml_re",
          "buffer",
          "validate_basetemp",
          "function",
          "set_repr_style",
          "_addoption",
          "CaptureIO",
          "__getattr__",
          "ReprFuncArgs",
          "FUNCARGNAMES",
          "MINUS_K_COLON",
          "append_skipped",
          "_get_stderr_fileno",
          "wrap_function_to_error_out_if_called_directly",
          "OptionGroup",
          "runitem",
          "collect_one_node",
          "format_explanation",
          "ReprLocals",
          "get_real_method",
          "rewrite_asserts",
          "evaluate",
          "runpytest_subprocess",
          "PytestUnhandledCoroutineWarning",
          "get_cache_dir",
          "append_error",
          "testdir",
          "pytest_keyboard_interrupt",
          "_width_of_current_line",
          "display",
          "_bestrelpath_cache",
          "is_potential_nosetest",
          "_truncate_by_char_count",
          "create_terminal_writer",
          "pytester",
          "cut",
          "pytest_collect_file",
          "set_when",
          "ResultLog",
          "STRING_TYPES",
          "FixtureDef",
          "capture",
          "Package",
          "_write_report_lines_from_hooks",
          "_translate_non_printable",
          "_highlight",
          "records",
          "node",
          "_disable_output_capturing_for_darwin",
          "truncate_if_required",
          "strip",
          "_getindent",
          "scopemismatch",
          "filter",
          "makefile",
          "get_user",
          "read_global_capture",
          "RUNNER_CLASS",
          "reports",
          "MINUS_K_DASH",
          "_legal_chars",
          "load_config_dict_from_file",
          "TestReport",
          "SetupState",
          "_surrounding_parens_on_own_lines",
          "flush",
          "pytest_runtest_logstart",
          "cleanup_numbered_dir",
          "config",
          "extract_suffixes",
          "_folded_skips",
          "tmp_path",
          "check_xfail_no_run",
          "getfixtureinfo",
          "obj",
          "WarningsRecorder",
          "parse",
          "_should_repr_global_name",
          "NOTSET",
          "inline_genitems",
          "set_location",
          "Scanner",
          "_compare_eq_dict",
          "with_args",
          "stopTest",
          "IDENT",
          "MarkEvaluator",
          "_eval_scope_callable",
          "compile_",
          "delitem",
          "param",
          "get_scope_node",
          "__repr__",
          "getitem",
          "_format_repr_exception",
          "nose",
          "capfd",
          "ensure_extended_length_path",
          "CwdSnapshot",
          "get_optionflags",
          "_is_doctest",
          "DoctestItem",
          "pytest_leave_pdb",
          "_pyfuncitem",
          "_is_setup_py",
          "_write_entry_lines",
          "hasnew",
          "parts",
          "get_common_ancestor",
          "teardown",
          "cleanup_candidates",
          "nodeid"
        ]
      },
      "response": {
        "keywords": [
          "cached_eval",
          "MarkEvaluator",
          "skipif",
          "xfail",
          "evaluate",
          "pytest",
          "_istrue",
          "evaluate",
          "test_module_1",
          "test_module_2"
        ]
      }
    },
    {
      "name": "FilesKeywordSearch",
      "metadata": {
        "start": "2024-08-19T13:46:10.794350",
        "usage": "261.30104064941406 ms"
      },
      "request": {
        "tokens": [
          "cached_eval",
          "MarkEvaluator",
          "skipif",
          "xfail",
          "evaluate",
          "pytest",
          "_istrue",
          "evaluate",
          "test_module_1",
          "test_module_2"
        ]
      },
      "response": {
        "results": {
          "pytest": [
            "src/_pytest/deprecated.py",
            "src/_pytest/faulthandler.py",
            "src/_pytest/logging.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/pastebin.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/pytest/__init__.py",
            "src/_pytest/recwarn.py",
            "src/pytest/collect.py",
            "src/_pytest/skipping.py",
            "src/_pytest/pytester.py",
            "src/_pytest/monkeypatch.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/main.py",
            "src/_pytest/timing.py",
            "src/_pytest/warnings.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/capture.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/nodes.py",
            "src/_pytest/unittest.py",
            "src/pytest/__main__.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/runner.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/compat.py",
            "src/_pytest/debugging.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/config/argparsing.py",
            "src/_pytest/terminal.py",
            "src/_pytest/python_api.py",
            "src/_pytest/reports.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/freeze_support.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/mark/__init__.py"
          ],
          "xfail": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/unittest.py",
            "src/_pytest/mark/__init__.py"
          ],
          "evaluate": [
            "src/_pytest/skipping.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/mark/__init__.py"
          ],
          "MarkEvaluator": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/evaluate.py"
          ],
          "skipif": [
            "src/_pytest/python.py",
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/outcomes.py"
          ],
          "cached_eval": [
            "src/_pytest/mark/evaluate.py"
          ],
          "_istrue": [
            "src/_pytest/mark/evaluate.py"
          ]
        }
      }
    },
    {
      "name": "FilesFilter",
      "metadata": {
        "start": "2024-08-19T13:46:11.055661",
        "usage": "6350.908994674683 ms"
      },
      "request": {
        "keyword_search_results": {
          "pytest": [
            "src/_pytest/deprecated.py",
            "src/_pytest/faulthandler.py",
            "src/_pytest/logging.py",
            "src/_pytest/config/findpaths.py",
            "src/_pytest/pastebin.py",
            "src/_pytest/python.py",
            "src/_pytest/cacheprovider.py",
            "src/pytest/__init__.py",
            "src/_pytest/recwarn.py",
            "src/pytest/collect.py",
            "src/_pytest/skipping.py",
            "src/_pytest/pytester.py",
            "src/_pytest/monkeypatch.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/main.py",
            "src/_pytest/timing.py",
            "src/_pytest/warnings.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/capture.py",
            "src/_pytest/tmpdir.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/nodes.py",
            "src/_pytest/unittest.py",
            "src/pytest/__main__.py",
            "src/_pytest/setuponly.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/setupplan.py",
            "src/_pytest/doctest.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/runner.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/stepwise.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/compat.py",
            "src/_pytest/debugging.py",
            "src/_pytest/warning_types.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/config/argparsing.py",
            "src/_pytest/terminal.py",
            "src/_pytest/python_api.py",
            "src/_pytest/reports.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/freeze_support.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/mark/__init__.py"
          ],
          "xfail": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/python.py",
            "src/pytest/__init__.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/unittest.py",
            "src/_pytest/mark/__init__.py"
          ],
          "evaluate": [
            "src/_pytest/skipping.py",
            "src/_pytest/_code/code.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/mark/expression.py",
            "src/_pytest/mark/__init__.py"
          ],
          "MarkEvaluator": [
            "src/_pytest/skipping.py",
            "src/_pytest/mark/evaluate.py"
          ],
          "skipif": [
            "src/_pytest/python.py",
            "src/_pytest/skipping.py",
            "src/_pytest/mark/structures.py",
            "src/_pytest/outcomes.py"
          ],
          "cached_eval": [
            "src/_pytest/mark/evaluate.py"
          ],
          "_istrue": [
            "src/_pytest/mark/evaluate.py"
          ]
        }
      },
      "response": {
        "scratch_pad": "The reported issue relates to incorrect caching for the `@pytest.mark.skipif` and `@pytest.mark.xfail` annotations due to the caching mechanism in `cached_eval`. The key relevant components here are the caching mechanism and how it evaluates the conditions relative to the globals within the item. \n\nThe problem mentions that the function `cached_eval` found in `_pytest/mark/evaluate.py` is central to the issue, as it's where the evaluation caching happens. Additionally, the solution suggests inlining `cached_eval` into `MarkEvaluator._istrue`, both of which are also in the file `src/_pytest/mark/evaluate.py`. This file is, therefore, crucial for addressing the caching problem.\n\nFurthermore, any references to `MarkEvaluator`, which handles the evaluation process, could be pertinent to the issue, and this keyword points again to `src/_pytest/mark/evaluate.py` as well as `src/_pytest/skipping.py`. \n\nThe keywords `skipif` and `evaluate` both point to multiple files, but `src/_pytest/skipping.py` would be of interest because it's specifically related to the handling of the `skipif` and possible `xfail` logic which directly relate to the issue described.\n\nBased on these details, the files most relevant to the reported issue are:\n- `src/_pytest/mark/evaluate.py`: Directly contains `cached_eval` and `MarkEvaluator._istrue` which are central to the problem and proposed solution.\n- `src/_pytest/skipping.py`: Likely involved with handling logic relevant to `skipif` and `xfail`.",
        "file_paths": [
          "src/_pytest/mark/evaluate.py",
          "src/_pytest/skipping.py"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:17.406584",
        "usage": "6507.467985153198 ms"
      },
      "request": {
        "file_path": "src/_pytest/mark/evaluate.py"
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 The reported issue describes a problem with caching of the evaluation of string conditions in pytest markers like `@pytest.mark.skipif`. Specifically, the caching key is the string itself, leading to incorrect evaluations when the same string appears in multiple modules with different global variables.\n1.2 Yes, the user provides an example with modules `test_module_1.py` and `test_module_2.py` to demonstrate the issue. In `test_module_1`, the `skip` variable is `True`, so the test should be skipped, whereas in `test_module_2`, the `skip` variable is `False`, so the test should not be skipped.\n1.3 The issue is caused by caching the evaluation based solely on the string, ignoring the global context in which the string is evaluated, leading to the same evaluation result for both tests despite different contexts.\n1.4 The user's expectation is that `test_should_skip` is skipped and `test_should_not_skip` is not skipped, but currently, both are skipped due to incorrect caching.\n1.6 A possible solution is to remove the caching of the evaluations and directly evaluate them in the `MarkEvaluator._istrue` method.\n\n2. Make File Evaluation:\n2.1 The `evaluate.py` file contains a `cached_eval` function which utilizes caching for evaluation results based on the expression string.\n2.2 The key element related to this issue is the `cached_eval` function and its interaction with the `MarkEvaluator._istrue` method. The `MarkEvaluator` class is responsible for evaluating the `pytest.mark` conditions.\n2.3 The `cached_eval` function is directly responsible for caching the evaluation, which aligns with the reported issue.\n2.4 The `MarkEvaluator._istrue()` method uses `cached_eval` to evaluate expressions, hence it plays a critical role in the issue.\n2.5 The file provides implementation for the relevant methods and shows signs of being relevant to the root cause of the issue.\n2.6 It seems that the `cached_eval` function is the root cause as it decides how expressions are cached, and it is improperly caching based on string alone.",
        "relevant": true,
        "root_cause": true,
        "relevant_files": [],
        "relevant_symbols": [
          "cached_eval",
          "MarkEvaluator._istrue",
          "MarkEvaluator._getglobals"
        ],
        "root_cause_symbols": [
          "cached_eval"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:23.914065",
        "usage": "7892.528057098389 ms"
      },
      "request": {
        "file_path": "src/_pytest/skipping.py"
      },
      "response": {
        "scratch_pad": "1. **Understand the Issue**:\n   - The issue is about incorrect caching behavior in pytest for conditions in `@pytest.mark.skipif` and potentially `@pytest.mark.xfail`.\n   - Pytest caches the result of the condition expressed as a string, but this condition may depend on globals from different modules leading to incorrect skipping of tests.\n   - Example provided shows two modules with different globals that affect the skipping condition, but due to caching, both tests are skipped when they shouldn't be.\n\n2. **Source of the Issue**:\n   - The caching mechanism in `cached_eval` ignores contextual differences between modules and global variables affecting the evaluation.\n   - This caching mechanism is mentioned to exist in `cached_eval` within `_pytest/mark/evaluate.py`.\n\n3. **Expectation and Cause**:\n   - Users expect that `@pytest.mark.skipif` conditions are evaluated accurately according to the module-scoped globals.\n   - The incorrect behavior stems from caching evaluations without considering module context.\n\n4. **Suggested Solution**:\n   - The proposed fix is to remove the caching so that conditions are evaluated in their correct context each time.\n   \n5. **Evaluation of `src/_pytest/skipping.py`**:\n   - This file contains logic dealing with skip and xfail markers.\n   - Functions like `pytest_runtest_setup`, `check_xfail_no_run`, and `Check_strict_xfail` are key in handling skip/xfail logic.\n   - `MarkEvaluator` which seems responsible for evaluating the conditions of skipif and xfail is used here.\n   - The hook function `pytest_runtest_setup` utilizes `MarkEvaluator` to determine if a test should be skipped based on conditions.\n   - This file uses `MarkEvaluator`, but the main issue seems to stem from `_pytest/mark/evaluate.py` where `cached_eval` likely resides, which was not part of this file's content.\n\n6. **Conclusion**:\n   - The file contains relevant logic for skip/xfail handling but doesn't seem to directly contain the caching error.\n   - The root problem seems to be located in `_pytest/mark/evaluate.py`, not here.\n\n7. **Further Steps**:\n   - Investigate `_pytest/mark/evaluate.py` for `cached_eval` implementation since it's the main suspect for caching issue.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/mark/evaluate.py"
        ],
        "relevant_symbols": [
          "pytest_runtest_setup",
          "MarkEvaluator.istrue",
          "check_xfail_no_run",
          "check_strict_xfail",
          "pytest_runtest_makereport"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileSymbolSearch",
      "metadata": {
        "start": "2024-08-19T13:46:31.806627",
        "usage": "105.54099082946777 ms"
      },
      "request": {
        "files": [
          {
            "file_path": "src/_pytest/mark/evaluate.py",
            "relevant_symbol_keys": [
              "cached_eval",
              "MarkEvaluator._istrue",
              "MarkEvaluator._getglobals"
            ],
            "root_cause_symbol_keys": [
              "cached_eval"
            ]
          },
          {
            "file_path": "src/_pytest/skipping.py",
            "relevant_symbol_keys": [
              "pytest_runtest_setup",
              "MarkEvaluator.istrue",
              "check_xfail_no_run",
              "check_strict_xfail",
              "pytest_runtest_makereport"
            ],
            "root_cause_symbol_keys": []
          }
        ],
        "with_content": true
      },
      "response": {
        "related_symbols": [
          {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
          }
        ]
      }
    },
    {
      "name": "SymbolsEvaluate",
      "metadata": {
        "start": "2024-08-19T13:46:31.912174",
        "usage": "10221.174001693726 ms"
      },
      "request": {
        "related_symbols": [
          {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
          }
        ]
      },
      "response": {
        "scratch_pad": "In the reported issue, pytest incorrectly skips a test due to caching of evaluated conditions that depend on module-specific globals. Specifically, when using `@pytest.mark.skipif`, the conditions get cached using only the condition string as a key, but the evaluation also relies on the module's globals, which can differ across modules/files as shown in the provided examples.\n\nTo fix this, the suggestion is to remove the caching altogether from `cached_eval` and directly incorporate the evaluation into the `MarkEvaluator._istrue` method. This would eliminate caching based solely on the condition string.\n\nSteps to solve the issue based on analysis:\n1. The primary problem is with the `cached_eval` method in `src/_pytest/mark/evaluate.py`. It caches the condition expression without considering different global namespaces from different modules leading to incorrect test execution results. The proposal is to remove caching to avoid evaluating with stale or incorrect global contexts.\n2. Another function, `MarkEvaluator._istrue` relies on `cached_eval`. Since it processes the marks, moving the expression evaluation logic directly into this method while ensuring the global context is newly retrieved each time would ensure correct behavior.\n3. Other hook implementations such as `pytest_runtest_setup` in `src/_pytest/skipping.py` use `MarkEvaluator`. Verifying or refactoring their reliance on `MarkEvaluator` should ensure consistent evaluation.",
        "symbols": [
          {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "reason": "This function caches the evaluation of the expression, causing incorrect results due to incorrect context dependency."
          },
          {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "reason": "This function is responsible for calling cached_eval. Removing caching and calling eval directly here would correct the issue."
          }
        ],
        "more_info": [],
        "is_done": true,
        "symbols_scratch_pad": [
          "The `cached_eval` function in `src/_pytest/mark/evaluate.py` is responsible for caching conditions based solely on the condition string. It's a clear candidate for change because it doesn't account for differing globals in different modules which leads to incorrect skip decision caching.",
          "The `MarkEvaluator._getglobals` method retrieves the global context of the item, allowing the skip condition to be evaluated within the correct context. This method is indirectly involved in the issue since it provides context data.",
          "The `_istrue` method in `MarkEvaluator` is critical to the solution as it decides if a mark condition is True or False. It's necessary to alter this method to avoid using `cached_eval` and instead use direct evaluation. Removing the cache and moving evaluation logic here would address the issue.",
          "Not directly related. This symbol pertains to capturing setup events in tests.",
          "Not directly related. This involves hook specification for pytest test setup, unrelated to mark evaluation.",
          "Not directly related. This involves making reports of tests, unrelated to mark evaluation.",
          "Not directly related. This involves logging setups in testing, unrelated to mark evaluations.",
          "Symbol is not directly related to the issue. It seems to be a partial or placeholder definition or note.",
          "Not directly related. This involves runner function related to test setup, unrelated to mark evaluations.",
          "Not directly related. This involves making reports of tests, unrelated to mark evaluation.",
          "Not directly related. This involves unittest setup in testing, unrelated to mark evaluations.",
          "Not directly related. This involves nose setup in testing, unrelated to mark evaluations."
        ]
      }
    },
    {
      "name": "SymbolSummary",
      "metadata": {
        "start": "2024-08-19T13:46:42.133368",
        "usage": "1.5079975128173828 ms"
      },
      "request": {
        "symbols": {
          "related_symbols": [
            {
              "name": "_pytest.mark.MarkEvaluator._getglobals",
              "file_path": "src/_pytest/mark/evaluate.py",
              "file_content": "def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d",
              "start_line": 75,
              "start_column": 2284,
              "end_line": 4,
              "end_column": 2558,
              "start_byte": 2284,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator._istrue",
              "file_path": "src/_pytest/mark/evaluate.py",
              "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
              "start_line": 81,
              "start_column": 2564,
              "end_line": 4,
              "end_column": 4143,
              "start_byte": 2564,
              "end_byte": 4
            },
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 735,
              "start_column": 23643,
              "end_line": 4,
              "end_column": 23821,
              "start_byte": 23643,
              "end_byte": 4
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 420,
              "start_column": 14531,
              "end_line": 0,
              "end_column": 14635,
              "start_byte": 14531,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 438,
              "start_column": 15172,
              "end_line": 0,
              "end_column": 15521,
              "start_byte": 15172,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 675,
              "start_column": 24746,
              "end_line": 4,
              "end_column": 25079,
              "start_byte": 24746,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkEvaluator.istrue",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
              "start_line": 56,
              "start_column": 1516,
              "end_line": 4,
              "end_column": 2278,
              "start_byte": 1516,
              "end_byte": 4
            },
            {
              "name": "_pytest.runner.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
              "start_line": 137,
              "start_column": 4365,
              "end_line": 0,
              "end_column": 4497,
              "start_byte": 4365,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 310,
              "start_column": 10281,
              "end_line": 0,
              "end_column": 10412,
              "start_byte": 10281,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
              "start_line": 82,
              "start_column": 2802,
              "end_line": 0,
              "end_column": 3543,
              "start_byte": 2802,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.check_xfail_no_run",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
              "start_line": 113,
              "start_column": 3768,
              "end_line": 0,
              "end_column": 4073,
              "start_byte": 3768,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.check_strict_xfail",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
              "start_line": 122,
              "start_column": 4076,
              "end_line": 0,
              "end_column": 4595,
              "start_byte": 4076,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
              "start_line": 134,
              "start_column": 4598,
              "end_line": 0,
              "end_column": 6754,
              "start_byte": 4598,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 290,
              "start_column": 10753,
              "end_line": 0,
              "end_column": 11542,
              "start_byte": 10753,
              "end_byte": 0
            },
            {
              "name": "_pytest.nose.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
              "start_line": 7,
              "start_column": 167,
              "end_line": 0,
              "end_column": 579,
              "start_byte": 167,
              "end_byte": 0
            }
          ],
          "root_caused_symbols": [
            {
              "name": "_pytest.mark.cached_eval",
              "file_path": "src/_pytest/mark/evaluate.py",
              "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
              "start_line": 20,
              "start_column": 394,
              "end_line": 0,
              "end_column": 791,
              "start_byte": 394,
              "end_byte": 0
            }
          ]
        },
        "symbols_evaluate": {
          "scratch_pad": "In the reported issue, pytest incorrectly skips a test due to caching of evaluated conditions that depend on module-specific globals. Specifically, when using `@pytest.mark.skipif`, the conditions get cached using only the condition string as a key, but the evaluation also relies on the module's globals, which can differ across modules/files as shown in the provided examples.\n\nTo fix this, the suggestion is to remove the caching altogether from `cached_eval` and directly incorporate the evaluation into the `MarkEvaluator._istrue` method. This would eliminate caching based solely on the condition string.\n\nSteps to solve the issue based on analysis:\n1. The primary problem is with the `cached_eval` method in `src/_pytest/mark/evaluate.py`. It caches the condition expression without considering different global namespaces from different modules leading to incorrect test execution results. The proposal is to remove caching to avoid evaluating with stale or incorrect global contexts.\n2. Another function, `MarkEvaluator._istrue` relies on `cached_eval`. Since it processes the marks, moving the expression evaluation logic directly into this method while ensuring the global context is newly retrieved each time would ensure correct behavior.\n3. Other hook implementations such as `pytest_runtest_setup` in `src/_pytest/skipping.py` use `MarkEvaluator`. Verifying or refactoring their reliance on `MarkEvaluator` should ensure consistent evaluation.",
          "symbols": [
            {
              "name": "_pytest.mark.cached_eval",
              "file_path": "src/_pytest/mark/evaluate.py",
              "reason": "This function caches the evaluation of the expression, causing incorrect results due to incorrect context dependency."
            },
            {
              "name": "_pytest.mark.MarkEvaluator._istrue",
              "file_path": "src/_pytest/mark/evaluate.py",
              "reason": "This function is responsible for calling cached_eval. Removing caching and calling eval directly here would correct the issue."
            }
          ],
          "more_info": [],
          "is_done": true,
          "symbols_scratch_pad": [
            "The `cached_eval` function in `src/_pytest/mark/evaluate.py` is responsible for caching conditions based solely on the condition string. It's a clear candidate for change because it doesn't account for differing globals in different modules which leads to incorrect skip decision caching.",
            "The `MarkEvaluator._getglobals` method retrieves the global context of the item, allowing the skip condition to be evaluated within the correct context. This method is indirectly involved in the issue since it provides context data.",
            "The `_istrue` method in `MarkEvaluator` is critical to the solution as it decides if a mark condition is True or False. It's necessary to alter this method to avoid using `cached_eval` and instead use direct evaluation. Removing the cache and moving evaluation logic here would address the issue.",
            "Not directly related. This symbol pertains to capturing setup events in tests.",
            "Not directly related. This involves hook specification for pytest test setup, unrelated to mark evaluation.",
            "Not directly related. This involves making reports of tests, unrelated to mark evaluation.",
            "Not directly related. This involves logging setups in testing, unrelated to mark evaluations.",
            "Symbol is not directly related to the issue. It seems to be a partial or placeholder definition or note.",
            "Not directly related. This involves runner function related to test setup, unrelated to mark evaluations.",
            "Not directly related. This involves making reports of tests, unrelated to mark evaluation.",
            "Not directly related. This involves unittest setup in testing, unrelated to mark evaluations.",
            "Not directly related. This involves nose setup in testing, unrelated to mark evaluations."
          ]
        }
      },
      "response": {
        "file_paths": [
          "src/_pytest/mark/evaluate.py"
        ],
        "symbols": [
          "_pytest.mark.cached_eval",
          "_pytest.mark.MarkEvaluator._istrue"
        ],
        "code_spans": {
          "_pytest.mark.MarkEvaluator._getglobals": {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator._istrue": {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
          },
          "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
          },
          "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_runtest_makereport": {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
          },
          "_pytest.mark.MarkEvaluator.istrue": {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
          },
          "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
          },
          "_pytest.runner.pytest_runtest_makereport": {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
          },
          "_pytest.skipping.check_xfail_no_run": {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
          },
          "_pytest.skipping.check_strict_xfail": {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
          },
          "_pytest.unittest.pytest_runtest_makereport": {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
          },
          "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
          },
          "_pytest.mark.cached_eval": {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
          }
        }
      }
    }
  ]
}