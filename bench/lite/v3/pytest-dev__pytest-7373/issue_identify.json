{
    "file_paths": [
        "src/_pytest/mark/evaluate.py"
    ],
    "symbols": [
        "_pytest.mark.cached_eval",
        "_pytest.mark.MarkEvaluator._istrue"
    ],
    "code_spans": {
        "_pytest.mark.MarkEvaluator._getglobals": {
            "name": "_pytest.mark.MarkEvaluator._getglobals",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d",
            "start_line": 75,
            "start_column": 2284,
            "end_line": 4,
            "end_column": 2558,
            "start_byte": 2284,
            "end_byte": 4
        },
        "_pytest.mark.MarkEvaluator._istrue": {
            "name": "_pytest.mark.MarkEvaluator._istrue",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False",
            "start_line": 81,
            "start_column": 2564,
            "end_line": 4,
            "end_column": 4143,
            "start_byte": 2564,
            "end_byte": 4
        },
        "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 735,
            "start_column": 23643,
            "end_line": 4,
            "end_column": 23821,
            "start_byte": 23643,
            "end_byte": 4
        },
        "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 420,
            "start_column": 14531,
            "end_line": 0,
            "end_column": 14635,
            "start_byte": 14531,
            "end_byte": 0
        },
        "_pytest.hookspec.pytest_runtest_makereport": {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 438,
            "start_column": 15172,
            "end_line": 0,
            "end_column": 15521,
            "start_byte": 15172,
            "end_byte": 0
        },
        "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 24746,
            "end_line": 4,
            "end_column": 25079,
            "start_byte": 24746,
            "end_byte": 4
        },
        "_pytest.mark.MarkEvaluator.istrue": {
            "name": "_pytest.mark.MarkEvaluator.istrue",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "# noqa: F821\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, rea",
            "start_line": 56,
            "start_column": 1516,
            "end_line": 4,
            "end_column": 2278,
            "start_byte": 1516,
            "end_byte": 4
        },
        "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "fail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanat",
            "start_line": 137,
            "start_column": 4365,
            "end_line": 0,
            "end_column": 4497,
            "start_byte": 4365,
            "end_byte": 0
        },
        "_pytest.runner.pytest_runtest_makereport": {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 310,
            "start_column": 10281,
            "end_line": 0,
            "end_column": 10412,
            "start_byte": 10281,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 82,
            "start_column": 2802,
            "end_line": 0,
            "end_column": 3543,
            "start_byte": 2802,
            "end_byte": 0
        },
        "_pytest.skipping.check_xfail_no_run": {
            "name": "_pytest.skipping.check_xfail_no_run",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_xfail_no_run(item: Item) -> None:\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())",
            "start_line": 113,
            "start_column": 3768,
            "end_line": 0,
            "end_column": 4073,
            "start_byte": 3768,
            "end_byte": 0
        },
        "_pytest.skipping.check_strict_xfail": {
            "name": "_pytest.skipping.check_strict_xfail",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def check_strict_xfail(pyfuncitem: Function) -> None:\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)",
            "start_line": 122,
            "start_column": 4076,
            "end_line": 0,
            "end_column": 4595,
            "start_byte": 4076,
            "end_byte": 0
        },
        "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        assert line is not None\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 134,
            "start_column": 4598,
            "end_line": 0,
            "end_column": 6754,
            "start_byte": 4598,
            "end_byte": 0
        },
        "_pytest.unittest.pytest_runtest_makereport": {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 290,
            "start_column": 10753,
            "end_line": 0,
            "end_column": 11542,
            "start_byte": 10753,
            "end_byte": 0
        },
        "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.python import Function\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = Stor",
            "start_line": 7,
            "start_column": 167,
            "end_line": 0,
            "end_column": 579,
            "start_byte": 167,
            "end_byte": 0
        },
        "_pytest.mark.cached_eval": {
            "name": "_pytest.mark.cached_eval",
            "file_path": "src/_pytest/mark/evaluate.py",
            "file_content": "def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x",
            "start_line": 20,
            "start_column": 394,
            "end_line": 0,
            "end_column": 791,
            "start_byte": 394,
            "end_byte": 0
        }
    }
}