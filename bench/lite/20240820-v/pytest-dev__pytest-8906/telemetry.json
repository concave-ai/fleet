{
  "total_cost": 0,
  "task_id": "pytest-dev__pytest-8906",
  "model": "gpt-4o-2024-08-06",
  "spans": [
    {
      "name": "KeywordExtract",
      "metadata": {
        "start": "2024-08-19T13:35:16.140683",
        "usage": "1274.256944656372 ms"
      },
      "request": {
        "repo_tokens": [
          "summary_passes",
          "Cache",
          "summary_failures",
          "visit_Name",
          "fixturenames",
          "push_format_context",
          "extract_suffixes",
          "extraline",
          "processoption",
          "fnmatch_lines",
          "reprfileloc",
          "_fixturemanager",
          "LogCaptureHandler",
          "append_collect_error",
          "_read_pyc",
          "getpluginversioninfo",
          "typename",
          "STRICT_OPTION",
          "cacheprovider",
          "summary_errors",
          "_plugin_nameversions",
          "ids",
          "append_collect_skipped",
          "exit",
          "traceback",
          "_consider_importhook",
          "_validate_args",
          "delattr",
          "filter_traceback_for_conftest_import_failure",
          "CallInfo",
          "write_fspath_result",
          "CLOSE_STDIN",
          "mkdir",
          "cwd_relative_nodeid",
          "getpathnode",
          "obj",
          "_parse_ini_config",
          "pytest_warning_captured",
          "_import_plugin_specs",
          "plugins",
          "_given_basetemp",
          "addSkip",
          "add_global_property",
          "store_mark",
          "OutcomeException",
          "parseconfig",
          "_format_args",
          "_ensure_basetemp",
          "_surrounding_parens_on_own_lines",
          "runpytest",
          "builtin_plugins",
          "__lt__",
          "DOCTEST_REPORT_CHOICE_NDIFF",
          "_getnextfixturedef",
          "_get_allow_unicode_flag",
          "HOOK_LEGACY_PATH_ARG",
          "check_interactive_exception",
          "id",
          "ARGUMENT_PERCENT_DEFAULT",
          "reprtraceback",
          "getitems",
          "pastebin",
          "capsys",
          "evaluate_skip_marks",
          "NFPlugin",
          "approx",
          "store",
          "startpath",
          "deselect_by_keyword",
          "_early_rewrite_bailout",
          "_get_increasing_order",
          "safeformat",
          "_determine_show_progress_info",
          "pytest_sessionstart",
          "writeorg",
          "_translate_non_printable",
          "rootpath",
          "cleanup_numbered_dir",
          "is_rewrite_disabled",
          "_no_match_line",
          "_get_plugin_specs_as_list",
          "_rewrite_test",
          "_assertion_supported",
          "consider_module",
          "Failed",
          "_get_raw_skip_reason",
          "V",
          "_ensure_supporting_files",
          "pathlib",
          "terminal",
          "PytestPluginManager",
          "buffer",
          "pytest_runtest_call",
          "TEST_OUTCOME",
          "suspend_global_capture",
          "SysCaptureBinary",
          "prepend",
          "get_lines_after",
          "pytest_internalerror",
          "pytestPDB",
          "runtest",
          "getrawcode",
          "__str__",
          "Exit",
          "getlocation",
          "UsageError",
          "SetupState",
          "RE_IMPORT_ERROR_NAME",
          "match",
          "Xfail",
          "write_ensure_prefix",
          "from_parent",
          "_check_scope",
          "pytester",
          "_inject_setup_method_fixture",
          "PytestUnraisableExceptionWarning",
          "_is_capturing",
          "Session",
          "PathAwareHookProxy",
          "inipath",
          "_write_pyc_fp",
          "activate_fixture",
          "stream",
          "setenv",
          "_F",
          "IGNORE_PAM",
          "_esctable",
          "pytest_report_collectionfinish",
          "classnamefilter",
          "args",
          "validate_basetemp",
          "reason",
          "assign",
          "get_plugin_manager",
          "MultipleDoctestFailures",
          "showlongtestinfo",
          "ReprLocals",
          "get_location",
          "_bestrelpath_cache",
          "expr",
          "NOTSET",
          "resolve_package_path",
          "_ET",
          "excinfo",
          "_ensure_relative_to_basetemp",
          "UnitTestCase",
          "fnmatch_lines_random",
          "LsofFdLeakChecker",
          "CallSpec2",
          "_iter_rewritable_modules",
          "raises",
          "_write_progress_information_filling_space",
          "addFailure",
          "cut",
          "readlines",
          "records",
          "pytest_assertrepr_compare",
          "getcrashentry",
          "is_async_function",
          "pytest_enter_pdb",
          "collect_one_node",
          "Module",
          "consider_conftest",
          "compile",
          "_try_repr_or_str",
          "reprcrash",
          "_config_for_test",
          "notset",
          "runpython_c",
          "_CACHE_PREFIX_VALUES",
          "pytest_pycollect_makeitem",
          "from_item",
          "frame",
          "getargs",
          "pytest_collectstart",
          "_checkversion",
          "set_when",
          "FDCapture",
          "_force_symlink",
          "countoutcomes",
          "scope",
          "CaptureIO",
          "__test__",
          "_LiveLoggingStreamHandler",
          "xfailed_key",
          "_init_runner_class",
          "LFPluginCollWrapper",
          "ApproxBase",
          "_reprcompare",
          "pytest_warning_recorded",
          "_validate_usepdb_cls",
          "get_stderr_fileno",
          "_validate_config_options",
          "pytest_collection_modifyitems",
          "DEFAULT_ABSOLUTE_TOLERANCE",
          "_is_last_item",
          "getfixturevalue",
          "_make_xunit_fixture",
          "isdatacls",
          "_get_main_color",
          "ExceptionRepr",
          "_getconftestmodules",
          "nofuncargs",
          "re_match_lines_random",
          "_matches_prefix_or_glob_option",
          "get_common_ancestor",
          "pop_format_context",
          "_color_for_type",
          "scope2index",
          "pytest_report_teststatus",
          "ReprFailDoctest",
          "PytestCollectionWarning",
          "_write_entry_lines",
          "NodeMeta",
          "search_pypath",
          "_collectfile",
          "_dump_lines",
          "fix_cache_order",
          "CollectErrorRepr",
          "_for_parametrize",
          "_try_load_conftest",
          "SysModulesSnapshot",
          "fault_handler_stderr_key",
          "SubRequest",
          "_report_to_json",
          "pluralize",
          "HelpAction",
          "type",
          "pytest_fixture_post_finalizer",
          "python",
          "stop_global_capturing",
          "ImportPathMismatchError",
          "build_summary_stats_line",
          "visit_Starred",
          "reports",
          "_IGNORED_WINERRORS",
          "extract_from",
          "Expression",
          "getreports",
          "mktemp",
          "initialnames",
          "fileno",
          "families",
          "_format_assertmsg",
          "DEFAULT_RELATIVE_TOLERANCE",
          "isdict",
          "LogCaptureFixture",
          "fail",
          "_warn_already_imported",
          "context",
          "explanation_param",
          "mkpydir",
          "from_function",
          "AssertionRewritingHook",
          "getoption",
          "from_call",
          "MoreQuietAction",
          "find_prefixed",
          "_PytestWrapper",
          "IDENT",
          "listextrakeywords",
          "getstatement",
          "template",
          "_AnyPurePath",
          "get_records",
          "maybe_delete_a_numbered_dir",
          "_call_assertion_pass",
          "_getscopeitem",
          "popen",
          "index",
          "pytest_collect_file",
          "conftest_options",
          "getfixtureclosure",
          "evaluate_xfail_marks",
          "readline",
          "Interrupted",
          "_has_param_ids",
          "_color_for_type_default",
          "TracebackEntry",
          "lines",
          "pytest_runtest_protocol",
          "record_tuples",
          "_assert_state",
          "summary_warnings",
          "_seen",
          "write_captured_output",
          "location",
          "_R",
          "_cachedir",
          "_resolve_arg_value_types",
          "done",
          "post_mortem",
          "pytest_deselected",
          "_NodeType",
          "UNITTEST_SKIP_DURING_COLLECTION",
          "__version__",
          "_approx_scalar",
          "NodeKeywords",
          "__setitem__",
          "skip",
          "_add_simple",
          "_is_marked_for_rewrite",
          "for_later",
          "_update_current_test_var",
          "record_xml_attribute",
          "NotSetType",
          "SysCapture",
          "resolve_from_str",
          "PytestAssertRewriteWarning",
          "print_teardown_sections",
          "name2pseudofixturedef_key",
          "append_error",
          "_truncate_explanation",
          "_folded_skips",
          "statement",
          "USAGE_ERROR",
          "Skip",
          "start_capturing",
          "list",
          "_report_kwargs_from_json",
          "pytest_plugin_registered",
          "line",
          "helpconfig",
          "TResult",
          "re_match_lines",
          "_ALLOW_MARKERS",
          "_write_content",
          "fault_handler_originally_enabled_key",
          "close",
          "PytestConfigWarning",
          "format",
          "_do_configure",
          "get_open_files",
          "call_runtest_hook",
          "node_reporter",
          "pytest_generate_tests",
          "compat",
          "_getentrysource",
          "doctest",
          "rex_outcome",
          "_tmppath_factory",
          "Function",
          "get_parametrized_fixture_keys",
          "tmpdir_factory",
          "exec_module",
          "PyobjMixin",
          "ReprExceptionInfo",
          "_getcrashline",
          "determine_setup",
          "caplog",
          "UNARY_MAP",
          "visit_Assert",
          "pytest_make_collect_report",
          "pytest_keyboard_interrupt",
          "patchsysdict",
          "syspathinsert",
          "scopes",
          "get_user",
          "parse_setoption",
          "BaseReport",
          "warning_record_to_str",
          "_getvaluepath",
          "_main",
          "simple",
          "File",
          "CwdSnapshot",
          "visit_BinOp",
          "HookRecorder",
          "LineMatcher",
          "pytest_collection",
          "console_main",
          "pytest_runtestloop",
          "_show_fixtures_per_test",
          "repr_instance",
          "apply_warning_filters",
          "show_test_item",
          "get_source",
          "getsource",
          "parse_known_and_unknown_args",
          "function",
          "_get_runner",
          "cleanup_candidates",
          "makefile",
          "setitem",
          "invocation_dir",
          "ApproxScalar",
          "FixtureValue",
          "start",
          "_handle_teardown_sections",
          "capture",
          "add_funcarg_pseudo_fixture_def",
          "importlib",
          "_get_verbose_word",
          "_create_formatter",
          "set_session",
          "get_lock_path",
          "STEPWISE_CACHE_DIR",
          "eval",
          "_version",
          "_start",
          "_typ_map",
          "_ensure_unconfigure",
          "__version_tuple__",
          "chdir",
          "_update_message",
          "_node_location_to_relpath",
          "runner",
          "readouterr",
          "_postmortem_traceback",
          "threadexception",
          "node",
          "_initini",
          "teardown_exact",
          "get_default_arg_names",
          "format_session_duration",
          "junitxml",
          "cls",
          "xml_key",
          "repr_args",
          "spawn",
          "ReprEntry",
          "Instance",
          "__dir__",
          "ReprEntryNative",
          "keywords",
          "STRING_TYPES",
          "_report_unserialization_failure",
          "_outrep_summary",
          "dir",
          "_recursive_debug",
          "module",
          "ReprTraceback",
          "NOT",
          "source",
          "_check_if_assertion_pass_impl",
          "resolve_fixture_function",
          "_get_teardown_reports",
          "reprlocals",
          "pytest_report_from_serializable",
          "pop_outerr_to_orig",
          "_log",
          "_showfixtures_main",
          "Class",
          "async_warn_and_skip",
          "reject",
          "wrap_function_to_error_out_if_called_directly",
          "imply_paths_hooks",
          "FILLFUNCARGS",
          "DEFAULT_MAX_LINES",
          "LEVELNAME_FMT_REGEX",
          "consider_preparse",
          "to_xml",
          "_pytest",
          "PytestExperimentalApiWarning",
          "__call__",
          "FSCOLLECTOR_GETHOOKPROXY_ISINITPATH",
          "handler",
          "_assertion_pass",
          "_truncate_recursive_traceback",
          "_width_of_current_line",
          "ensure_deletable",
          "applymarker",
          "_find_spec",
          "_PLUGGY_DIR",
          "_pyfuncitem",
          "USAGE_MSG",
          "python_api",
          "_compare_eq_verbose",
          "_import_pdb_cls",
          "TempdirFactory",
          "formatrepr",
          "assert_contains",
          "XFailed",
          "summary_stats",
          "head_line",
          "idmaker",
          "_inject_setup_class_fixture",
          "set_repr_style",
          "_ANSI_ESCAPE_SEQ",
          "_getindent",
          "iter_markers_with_node",
          "display",
          "isnamedtuple",
          "_replace",
          "EOF",
          "Node",
          "_printcollecteditems",
          "pytest_runtest_logstart",
          "_get_unknown_ini_keys",
          "add_property",
          "MarkDecorator",
          "maybe_wrap_pytest_function_for_tracing",
          "showlocals",
          "set_fixture",
          "_sys_snapshot",
          "register_assert_rewrite",
          "warning_types",
          "faulthandler",
          "_get_assertion_exprs",
          "directory_arg",
          "Package",
          "consider_env",
          "CHECKER_CLASS",
          "get_fslocation_from_item",
          "_strtobool",
          "rewrite",
          "showheader",
          "_get_override_ini_value",
          "__repr__",
          "getini",
          "_inject_setup_function_fixture",
          "helper",
          "unset_fixture",
          "istestfunction",
          "deactivate_fixture",
          "parse",
          "bestrelpath",
          "_running_on_ci",
          "_basetemp",
          "FastFilesCompleter",
          "showversion",
          "gethookproxy",
          "indent",
          "__getitem__",
          "get_empty_parameterset_mark",
          "EncodedFile",
          "outcome",
          "ihook",
          "prune_dependency_tree",
          "_importtestmodule",
          "no_header",
          "_get_checker",
          "_match_lines_random",
          "get_optionflags",
          "LogXML",
          "Skipped",
          "_set_main_color",
          "_compare_eq_set",
          "_is_skipped",
          "Pytester",
          "_should_repr_global_name",
          "pytest_exception_interact",
          "TestReport",
          "_init_pdb",
          "MultiCapture",
          "yield_fixture",
          "_resolve_arg_ids",
          "kwargs",
          "nodeid",
          "TestCaseFunction",
          "get_exconly",
          "_imply_path",
          "get_last_failed_paths",
          "_preparse",
          "_mark_plugins_for_rewrite",
          "TokenType",
          "UnformattedWarning",
          "visit_Compare",
          "getfslineno",
          "reorder_items",
          "message",
          "__init__",
          "visit_UnaryOp",
          "_is_numpy_array",
          "MINUS_K_COLON",
          "normalize_mark_list",
          "CFG_PYTEST_SECTION",
          "when",
          "_yield_comparisons",
          "isatty",
          "catch_warnings_for_item",
          "OK",
          "params",
          "_check_type",
          "ApproxMapping",
          "Source",
          "unraisable_exception_runtest_hook",
          "pytest_addoption",
          "parse_args",
          "pytest",
          "listchain",
          "get",
          "NoCapture",
          "_prepare_content",
          "YIELD_FIXTURE",
          "timing",
          "_validate_plugins",
          "__all__",
          "listnames",
          "_finalize",
          "copytree",
          "duration",
          "PYC_TAIL",
          "append",
          "hasopt",
          "getstatementrange_ast",
          "assert_contains_lines",
          "IGNORED_ATTRIBUTES",
          "fill_unfilled",
          "_set_opt_strings",
          "is_started",
          "pytest_pycollect_makemodule",
          "_genfunctions",
          "_format",
          "Config",
          "_readline_workaround",
          "issue_config_time_warning",
          "from_current",
          "bin_xml_escape",
          "tmp_path_factory",
          "clear_cache",
          "_getpytestargs",
          "get_closest_marker",
          "_warn_or_fail_if_strict",
          "NO_TESTS_COLLECTED",
          "longrepr",
          "REPORT_COLLECTING_RESOLUTION",
          "__delitem__",
          "get_real_func",
          "SysPathsSnapshot",
          "_is_mocked",
          "try_makedirs",
          "T",
          "funcargs",
          "get_log_level_for_setting",
          "RaisesContext",
          "_setupstate",
          "matchreport",
          "set_location",
          "essential_plugins",
          "parsefactories",
          "_should_rewrite",
          "ImportMode",
          "prog",
          "update_testcase_duration",
          "thread_exception_runtest_hook",
          "DEFAULT_LOG_DATE_FORMAT",
          "linecomp",
          "pytest_collectreport",
          "mark_rewrite",
          "unittest",
          "FILE_OR_DIR",
          "_pformat_dispatch",
          "_getparser",
          "_call_with_optional_argument",
          "generic_visit",
          "_get_auto_indent",
          "ParseError",
          "_getfailureheadline",
          "skipped",
          "reprentries",
          "flush",
          "CaptureResult",
          "capfdbinary",
          "abspath",
          "register_cleanup_lock_removal",
          "find_spec",
          "EMPTY_BUFFER",
          "rm_rf",
          "LPAREN",
          "DOCTEST_REPORT_CHOICES",
          "addSuccess",
          "old_mark_config_key",
          "istext",
          "safe_getattr",
          "get_scope_node",
          "addError",
          "AssertionState",
          "raiseerror",
          "pytester_assertions",
          "assertoutcome",
          "pytest_pyfunc_call",
          "ishidden",
          "_show_fixture_action",
          "getmodulecol",
          "_get_reports_to_display",
          "getreportopt",
          "_ignore_error",
          "DummyRewriteHook",
          "_check_all_skipped",
          "cacheshow",
          "Exception",
          "pytest_configure",
          "count",
          "copy_example",
          "_check_initialpaths_for_relpath",
          "_recurse",
          "WarningsRecorder",
          "xfail",
          "_getini",
          "_runtest_for",
          "symlink_or_skip",
          "FakeCollectModule",
          "logging",
          "_highlight",
          "warn",
          "run",
          "hookspec",
          "Item",
          "report_collect",
          "spawn_pytest",
          "messages",
          "_FixtureCachedResult",
          "resolve_collection_argument",
          "name2fixturedefs",
          "fslocation",
          "_state",
          "ExitCode",
          "astcache",
          "test_tmproot",
          "_FixtureFunc",
          "addoption",
          "reportinfo",
          "finish_recording",
          "repr",
          "ReprTracebackNative",
          "inline_runsource",
          "fromdictargs",
          "_get_direct_parametrize_args",
          "cmdline",
          "annotated_getattr",
          "get_unpacked_marks",
          "runpytest_inprocess",
          "capstdout",
          "_resume",
          "getfixturedefs",
          "__array_ufunc__",
          "setupplan",
          "strip",
          "makepyprojecttoml",
          "start_global_capturing",
          "pytest_leave_pdb",
          "getparent",
          "_idval",
          "unraisableexception",
          "E",
          "catch_threading_exception",
          "_with_exception",
          "Traceback",
          "issequence",
          "LFPluginCollSkipfiles",
          "safe_isclass",
          "deselect_by_mark",
          "get_scope_package",
          "TerminalWriter",
          "MarkGenerator",
          "pastebinfile_key",
          "ApproxSequencelike",
          "ExceptionChainRepr",
          "exconly",
          "_assert_expr_to_lineno",
          "pytest_terminal_summary",
          "wrap_session",
          "LOGLEVEL_COLOROPTS",
          "delenv",
          "finish",
          "_in_venv",
          "collect_by_name",
          "repr_traceback",
          "getplugin",
          "showfixtures",
          "config",
          "WARNS_NONE_ARG",
          "getinicfg",
          "_get_pdb_wrapper_class",
          "add_cleanup",
          "assert_outcomes",
          "_colorama_workaround",
          "cached_result",
          "builtin",
          "warns",
          "addExpectedFailure",
          "_truncate_by_char_count",
          "record_property",
          "TimeoutExpired",
          "__enter__",
          "_idvalset",
          "_format_repr_exception",
          "newinstance",
          "firstlineno",
          "getparam",
          "__init_subclass__",
          "_PluggyPlugin",
          "PrintHelp",
          "_makepath",
          "rewrite_asserts",
          "fillfixtures",
          "commonpath",
          "INTERRUPTED",
          "visit_Call",
          "DoctestTextfile",
          "no_re_match_line",
          "showfspath",
          "WARNING_CAPTURED_HOOK",
          "recursionindex",
          "_get_number_flag",
          "_CACHE_PREFIX_DIRS",
          "consider_pluginarg",
          "inifile",
          "_parse_parametrize_parameters",
          "make_numbered_dir",
          "deprecated_call",
          "filter",
          "_saferepr",
          "_inject_setup_teardown_fixtures",
          "version_tuple",
          "toterminal",
          "global_and_fixture_disabled",
          "repr_excinfo",
          "execute",
          "DEPRECATED_EXTERNAL_PLUGINS",
          "create_module",
          "evaluate_condition",
          "getfirstlinesource",
          "DEFAULT_MAX_CHARS",
          "inline_genitems",
          "_WithException",
          "DontReadFromInput",
          "write",
          "BINOP_MAP",
          "FixtureFunction",
          "_fill_fixtures_impl",
          "addsection",
          "pytest_unconfigure",
          "_Key",
          "FixtureRequest",
          "findsource",
          "runpytest_subprocess",
          "delitem",
          "param",
          "_getlines",
          "getnode",
          "_PY_DIR",
          "with_args",
          "truncate_locals",
          "make_hook_recorder",
          "_factorytraceback",
          "Store",
          "PdbInvoke",
          "_rget_with_confmod",
          "ensure_extended_length_path",
          "RunResult",
          "FixtureLookupError",
          "DOCTEST_REPORT_CHOICE_CDIFF",
          "TeeCaptureIO",
          "visit_Attribute",
          "_bytes_to_ascii",
          "markname",
          "_getautousenames",
          "InvocationParams",
          "skipping",
          "is_globally_capturing",
          "_non_printable_ascii_translate_table",
          "get_timeout_config_value",
          "PYTEST_TAG",
          "isnosetest",
          "no_summary",
          "find_suffixes",
          "copy",
          "pytest_addhooks",
          "_FileHandler",
          "make_properties_node",
          "CaptureFixture",
          "_py36_windowsconsoleio_workaround",
          "ARGUMENT_TYPE_STR_CHOICE",
          "_code",
          "visit",
          "create_cleanup_lock",
          "no_fnmatch_line",
          "write_line",
          "pytest_cmdline_main",
          "LOCK_TIMEOUT",
          "get_extended_length_path_str",
          "DEFAULT_REPR_MAX_SIZE",
          "_result",
          "_compare_eq_cls",
          "KNOWN_TYPES",
          "sections",
          "assertstate_key",
          "repr_traceback_entry",
          "_write_report_lines_from_hooks",
          "isinitpath",
          "_addfinalizer",
          "fspath",
          "_fail",
          "set_level",
          "PytestArg",
          "collect",
          "_validate_if_using_arg_names",
          "is_generator",
          "_names",
          "PyCollector",
          "ReprFileLocation",
          "filename_arg",
          "evaluate",
          "SEP",
          "getfuncargnames",
          "call_optional",
          "get_statement_startend2",
          "COLLECT_FAKEMODULE_ATTRIBUTES",
          "get_option_ini",
          "is_capturing",
          "PRIVATE",
          "from_item_and_call",
          "_striptext",
          "addUnexpectedSuccess",
          "resolve",
          "pytest_report_to_serializable",
          "and_expr",
          "wrap_pytest_function_for_tracing",
          "f_globals",
          "pytest_runtest_setup",
          "isset",
          "_fillfuncargs",
          "rex_session_duration",
          "getrepr",
          "PytestUnhandledCoroutineWarning",
          "version",
          "reprfuncargs",
          "_getobj",
          "FixtureManager",
          "on_rm_rf_error",
          "PytestCacheWarning",
          "_compare_eq_sequence",
          "_NodeReporter",
          "finalize",
          "runpython",
          "setuponly",
          "deprecated",
          "_init_checker_class",
          "resume_global_capture",
          "REGEX_TYPE",
          "ensure_newline",
          "__take_sys_modules_snapshot",
          "import_plugin",
          "getcalls",
          "makeini",
          "setup",
          "load_config_dict_from_file",
          "strict",
          "code",
          "getfixturemarker",
          "_argcomplete",
          "pytest_runtest_logfinish",
          "tmp_path",
          "addinivalue_line",
          "parse_summary_nouns",
          "addfinalizer",
          "_find_parametrized_scope",
          "_ellipsize",
          "MARK_GEN",
          "_disable_output_capturing_for_darwin",
          "encoding",
          "setmulti2",
          "__hash__",
          "StoreKey",
          "create_terminal_writer",
          "_iter_all_modules",
          "getvalueorskip",
          "Token",
          "TYPE_CHECKING",
          "filter_traceback",
          "getfailedcollections",
          "Frame",
          "get_cache_dir",
          "for_config",
          "__attrs_post_init__",
          "resume_fixture",
          "_wrapped_pdb_cls",
          "DEFAULT_LOG_FORMAT",
          "parametrize",
          "ReprFuncArgs",
          "_compare_eq_dict",
          "teardown",
          "__ne__",
          "listoutcomes",
          "_get_report_choice",
          "ParsedCall",
          "_locationline",
          "passed",
          "MonkeyPatch",
          "register",
          "_EmptyClass",
          "exitstatus",
          "FuncFixtureInfo",
          "AssertionRewriter",
          "tbfilter",
          "install_importhook",
          "parseconfigure",
          "_addexcinfo",
          "__array_priority__",
          "saferepr",
          "getmodpath",
          "_args_converter",
          "stopTest",
          "read",
          "__next__",
          "showhelp",
          "caplog_handler_key",
          "runitem",
          "pytest_ignore_collect",
          "gethookrecorder",
          "verbosity",
          "wcswidth",
          "longreprtext",
          "mode",
          "append_pass",
          "perform_collect",
          "FixtureFunctionMarker",
          "_notin_text",
          "_split_lines",
          "__contains__",
          "_prepareconfig",
          "try_cleanup",
          "mangle_test_address",
          "_as_numpy_array",
          "_format_boolop",
          "PytestDeprecationWarning",
          "not_expr",
          "pytest_cmdline_parse",
          "_assert_start_repr",
          "monkeypatch",
          "_format_action_invocation",
          "set_trace",
          "argnames",
          "_log_text",
          "CollectReport",
          "_params_converter",
          "WarningReport",
          "getimfunc",
          "add_stats",
          "FunctionDefinition",
          "_warn_about_missing_assertion",
          "genitems",
          "getworkerinfoline",
          "stop_capturing",
          "pytest_markeval_namespace",
          "reorder_items_atscope",
          "section",
          "autouse",
          "_excinfo",
          "_param_ids_from",
          "from_exc_info",
          "parseoutcomes",
          "_get_first_non_fixture_func",
          "relline",
          "pytest_sessionfinish",
          "Mark",
          "import_path",
          "_diff_text",
          "parse_num",
          "_should_truncate_item",
          "_eval_scope_callable",
          "main",
          "setdefault",
          "fail_marker",
          "read_global_capture",
          "FDCaptureBinary",
          "get_terminal_writer",
          "check_ispytest",
          "startdir",
          "handleError",
          "get_sections",
          "hasplugin",
          "_processopt",
          "names_closure",
          "_parse_parametrize_args",
          "ApproxNumpy",
          "truncate_if_required",
          "expression",
          "_recursive_list_map",
          "style",
          "_get_pos",
          "_build_normal_summary_stats_line",
          "MatcherAdapter",
          "_hook",
          "write_sep",
          "repr_locals",
          "Markable",
          "notify_exception",
          "DOCTEST_REPORT_CHOICE_NONE",
          "recwarn",
          "path",
          "_get_continue_on_failure",
          "__eq__",
          "catch_unraisable_exception",
          "scopemismatch",
          "_W",
          "resume_capturing",
          "stardir",
          "parse_warning_filter",
          "nextitem",
          "_traceback",
          "parts",
          "insert_missing_modules",
          "INTERNAL_ERROR",
          "add_report_section",
          "getcall",
          "set_log_path",
          "README_CONTENT",
          "OR",
          "fail_fixturefunc",
          "pop",
          "_validate_ids",
          "lineno",
          "item_capture",
          "_teardown_yield_fixture",
          "isiterable",
          "Testdir",
          "names",
          "_get_active_fixturedef",
          "__len__",
          "call_fixture_func",
          "_get_multicapture",
          "addini",
          "pytest_report_header",
          "_get_maxsize_for_saferepr",
          "DoctestModule",
          "name",
          "getgroup",
          "_get_allow_bytes_flag",
          "_compare_approx",
          "make_numbered_dir_with_cleanup",
          "mark",
          "startTest",
          "CaptureManager",
          "suspend_fixture",
          "_HandlerType",
          "RPAREN",
          "suspend_capturing",
          "get_data",
          "add_marker",
          "_format_lines",
          "rootdir",
          "_addoption",
          "popcall",
          "_warn_incompatibility_with_xunit2",
          "PYC_EXT",
          "tmpdir",
          "nose",
          "tb",
          "FixtureDef",
          "write_docstring",
          "istestclass",
          "session",
          "PdbTrace",
          "_check_non_top_pytest_plugins",
          "_makefile",
          "_io",
          "_compute_fixture_value",
          "_pluginmanager",
          "_build_collect_only_summary_stats_line",
          "_importconftest",
          "debugging",
          "PYTEST_COLLECT_MODULE",
          "DropShorterLongHelpFormatter",
          "append_failure",
          "importorskip",
          "testdir",
          "_create",
          "module_name_from_path",
          "deindent",
          "pytest_collection_finish",
          "funcnamefilter",
          "markup",
          "pytest_load_initial_conftests",
          "undo",
          "SafeRepr",
          "pytest_cmdline_preparse",
          "_get_progress_information_message",
          "getstatementrange",
          "request",
          "repr_failure",
          "default_plugins",
          "_remove_ansi_escape_sequences",
          "from_config",
          "_schedule_finalizers",
          "setattr",
          "hasinit",
          "_getconftest_pathlist",
          "hookimpl",
          "nodes",
          "_initrequest",
          "assert_never",
          "FixtureLookupErrorRepr",
          "legacy_path",
          "parse_hookimpl_opts",
          "_S",
          "flow_marker",
          "tracebackcutdir",
          "OptionGroup",
          "reset",
          "_testcase",
          "FSCollector",
          "LFPlugin",
          "pytest_assertion_pass",
          "MarkMatcher",
          "_repr_failure_py",
          "hasnew",
          "format_explanation",
          "_determine_main_color",
          "visit_BoolOp",
          "getitem",
          "cache_dir_from_config",
          "capstderr",
          "get_dirs_from_args",
          "_set_initial_conftests",
          "_split_explanation",
          "_suspend",
          "iter_markers",
          "LEGACY_PATH",
          "pos",
          "cache_key",
          "_LiveLoggingNullHandler",
          "_get_fixturestack",
          "disabled",
          "chain",
          "_write_source",
          "capsysbinary",
          "syspath_prepend",
          "getfailures",
          "inline_run",
          "f_locals",
          "_report_keyboardinterrupt",
          "_compare_eq_any",
          "TerminalRepr",
          "path_matches_patterns",
          "PytestUnhandledThreadExceptionWarning",
          "snap",
          "_mk_tmp",
          "FormattedExcinfo",
          "record_testreport",
          "PseudoFixtureDef",
          "warnings",
          "matching_platform",
          "pytest_itemcollected",
          "stepwise",
          "record_testsuite_property",
          "TerminalReporter",
          "DOCTEST_REPORT_CHOICE_UDIFF",
          "ArgumentError",
          "call_and_report",
          "_addoption_instance",
          "_REPORTCHARS_DEFAULT",
          "freeze_support",
          "_config",
          "_get_line_with_reprcrash_message",
          "_inject_setup_module_fixture",
          "fnmatch_ex",
          "D",
          "variable",
          "get_real_method",
          "CollectError",
          "Code",
          "_to_json",
          "LoggingPlugin",
          "ColoredLevelFormatter",
          "lex",
          "Collector",
          "merge_family",
          "get_terminal_width",
          "scopenum_function",
          "suspend",
          "PercentStyleMultiline",
          "restore",
          "_opentestcase",
          "KeywordMatcher",
          "doctest_namespace",
          "sep",
          "append_skipped",
          "Scanner",
          "WarningsChecker",
          "StepwisePlugin",
          "_setup_fixtures",
          "pytest_runtest_makereport",
          "get_public_names",
          "get_config",
          "LineMatcher_fixture",
          "ParameterSet",
          "_warn_about_skipped_plugins",
          "IDENT_PREFIX",
          "DoctestItem",
          "locals",
          "caplog_records_key",
          "instance",
          "_log_cli_enabled",
          "catching_logs",
          "CACHEDIR_TAG_CONTENT",
          "fullwidth",
          "_get_flag_lookup",
          "ARGUMENT_TYPE_STR",
          "resume",
          "tolerance",
          "DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE",
          "__iter__",
          "__getattr__",
          "_enter_pdb",
          "__exit__",
          "Metafunc",
          "PytestUnknownMarkWarning",
          "set",
          "entrysep",
          "errisinstance",
          "_patch_unwrap_mock_aware",
          "accept",
          "_getreprcrash",
          "result",
          "pytest_runtest_logreport",
          "Parser",
          "istestfunc",
          "K",
          "outcomes",
          "iscoroutinefunction",
          "combined_with",
          "fixtures",
          "fullsource",
          "_from_json",
          "LineComp",
          "at_level",
          "__slots__",
          "assertrepr_compare",
          "pytest_plugins",
          "show_fixtures_per_test",
          "failed",
          "EMPTY_PARAMETERSET_OPTION",
          "_call_reprcompare",
          "absolutepath",
          "MyOptionParser",
          "TempPathFactory",
          "getbasetemp",
          "clear",
          "__module__",
          "should_do_markup",
          "_add_stats",
          "value",
          "error",
          "pytest_make_parametrize_id",
          "ApproxDecimal",
          "_ascii_escaped_by_config",
          "_prunetraceback",
          "ExceptionInfo",
          "emit",
          "RUNNER_CLASS",
          "pytest_fixture_setup",
          "_match_lines",
          "count_towards_summary",
          "isattrs",
          "MINUS_K_DASH",
          "Notset",
          "token",
          "maketxtfile",
          "wcwidth",
          "parse_known_args",
          "makeconftest",
          "derive_importpath",
          "str",
          "_PYTEST_DIR",
          "num_mock_patch_args",
          "stop",
          "category",
          "runtestprotocol",
          "pytestconfig",
          "_fillfixtures",
          "fixture",
          "check_testcase_implements_trial_reporter",
          "ascii_escaped",
          "add_attribute",
          "TESTS_FAILED",
          "_T",
          "getvalue",
          "_get_global_properties_node",
          "width_of_current_line",
          "FSHookProxy",
          "_is_setup_py",
          "_matchfactories",
          "freeze_includes",
          "own_mark_names",
          "create_new_paste",
          "text",
          "pytest_runtest_teardown",
          "AND",
          "parse_hookspec_opts",
          "PytestWarning",
          "_trace",
          "__missing__",
          "short_test_summary",
          "_ensure_immutable_ids",
          "ConftestImportFailure",
          "locate_config",
          "get_direct_param_fixture_func",
          "_saved",
          "Argument",
          "capfd",
          "makedir",
          "_param_ids_generated",
          "_non_numeric_type_error",
          "cache",
          "_ensure_removed_sysmodule",
          "_format_trimmed",
          "getfixtureinfo",
          "iterparentnodeids",
          "_IGNORED_ERRORS",
          "_check_record_param_type",
          "_in_suspended",
          "assertion",
          "AlwaysDispatchingPrettyPrinter",
          "attrs",
          "_repr_compare",
          "_is_doctest",
          "_is_started",
          "_compare_eq_iterable",
          "makepyfile"
        ]
      },
      "response": {
        "keywords": [
          "skip",
          "raises",
          "evaluate_skip_marks",
          "evaluate_xfail_marks",
          "XFailed",
          "Skipped",
          "skip_module",
          "evaluate",
          "determine_setup",
          "Module"
        ]
      }
    },
    {
      "name": "FilesKeywordSearch",
      "metadata": {
        "start": "2024-08-19T13:35:17.414957",
        "usage": "226.82809829711914 ms"
      },
      "request": {
        "tokens": [
          "skip",
          "raises",
          "evaluate_skip_marks",
          "evaluate_xfail_marks",
          "XFailed",
          "Skipped",
          "skip_module",
          "evaluate",
          "determine_setup",
          "Module"
        ]
      },
      "response": {
        "results": {
          "Skipped": [
            "src/_pytest/junitxml.py",
            "src/_pytest/terminal.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/runner.py",
            "src/_pytest/outcomes.py"
          ],
          "skip": [
            "src/_pytest/mark/structures.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/python.py",
            "src/_pytest/terminal.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/pytester.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/unittest.py",
            "src/_pytest/reports.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/doctest.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/deprecated.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/skipping.py",
            "src/pytest/__init__.py",
            "src/_pytest/stepwise.py"
          ],
          "evaluate": [
            "src/_pytest/mark/expression.py",
            "src/_pytest/skipping.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/mark/__init__.py"
          ],
          "raises": [
            "src/_pytest/mark/structures.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/recwarn.py",
            "src/_pytest/python_api.py",
            "src/_pytest/debugging.py",
            "src/_pytest/nodes.py",
            "src/_pytest/python.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/threadexception.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/_code/code.py",
            "src/pytest/__init__.py"
          ],
          "evaluate_skip_marks": [
            "src/_pytest/skipping.py"
          ],
          "evaluate_xfail_marks": [
            "src/_pytest/skipping.py"
          ],
          "Module": [
            "src/_pytest/main.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/python.py",
            "src/_pytest/runner.py",
            "src/_pytest/store.py",
            "src/pytest/collect.py",
            "src/_pytest/cacheprovider.py",
            "src/pytest/__init__.py",
            "src/_pytest/doctest.py"
          ],
          "XFailed": [
            "src/_pytest/outcomes.py"
          ],
          "determine_setup": [
            "src/_pytest/config/findpaths.py",
            "src/_pytest/config/__init__.py"
          ]
        }
      }
    },
    {
      "name": "FilesFilter",
      "metadata": {
        "start": "2024-08-19T13:35:17.641802",
        "usage": "7167.216777801514 ms"
      },
      "request": {
        "keyword_search_results": {
          "Skipped": [
            "src/_pytest/junitxml.py",
            "src/_pytest/terminal.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/runner.py",
            "src/_pytest/outcomes.py"
          ],
          "skip": [
            "src/_pytest/mark/structures.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/junitxml.py",
            "src/_pytest/python.py",
            "src/_pytest/terminal.py",
            "src/_pytest/helpconfig.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/pytester.py",
            "src/_pytest/assertion/util.py",
            "src/_pytest/unittest.py",
            "src/_pytest/reports.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/doctest.py",
            "src/_pytest/config/exceptions.py",
            "src/_pytest/deprecated.py",
            "src/_pytest/config/__init__.py",
            "src/_pytest/skipping.py",
            "src/pytest/__init__.py",
            "src/_pytest/stepwise.py"
          ],
          "evaluate": [
            "src/_pytest/mark/expression.py",
            "src/_pytest/skipping.py",
            "src/_pytest/cacheprovider.py",
            "src/_pytest/mark/__init__.py"
          ],
          "raises": [
            "src/_pytest/mark/structures.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/recwarn.py",
            "src/_pytest/python_api.py",
            "src/_pytest/debugging.py",
            "src/_pytest/nodes.py",
            "src/_pytest/python.py",
            "src/_pytest/pathlib.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/threadexception.py",
            "src/_pytest/skipping.py",
            "src/_pytest/outcomes.py",
            "src/_pytest/_code/code.py",
            "src/pytest/__init__.py"
          ],
          "evaluate_skip_marks": [
            "src/_pytest/skipping.py"
          ],
          "evaluate_xfail_marks": [
            "src/_pytest/skipping.py"
          ],
          "Module": [
            "src/_pytest/main.py",
            "src/_pytest/assertion/rewrite.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/python.py",
            "src/_pytest/runner.py",
            "src/_pytest/store.py",
            "src/pytest/collect.py",
            "src/_pytest/cacheprovider.py",
            "src/pytest/__init__.py",
            "src/_pytest/doctest.py"
          ],
          "XFailed": [
            "src/_pytest/outcomes.py"
          ],
          "determine_setup": [
            "src/_pytest/config/findpaths.py",
            "src/_pytest/config/__init__.py"
          ]
        }
      },
      "response": {
        "scratch_pad": "The issue at hand is about improving handling of test skips in pytest for modules, particularly when Python version compatibility is a concern. The problem is that module-level skipping is generating misleading error messages, and there's a proposal to improve this by either making error messages clearer or implementing a new API call.\n\nLet's examine the keyword search results:\n\n1. **skip**: This is the most relevant keyword related to the issue. Files associated with this keyword that could be relevant include:\n    - `src/_pytest/mark/structures.py`: This file might contain structures or decorators related to skipping tests.\n    - `src/_pytest/skipping.py`: This file is particularly relevant as it likely handles skipping logic within pytest. It's likely to address functionality around skip and possibly module-level skips.\n    - `src/_pytest/outcomes.py`: Since this deals with different outcomes in pytest, it might relate to the handling of skip outcomes specifically.\n\n2. **Skipped**: This indicates features related to items that have been skipped. Not as directly relevant as \"skip\", but can be useful. However, files here seem less directly involved compared to those directly under \"skipping.py\".\n\n3. **evaluate_skip_marks**: Also points to `src/_pytest/skipping.py`. Given its context, it reinforces that `skipping.py` handles the logic related to evaluating skip marks which could include module-level skips.\n\n4. **evaluate**: This keyword also points to `src/_pytest/skipping.py` among others. It indicates that there might be some evaluation logic in `skipping.py` relevant to skips.\n\nBased on this analysis, the most relevant files should relate to the actual skipping implementation and outcomes in pytest, focused on `src/_pytest/skipping.py` and potentially related handler files like `src/_pytest/outcomes.py` and structures like `src/_pytest/mark/structures.py`. These files are likely to contain the logic where improvements could be made or where a new API could be integrated.",
        "file_paths": [
          "src/_pytest/skipping.py",
          "src/_pytest/mark/structures.py",
          "src/_pytest/outcomes.py"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:35:24.809048",
        "usage": "7157.922744750977 ms"
      },
      "request": {
        "file_path": "src/_pytest/skipping.py"
      },
      "response": {
        "scratch_pad": "1. Analyzing the User Issue:\n\n1.1 The issue described by the user is about handling the skip function for module level tests under pytest, primarily when using syntax that is only supported in newer versions of Python (such as Python 3.8) and needing to skip tests in older Python versions (3.6, 3.7). \n\n1.2 The user provides a scenario where such syntax causes errors when running the test with older Python versions, and suggests that the current way to skip tests at the module level is either not clear enough or not functioning as expected. Particularly, the problem arises with `pytest.mark.skipif` not preventing the module from being processed, which leads to syntax errors.\n\n1.3 The issue is caused by the fact that skipping mechanisms suggested by pytest (using `pytestmark`) are not effective at a module level before the module is loaded and syntax is parsed. Hence, this causes errors if the non-compatible syntax is encountered.\n\n1.4 The expectation is to bypass the test file completely if a certain condition isn't met, such as running on an unsupported Python version, without running into syntax errors first. The current documentation and error messages do not clearly guide the users on this process.\n\n1.6 Possible solutions include introducing an alternative API, like `skip_module()`, to improve the usability and effectiveness of module-level skipping.\n\n2. File Evaluation:\n\n2.1 The file, `src/_pytest/skipping.py`, contains definitions for handling skipping and xfail operations within pytest.\n\n2.2 It's highly relevant to the issue since it implements the mechanisms for skipping tests and could be a point of implementation for any changes like introducing `skip_module()` functionality.\n\n2.3 The `evaluate_skip_marks`, `pytest_runtest_setup`, and `pytest_runtest_call` functions evaluate skip marks and could be areas where conditions for module-level skip are decided or applied.\n\n2.5 There are no specific mentions of `allow_module_level` in the document, highlighting that the handling for module-level skips may not be adequately implemented.\n\n2.6 The file seems related to the root cause of the issues as it defines how skipping is processed and could be extended to resolve the issue raised by the user by supporting module-level skips better.",
        "relevant": true,
        "root_cause": true,
        "relevant_files": [],
        "relevant_symbols": [
          "evaluate_skip_marks",
          "pytest_runtest_setup",
          "pytest_runtest_call"
        ],
        "root_cause_symbols": [
          "evaluate_skip_marks",
          "pytest_runtest_setup",
          "pytest_runtest_call"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:35:31.967004",
        "usage": "9079.70118522644 ms"
      },
      "request": {
        "file_path": "src/_pytest/mark/structures.py"
      },
      "response": {
        "scratch_pad": "1. **Analyzing the issue:**\n   The user is describing a problem with skipping tests at the module level when using Python features that are not backwards compatible (like Python 3.8 syntax in older versions). The user points out that while `pytest.mark.skipif` allows skipping a module, it doesn\u2019t prevent Python from parsing the module, leading to syntax failures.\n\n2. **Reproducing steps:**\n   The issue arises when a module requires a certain Python version to work, and the current method `pytestmark = pytest.mark.skipif` doesn\u2019t prevent execution of code with incompatible syntax. \n\n3. **Cause of the issue:**\n   The suggested approach by the error \"Using pytest.skip...\" is misunderstood. The function `skip` can be used with `allow_module_level=True`, but this goes against the spirit of error messages generated by pytest.\n\n4. **User expectation:**\n   The user expects to skip modules with incompatible syntax early in the test collection phase, avoiding syntax errors.\n\n5. **Possible solutions:**\n   a. Update error messages to guide the use of `skip` with `allow_module_level`.\n   b. Introduce a new API like `skip_module()` for this behavior, which would internally handle it correctly.\n\n\n**Evaluation of the file:**\n1. **Relevance Assessment:**\n   - The file deals with pytest's marking/decorator mechanisms, including parameterization, skipping, and others.\n   - This is directly relevant because the issue involves pytest's test skipping mechanics.\n2. **Search for Symbols Related to Issue:**\n   - There is a `MarkDecorator` class, which represents decorators like skip, skipif.\n   - `MarkGenerator` is the factory to generate these decorators.\n   - `skip` and `skipif` are handled as components of `MarkGenerator` designed to return `MarkDecorator` instances.\n   \n3. **Determining Root Cause or Relevance:**\n   - Since we're discussing extending or changing behavior in decorators, this file likely contributes part of the root cause but may not encapsulate it entirely. It's mostly responsible for marking/decoration.\n\n**Conclusion:** The file is relevant, potentially contributing to the given issue, and worth evaluating further with symbols/ideas to address the expansion or modification of error handling/behavior.\n",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/outcomes.py",
          "src/_pytest/mark/utils.py",
          "src/_pytest/config/exception_handling.py"
        ],
        "relevant_symbols": [
          "MarkGenerator.__getattr__",
          "MarkDecorator",
          "ParameterSet.param"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-19T13:35:41.046733",
        "usage": "9543.969869613647 ms"
      },
      "request": {
        "file_path": "src/_pytest/outcomes.py"
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 What issue is the user describing?\n   - The user describes an issue with using pytest to skip certain tests when using syntax that isn't compatible with older Python versions (e.g., positional only arguments).\n\n1.2 User give a reproduce steps?, if yes, think why this steps cause the issue.\n   - Yes, the user provides a reproduction scenario involving a Python script using syntax specific to Python 3.8, which causes errors when the script is run in older Python versions. The attempted steps to skip tests using such syntax led to an error due to how pytest handles skips.\n\n1.3 How was this issue caused?\n   - The issue arises because pytest's recommended ways of skipping tests either don't support skipping entire modules at a module level or are not compatible when the module contains syntax not supported in older Python versions.\n\n1.4 Summary the user expectation and why cause the issue.\n   - The user expects a convenient and error-free way to skip tests or modules based on the Python version. They find the current state \"unpolished\" because the error message discourages the use of a viable solution (`allow_module_level=True`) that they desire.\n\n1.6 Some possible solutions to the issue.\n   - Update error messages to better guide users on using `allow_module_level=True` for module-wide skips.\n   - Introduce a new API method such as `skip_module(\"reason\")` to simplify skipping modules, internally setting `allow_module_level=True`.\n\n2. Make file evaluation:\n2.1. Thoroughly analyze each lines in the <file_content> tag.\n   - The file contains functionality relating to different test outcomes in pytest, including skipping tests (_handled by `skip()` function_).\n\n2.2. Match the symbol with the key elements, functions, variables, or patterns identified in the reported issue.\n   - The report discusses `pytest.skip` and handling of `allow_module_level`. These are addressed in the `skip()` function, which raises a `Skipped` exception when called with `allow_module_level=True`.\n\n2.3. Evaluate the relevance of each symbol based on how well it aligns with the reported issue and current file context.\n   - The `skip()` function and `Skipped` class are directly related due to their role in determining behavior for skipping tests with options for module level control.\n\n2.4. Review entire sections of code:\n   - _Skipped Class_: Contains the constructor that accepts `allow_module_level`. This is integral for understanding how skipping at module level operates.\n   Additionally, the `importorskip` method also seems related, since it raises a `Skipped` exception, potentially with module-level control.\n\n2.5. Verify if there are references to other parts of the codebase that might be relevant:\n   - No direct references to other code parts that alter the `skip` logic fundamentally, based on this snippet.\n\n2.6. Determine if the file is the root cause of the issue or if it's related to the root cause.\n   - While the file is integral in skip operations, the issue could also pertain to error messaging and user guidance, which might be distributed across other parts of pytest's codebase.\n\nDeciding to list `skip` as relevant symbols and conclude that the file is relevant as it houses the `skip` methods and associated exception classes.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [
          "pytest/main.py",
          "pytest/mark.py"
        ],
        "relevant_symbols": [
          "Skipped",
          "skip"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileSymbolSearch",
      "metadata": {
        "start": "2024-08-19T13:35:50.590722",
        "usage": "144.0408229827881 ms"
      },
      "request": {
        "files": [
          {
            "file_path": "src/_pytest/skipping.py",
            "relevant_symbol_keys": [
              "evaluate_skip_marks",
              "pytest_runtest_setup",
              "pytest_runtest_call"
            ],
            "root_cause_symbol_keys": [
              "evaluate_skip_marks",
              "pytest_runtest_setup",
              "pytest_runtest_call"
            ]
          },
          {
            "file_path": "src/_pytest/mark/structures.py",
            "relevant_symbol_keys": [
              "MarkGenerator.__getattr__",
              "MarkDecorator",
              "ParameterSet.param"
            ],
            "root_cause_symbol_keys": []
          },
          {
            "file_path": "src/_pytest/outcomes.py",
            "relevant_symbol_keys": [
              "Skipped",
              "skip"
            ],
            "root_cause_symbol_keys": []
          }
        ],
        "with_content": true
      },
      "response": {
        "related_symbols": [
          {
            "name": "_pytest.mark.ParameterSet.param",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@classmethod\n    def param(\n        cls,\n        *values: object,\n        marks: Union[\"MarkDecorator\", Collection[Union[\"MarkDecorator\", \"Mark\"]]] = (),\n        id: Optional[str] = None,\n    ) -> \"ParameterSet\":\n        if isinstance(marks, MarkDecorator):\n            marks = (marks,)\n        else:\n            assert isinstance(marks, collections.abc.Collection)\n\n        if id is not None:\n            if not isinstance(id, str):\n                raise TypeError(f\"Expected id to be a string, got {type(id)}: {id!r}\")\n            id = ascii_escaped(id)\n        return cls(values, marks, id)",
            "start_line": 87,
            "start_column": 2272,
            "end_line": 4,
            "end_column": 2865,
            "start_byte": 2272,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@attr.s(init=False, auto_attribs=True)\nclass MarkDecorator:\n    \"\"\"A decorator for applying a mark on test functions and classes.\n\n    ``MarkDecorators`` are created with ``pytest.mark``::\n\n        mark1 = pytest.mark.NAME              # Simple MarkDecorator\n        mark2 = pytest.mark.NAME(name1=value) # Parametrized MarkDecorator\n\n    and can then be applied as decorators to test functions::\n\n        @mark2\n        def test_function():\n            pass\n\n    When a ``MarkDecorator`` is called, it does the following:\n\n    1. If called with a single class as its only positional argument and no\n       additional keyword arguments, it attaches the mark to the class so it\n       gets applied automatically to all test cases found in that class.\n\n    2. If called with a single function as its only positional argument and\n       no additional keyword arguments, it attaches the mark to the function,\n       containing all the arguments already stored internally in the\n       ``MarkDecorator``.\n\n    3. When called in any other case, it returns a new ``MarkDecorator``\n       instance with the original ``MarkDecorator``'s content updated with\n       the arguments passed to this call.\n\n    Note: The rules above prevent a ``MarkDecorator`` from storing only a\n    single function or class reference as its positional argument with no\n    additional keyword or positional arguments. You can work around this by\n    using `with_args()`.\n    \"\"\"\n\n    mark: Mark\n\n    def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark\n\n    @property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name\n\n    @property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args\n\n    @property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs\n\n    @property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)\n\n    def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"\n\n    def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)\n\n    # Type ignored because the overloads overlap with an incompatible\n    # return type. Not much we can do about that. Thankfully mypy picks\n    # the first match so it works out even if we break the rules.\n    @overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass\n\n    @overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass\n\n    def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
            "start_line": 271,
            "start_column": 8979,
            "end_line": 0,
            "end_column": 12391,
            "start_byte": 8979,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.MarkDecorator.mark",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "mark: Mark",
            "start_line": 307,
            "start_column": 10433,
            "end_line": 4,
            "end_column": 10443,
            "start_byte": 10433,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__init__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark",
            "start_line": 309,
            "start_column": 10449,
            "end_line": 4,
            "end_column": 10604,
            "start_byte": 10449,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.name",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name",
            "start_line": 314,
            "start_column": 10610,
            "end_line": 4,
            "end_column": 10711,
            "start_byte": 10610,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.args",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args",
            "start_line": 319,
            "start_column": 10717,
            "end_line": 4,
            "end_column": 10830,
            "start_byte": 10717,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.kwargs",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs",
            "start_line": 324,
            "start_column": 10836,
            "end_line": 4,
            "end_column": 10957,
            "start_byte": 10836,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.markname",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)",
            "start_line": 329,
            "start_column": 10963,
            "end_line": 4,
            "end_column": 11102,
            "start_byte": 10963,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__repr__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"",
            "start_line": 334,
            "start_column": 11108,
            "end_line": 4,
            "end_column": 11182,
            "start_byte": 11108,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.with_args",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)",
            "start_line": 337,
            "start_column": 11188,
            "end_line": 4,
            "end_column": 11593,
            "start_byte": 11188,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass",
            "start_line": 349,
            "start_column": 11807,
            "end_line": 4,
            "end_column": 11902,
            "start_byte": 11807,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass",
            "start_line": 353,
            "start_column": 11908,
            "end_line": 4,
            "end_column": 12006,
            "start_byte": 11908,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
            "start_line": 357,
            "start_column": 12012,
            "end_line": 4,
            "end_column": 12391,
            "start_byte": 12012,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkGenerator.__getattr__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __getattr__(self, name: str) -> MarkDecorator:\n        \"\"\"Generate a new :class:`MarkDecorator` with the given name.\"\"\"\n        if name[0] == \"_\":\n            raise AttributeError(\"Marker name must NOT start with underscore\")\n\n        if self._config is not None:\n            # We store a set of markers as a performance optimisation - if a mark\n            # name is in the set we definitely know it, but a mark may be known and\n            # not in the set.  We therefore start by updating the set!\n            if name not in self._markers:\n                for line in self._config.getini(\"markers\"):\n                    # example lines: \"skipif(condition): skip the given test if...\"\n                    # or \"hypothesis: tests which use Hypothesis\", so to get the\n                    # marker name we split on both `:` and `(`.\n                    marker = line.split(\":\")[0].split(\"(\")[0].strip()\n                    self._markers.add(marker)\n\n            # If the name is not in the set of known marks after updating,\n            # then it really is time to issue a warning or an error.\n            if name not in self._markers:\n                if self._config.option.strict_markers or self._config.option.strict:\n                    fail(\n                        f\"{name!r} not found in `markers` configuration option\",\n                        pytrace=False,\n                    )\n\n                # Raise a specific error for common misspellings of \"parametrize\".\n                if name in [\"parameterize\", \"parametrise\", \"parameterise\"]:\n                    __tracebackhide__ = True\n                    fail(f\"Unknown '{name}' mark, did you mean 'parametrize'?\")\n\n                warnings.warn(\n                    \"Unknown pytest.mark.%s - is this a typo?  You can register \"\n                    \"custom marks to avoid this warning - for details, see \"\n                    \"https://docs.pytest.org/en/stable/how-to/mark.html\" % name,\n                    PytestUnknownMarkWarning,\n                    2,\n                )\n\n        return MarkDecorator(Mark(name, (), {}, _ispytest=True), _ispytest=True)",
            "start_line": 498,
            "start_column": 16601,
            "end_line": 4,
            "end_column": 18718,
            "start_byte": 16601,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
          },
          {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.Skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "class Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 50,
            "start_column": 1592,
            "end_line": 0,
            "end_column": 2297,
            "start_byte": 1592,
            "end_byte": 0
          },
          {
            "name": "_pytest.outcomes.Skipped.__module__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "__module__ = \"builtins\"",
            "start_line": 53,
            "start_column": 1750,
            "end_line": 4,
            "end_column": 1773,
            "start_byte": 1750,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
          },
          {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
          },
          {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
          },
          {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 807,
            "start_column": 26220,
            "end_line": 4,
            "end_column": 26389,
            "start_byte": 26220,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 491,
            "start_column": 17030,
            "end_line": 0,
            "end_column": 17204,
            "start_byte": 17030,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 712,
            "start_column": 25956,
            "end_line": 4,
            "end_column": 26165,
            "start_byte": 25956,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n   ",
            "start_line": 158,
            "start_column": 5105,
            "end_line": 0,
            "end_column": 5640,
            "start_byte": 5105,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # The test run may have added an xfail mark dynamically.\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
          },
          {
            "name": "_pytest.threadexception.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/",
            "start_line": 80,
            "start_column": 2599,
            "end_line": 0,
            "end_column": 2753,
            "start_byte": 2599,
            "end_byte": 0
          },
          {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
          },
          {
            "name": "_pytest.unraisableexception.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ect) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    c",
            "start_line": 85,
            "start_column": 2867,
            "end_line": 0,
            "end_column": 3025,
            "start_byte": 2867,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
          }
        ]
      }
    },
    {
      "name": "SymbolsEvaluate",
      "metadata": {
        "start": "2024-08-19T13:35:50.734769",
        "usage": "14078.89199256897 ms"
      },
      "request": {
        "related_symbols": [
          {
            "name": "_pytest.mark.ParameterSet.param",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@classmethod\n    def param(\n        cls,\n        *values: object,\n        marks: Union[\"MarkDecorator\", Collection[Union[\"MarkDecorator\", \"Mark\"]]] = (),\n        id: Optional[str] = None,\n    ) -> \"ParameterSet\":\n        if isinstance(marks, MarkDecorator):\n            marks = (marks,)\n        else:\n            assert isinstance(marks, collections.abc.Collection)\n\n        if id is not None:\n            if not isinstance(id, str):\n                raise TypeError(f\"Expected id to be a string, got {type(id)}: {id!r}\")\n            id = ascii_escaped(id)\n        return cls(values, marks, id)",
            "start_line": 87,
            "start_column": 2272,
            "end_line": 4,
            "end_column": 2865,
            "start_byte": 2272,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@attr.s(init=False, auto_attribs=True)\nclass MarkDecorator:\n    \"\"\"A decorator for applying a mark on test functions and classes.\n\n    ``MarkDecorators`` are created with ``pytest.mark``::\n\n        mark1 = pytest.mark.NAME              # Simple MarkDecorator\n        mark2 = pytest.mark.NAME(name1=value) # Parametrized MarkDecorator\n\n    and can then be applied as decorators to test functions::\n\n        @mark2\n        def test_function():\n            pass\n\n    When a ``MarkDecorator`` is called, it does the following:\n\n    1. If called with a single class as its only positional argument and no\n       additional keyword arguments, it attaches the mark to the class so it\n       gets applied automatically to all test cases found in that class.\n\n    2. If called with a single function as its only positional argument and\n       no additional keyword arguments, it attaches the mark to the function,\n       containing all the arguments already stored internally in the\n       ``MarkDecorator``.\n\n    3. When called in any other case, it returns a new ``MarkDecorator``\n       instance with the original ``MarkDecorator``'s content updated with\n       the arguments passed to this call.\n\n    Note: The rules above prevent a ``MarkDecorator`` from storing only a\n    single function or class reference as its positional argument with no\n    additional keyword or positional arguments. You can work around this by\n    using `with_args()`.\n    \"\"\"\n\n    mark: Mark\n\n    def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark\n\n    @property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name\n\n    @property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args\n\n    @property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs\n\n    @property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)\n\n    def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"\n\n    def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)\n\n    # Type ignored because the overloads overlap with an incompatible\n    # return type. Not much we can do about that. Thankfully mypy picks\n    # the first match so it works out even if we break the rules.\n    @overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass\n\n    @overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass\n\n    def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
            "start_line": 271,
            "start_column": 8979,
            "end_line": 0,
            "end_column": 12391,
            "start_byte": 8979,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.MarkDecorator.mark",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "mark: Mark",
            "start_line": 307,
            "start_column": 10433,
            "end_line": 4,
            "end_column": 10443,
            "start_byte": 10433,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__init__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark",
            "start_line": 309,
            "start_column": 10449,
            "end_line": 4,
            "end_column": 10604,
            "start_byte": 10449,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.name",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name",
            "start_line": 314,
            "start_column": 10610,
            "end_line": 4,
            "end_column": 10711,
            "start_byte": 10610,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.args",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args",
            "start_line": 319,
            "start_column": 10717,
            "end_line": 4,
            "end_column": 10830,
            "start_byte": 10717,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.kwargs",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs",
            "start_line": 324,
            "start_column": 10836,
            "end_line": 4,
            "end_column": 10957,
            "start_byte": 10836,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.markname",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)",
            "start_line": 329,
            "start_column": 10963,
            "end_line": 4,
            "end_column": 11102,
            "start_byte": 10963,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__repr__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"",
            "start_line": 334,
            "start_column": 11108,
            "end_line": 4,
            "end_column": 11182,
            "start_byte": 11108,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.with_args",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)",
            "start_line": 337,
            "start_column": 11188,
            "end_line": 4,
            "end_column": 11593,
            "start_byte": 11188,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass",
            "start_line": 349,
            "start_column": 11807,
            "end_line": 4,
            "end_column": 11902,
            "start_byte": 11807,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass",
            "start_line": 353,
            "start_column": 11908,
            "end_line": 4,
            "end_column": 12006,
            "start_byte": 11908,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
            "start_line": 357,
            "start_column": 12012,
            "end_line": 4,
            "end_column": 12391,
            "start_byte": 12012,
            "end_byte": 4
          },
          {
            "name": "_pytest.mark.MarkGenerator.__getattr__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __getattr__(self, name: str) -> MarkDecorator:\n        \"\"\"Generate a new :class:`MarkDecorator` with the given name.\"\"\"\n        if name[0] == \"_\":\n            raise AttributeError(\"Marker name must NOT start with underscore\")\n\n        if self._config is not None:\n            # We store a set of markers as a performance optimisation - if a mark\n            # name is in the set we definitely know it, but a mark may be known and\n            # not in the set.  We therefore start by updating the set!\n            if name not in self._markers:\n                for line in self._config.getini(\"markers\"):\n                    # example lines: \"skipif(condition): skip the given test if...\"\n                    # or \"hypothesis: tests which use Hypothesis\", so to get the\n                    # marker name we split on both `:` and `(`.\n                    marker = line.split(\":\")[0].split(\"(\")[0].strip()\n                    self._markers.add(marker)\n\n            # If the name is not in the set of known marks after updating,\n            # then it really is time to issue a warning or an error.\n            if name not in self._markers:\n                if self._config.option.strict_markers or self._config.option.strict:\n                    fail(\n                        f\"{name!r} not found in `markers` configuration option\",\n                        pytrace=False,\n                    )\n\n                # Raise a specific error for common misspellings of \"parametrize\".\n                if name in [\"parameterize\", \"parametrise\", \"parameterise\"]:\n                    __tracebackhide__ = True\n                    fail(f\"Unknown '{name}' mark, did you mean 'parametrize'?\")\n\n                warnings.warn(\n                    \"Unknown pytest.mark.%s - is this a typo?  You can register \"\n                    \"custom marks to avoid this warning - for details, see \"\n                    \"https://docs.pytest.org/en/stable/how-to/mark.html\" % name,\n                    PytestUnknownMarkWarning,\n                    2,\n                )\n\n        return MarkDecorator(Mark(name, (), {}, _ispytest=True), _ispytest=True)",
            "start_line": 498,
            "start_column": 16601,
            "end_line": 4,
            "end_column": 18718,
            "start_byte": 16601,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
          },
          {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
          },
          {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.Skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "class Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 50,
            "start_column": 1592,
            "end_line": 0,
            "end_column": 2297,
            "start_byte": 1592,
            "end_byte": 0
          },
          {
            "name": "_pytest.outcomes.Skipped.__module__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "__module__ = \"builtins\"",
            "start_line": 53,
            "start_column": 1750,
            "end_line": 4,
            "end_column": 1773,
            "start_byte": 1750,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
          },
          {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
          },
          {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
          },
          {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
          },
          {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
          },
          {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 807,
            "start_column": 26220,
            "end_line": 4,
            "end_column": 26389,
            "start_byte": 26220,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 491,
            "start_column": 17030,
            "end_line": 0,
            "end_column": 17204,
            "start_byte": 17030,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 712,
            "start_column": 25956,
            "end_line": 4,
            "end_column": 26165,
            "start_byte": 25956,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n   ",
            "start_line": 158,
            "start_column": 5105,
            "end_line": 0,
            "end_column": 5640,
            "start_byte": 5105,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # The test run may have added an xfail mark dynamically.\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
          },
          {
            "name": "_pytest.threadexception.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/",
            "start_line": 80,
            "start_column": 2599,
            "end_line": 0,
            "end_column": 2753,
            "start_byte": 2599,
            "end_byte": 0
          },
          {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
          },
          {
            "name": "_pytest.unraisableexception.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ect) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    c",
            "start_line": 85,
            "start_column": 2867,
            "end_line": 0,
            "end_column": 3025,
            "start_byte": 2867,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
          }
        ]
      },
      "response": {
        "scratch_pad": "1.1 The user is describing an issue with skipping tests in pytest for a module that uses features only available in more recent versions of Python (3.8 and above). When trying to skip these tests on older versions, they are running into issues with how pytest handles the `skip` functionality at the module level.\n\n1.2 The issue arises because the `skip` function from pytest is not supposed to be used at the module level without the `allow_module_level=True` parameter. However, the error message does not make this clear and suggests using `pytest.mark.skip` or `pytest.mark.skipif`, which leads to the tests attempting to run anyway and failing on syntax errors.\n\n1.3 The user proposes that either the documentation or the error messages be improved to clearly indicate how to handle skips at the module level, or that a new API (`skip_module`) be introduced to simplify this process.\n\n1.4 The expectation should be to skip tests entirely at the module level in a seamless way that is clearly documented or easy to use.\n\n1.5 The issue could be resolved by updating the error messages in pytest to include information about the `allow_module_level` parameter when the `skip` function is misused, and/or by introducing a new API that encapsulates this behavior in a user-friendly manner.\n\n2. Importantly though, the solution needs to be accurately reflected in the parts of the codebase where skipping and handling of skips is defined or documented.\n\n\n2.1 Reviewing the symbols, most are hook implementations or involve exception handling/logging.\n2.2 as this only affects on module level skipping and errors associated with it not being clear enough.\n2.3 the `evaluate_skip_marks` function is highly relevant since it evaluates whether a skip or skipif marker should cause the given test/item to be skipped at runtime.\n2.4 the `skip` function itself should be reviewed to ensure adequate handling of the `allow_module_level` flag and to clarify its documentation.\n\nIdentified Place to make updates: \n1. \"_pytest.skipping.evaluate_skip_marks\" function should support the proposed solution or needs enhancement on explaining the module level skipping.\n2. \"_pytest.outcomes.skip\" function's handling of `allow_module_level` flag seems central to the issue.",
        "symbols": [
          {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "reason": "This function evaluates skip marks and could potentially be updated to handle or message about module-level skips more effectively or provide the necessary hooks for a new API."
          },
          {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "reason": "The skip function defines how skips are triggered, including the `allow_module_level` parameter that needs clearer documentation or alteration as per the issue described."
          }
        ],
        "more_info": [],
        "is_done": false,
        "symbols_scratch_pad": [
          "_pytest.capture.CaptureManager.pytest_runtest_setup in src/_pytest/skipping.py: No content provided; seems like a hook that deals with setup, but not directly related to skipping or error messaging. Ignored for relevance to the issue.",
          "_pytest.capture.CaptureManager.pytest_runtest_call in src/_pytest/skipping.py: Similar to above; No content provided. Hook related to the test call stage. Not directly relevant to skipping functionality.",
          "_pytest.hookspec.pytest_runtest_setup in src/_pytest/skipping.py: No content. Likely a hookspec for setup. Not directly related to skipping or error messaging.",
          "_pytest.hookspec.pytest_runtest_call in src/_pytest/skipping.py: No content. Likely a hookspec for call. Not directly related to skipping or error messaging.",
          "_pytest.logging.LoggingPlugin.pytest_runtest_setup in src/_pytest/skipping.py: No content. Likely deals with logging during setup. Not directly relevant unless skipping logs are affected.",
          "_pytest.logging.LoggingPlugin.pytest_runtest_call in src/_pytest/skipping.py: No content. Likely deals with logging during calls. Not relevant to the skip logic or message.",
          "_pytest.runner.pytest_runtest_setup in src/_pytest/skipping.py: Contains some logic but doesn't seem related to skipping functionality. Likely handles setup complexities.",
          "_pytest.runner.pytest_runtest_call in src/_pytest/skipping.py: Deals with reasons and results for failure, likely of little interest unless related to skip messages.",
          "_pytest.skipping.evaluate_skip_marks in outcomes.py: Highly relevant. Evaluates skips, needs review for module-level or improve document explanation.",
          "_pytest.skipping.pytest_runtest_setup in skipping.py: Manages how skips are raised, could have a role in how messages are formulated for module-level skips.",
          "_pytest.outcomes.skip: Defines skip functionality, including module-level skipping. Relevant to issues raised.",
          "...other symbols: mainly related to error logging and pytest hooks, largely peripheral to the specific skipping module issue noted."
        ]
      }
    },
    {
      "name": "SymbolSummary",
      "metadata": {
        "start": "2024-08-19T13:36:04.813691",
        "usage": "2.7000904083251953 ms"
      },
      "request": {
        "symbols": {
          "related_symbols": [
            {
              "name": "_pytest.mark.ParameterSet.param",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@classmethod\n    def param(\n        cls,\n        *values: object,\n        marks: Union[\"MarkDecorator\", Collection[Union[\"MarkDecorator\", \"Mark\"]]] = (),\n        id: Optional[str] = None,\n    ) -> \"ParameterSet\":\n        if isinstance(marks, MarkDecorator):\n            marks = (marks,)\n        else:\n            assert isinstance(marks, collections.abc.Collection)\n\n        if id is not None:\n            if not isinstance(id, str):\n                raise TypeError(f\"Expected id to be a string, got {type(id)}: {id!r}\")\n            id = ascii_escaped(id)\n        return cls(values, marks, id)",
              "start_line": 87,
              "start_column": 2272,
              "end_line": 4,
              "end_column": 2865,
              "start_byte": 2272,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@attr.s(init=False, auto_attribs=True)\nclass MarkDecorator:\n    \"\"\"A decorator for applying a mark on test functions and classes.\n\n    ``MarkDecorators`` are created with ``pytest.mark``::\n\n        mark1 = pytest.mark.NAME              # Simple MarkDecorator\n        mark2 = pytest.mark.NAME(name1=value) # Parametrized MarkDecorator\n\n    and can then be applied as decorators to test functions::\n\n        @mark2\n        def test_function():\n            pass\n\n    When a ``MarkDecorator`` is called, it does the following:\n\n    1. If called with a single class as its only positional argument and no\n       additional keyword arguments, it attaches the mark to the class so it\n       gets applied automatically to all test cases found in that class.\n\n    2. If called with a single function as its only positional argument and\n       no additional keyword arguments, it attaches the mark to the function,\n       containing all the arguments already stored internally in the\n       ``MarkDecorator``.\n\n    3. When called in any other case, it returns a new ``MarkDecorator``\n       instance with the original ``MarkDecorator``'s content updated with\n       the arguments passed to this call.\n\n    Note: The rules above prevent a ``MarkDecorator`` from storing only a\n    single function or class reference as its positional argument with no\n    additional keyword or positional arguments. You can work around this by\n    using `with_args()`.\n    \"\"\"\n\n    mark: Mark\n\n    def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark\n\n    @property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name\n\n    @property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args\n\n    @property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs\n\n    @property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)\n\n    def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"\n\n    def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)\n\n    # Type ignored because the overloads overlap with an incompatible\n    # return type. Not much we can do about that. Thankfully mypy picks\n    # the first match so it works out even if we break the rules.\n    @overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass\n\n    @overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass\n\n    def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
              "start_line": 271,
              "start_column": 8979,
              "end_line": 0,
              "end_column": 12391,
              "start_byte": 8979,
              "end_byte": 0
            },
            {
              "name": "_pytest.mark.MarkDecorator.mark",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "mark: Mark",
              "start_line": 307,
              "start_column": 10433,
              "end_line": 4,
              "end_column": 10443,
              "start_byte": 10433,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.__init__",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark",
              "start_line": 309,
              "start_column": 10449,
              "end_line": 4,
              "end_column": 10604,
              "start_byte": 10449,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.name",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name",
              "start_line": 314,
              "start_column": 10610,
              "end_line": 4,
              "end_column": 10711,
              "start_byte": 10610,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.args",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args",
              "start_line": 319,
              "start_column": 10717,
              "end_line": 4,
              "end_column": 10830,
              "start_byte": 10717,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.kwargs",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs",
              "start_line": 324,
              "start_column": 10836,
              "end_line": 4,
              "end_column": 10957,
              "start_byte": 10836,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.markname",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)",
              "start_line": 329,
              "start_column": 10963,
              "end_line": 4,
              "end_column": 11102,
              "start_byte": 10963,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.__repr__",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"",
              "start_line": 334,
              "start_column": 11108,
              "end_line": 4,
              "end_column": 11182,
              "start_byte": 11108,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.with_args",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)",
              "start_line": 337,
              "start_column": 11188,
              "end_line": 4,
              "end_column": 11593,
              "start_byte": 11188,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.__call__",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass",
              "start_line": 349,
              "start_column": 11807,
              "end_line": 4,
              "end_column": 11902,
              "start_byte": 11807,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.__call__",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "@overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass",
              "start_line": 353,
              "start_column": 11908,
              "end_line": 4,
              "end_column": 12006,
              "start_byte": 11908,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkDecorator.__call__",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
              "start_line": 357,
              "start_column": 12012,
              "end_line": 4,
              "end_column": 12391,
              "start_byte": 12012,
              "end_byte": 4
            },
            {
              "name": "_pytest.mark.MarkGenerator.__getattr__",
              "file_path": "src/_pytest/mark/structures.py",
              "file_content": "def __getattr__(self, name: str) -> MarkDecorator:\n        \"\"\"Generate a new :class:`MarkDecorator` with the given name.\"\"\"\n        if name[0] == \"_\":\n            raise AttributeError(\"Marker name must NOT start with underscore\")\n\n        if self._config is not None:\n            # We store a set of markers as a performance optimisation - if a mark\n            # name is in the set we definitely know it, but a mark may be known and\n            # not in the set.  We therefore start by updating the set!\n            if name not in self._markers:\n                for line in self._config.getini(\"markers\"):\n                    # example lines: \"skipif(condition): skip the given test if...\"\n                    # or \"hypothesis: tests which use Hypothesis\", so to get the\n                    # marker name we split on both `:` and `(`.\n                    marker = line.split(\":\")[0].split(\"(\")[0].strip()\n                    self._markers.add(marker)\n\n            # If the name is not in the set of known marks after updating,\n            # then it really is time to issue a warning or an error.\n            if name not in self._markers:\n                if self._config.option.strict_markers or self._config.option.strict:\n                    fail(\n                        f\"{name!r} not found in `markers` configuration option\",\n                        pytrace=False,\n                    )\n\n                # Raise a specific error for common misspellings of \"parametrize\".\n                if name in [\"parameterize\", \"parametrise\", \"parameterise\"]:\n                    __tracebackhide__ = True\n                    fail(f\"Unknown '{name}' mark, did you mean 'parametrize'?\")\n\n                warnings.warn(\n                    \"Unknown pytest.mark.%s - is this a typo?  You can register \"\n                    \"custom marks to avoid this warning - for details, see \"\n                    \"https://docs.pytest.org/en/stable/how-to/mark.html\" % name,\n                    PytestUnknownMarkWarning,\n                    2,\n                )\n\n        return MarkDecorator(Mark(name, (), {}, _ispytest=True), _ispytest=True)",
              "start_line": 498,
              "start_column": 16601,
              "end_line": 4,
              "end_column": 18718,
              "start_byte": 16601,
              "end_byte": 4
            },
            {
              "name": "_pytest.config.Config.getvalueorskip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1522,
              "start_column": 53901,
              "end_line": 4,
              "end_column": 54055,
              "start_byte": 53901,
              "end_byte": 4
            },
            {
              "name": "_pytest.config.Config._warn_about_skipped_plugins",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1547,
              "start_column": 54888,
              "end_line": 4,
              "end_column": 55169,
              "start_byte": 54888,
              "end_byte": 4
            },
            {
              "name": "_pytest.doctest._check_all_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 440,
              "start_column": 14834,
              "end_line": 0,
              "end_column": 15165,
              "start_byte": 14834,
              "end_byte": 0
            },
            {
              "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 216,
              "start_column": 7806,
              "end_line": 4,
              "end_column": 7947,
              "start_byte": 7806,
              "end_byte": 4
            },
            {
              "name": "_pytest.junitxml._NodeReporter.append_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 235,
              "start_column": 8519,
              "end_line": 4,
              "end_column": 9421,
              "start_byte": 8519,
              "end_byte": 4
            },
            {
              "name": "_pytest.outcomes.Skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "class Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
              "start_line": 50,
              "start_column": 1592,
              "end_line": 0,
              "end_column": 2297,
              "start_byte": 1592,
              "end_byte": 0
            },
            {
              "name": "_pytest.outcomes.Skipped.__module__",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "__module__ = \"builtins\"",
              "start_line": 53,
              "start_column": 1750,
              "end_line": 4,
              "end_column": 1773,
              "start_byte": 1750,
              "end_byte": 4
            },
            {
              "name": "_pytest.outcomes.Skipped.__init__",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
              "start_line": 55,
              "start_column": 1779,
              "end_line": 4,
              "end_column": 2297,
              "start_byte": 1779,
              "end_byte": 4
            },
            {
              "name": "_pytest.outcomes.skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
              "start_line": 122,
              "start_column": 3684,
              "end_line": 0,
              "end_column": 4711,
              "start_byte": 3684,
              "end_byte": 0
            },
            {
              "name": "_pytest.outcomes.importorskip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
              "start_line": 179,
              "start_column": 5761,
              "end_line": 0,
              "end_column": 7676,
              "start_byte": 5761,
              "end_byte": 0
            },
            {
              "name": "_pytest.pathlib.symlink_or_skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 428,
              "start_column": 13871,
              "end_line": 0,
              "end_column": 14118,
              "start_byte": 13871,
              "end_byte": 0
            },
            {
              "name": "_pytest.python.async_warn_and_skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
              "start_line": 163,
              "start_column": 5601,
              "end_line": 0,
              "end_column": 6147,
              "start_byte": 5601,
              "end_byte": 0
            },
            {
              "name": "_pytest.reports.BaseReport.skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
              "start_line": 151,
              "start_column": 4247,
              "end_line": 4,
              "end_column": 4328,
              "start_byte": 4247,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.pytest_addoption",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
              "start_line": 26,
              "start_column": 664,
              "end_line": 0,
              "end_column": 1160,
              "start_byte": 664,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_configure",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
              "start_line": 45,
              "start_column": 1163,
              "end_line": 0,
              "end_column": 2803,
              "start_byte": 1163,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.evaluate_condition",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
              "start_line": 84,
              "start_column": 2806,
              "end_line": 0,
              "end_column": 5541,
              "start_byte": 2806,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Skip",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
              "start_line": 159,
              "start_column": 5544,
              "end_line": 0,
              "end_column": 5697,
              "start_byte": 5544,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Skip.reason",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "onditions\n        like known bugs or missing features.\n ",
              "start_line": 163,
              "start_column": 5641,
              "end_line": 4,
              "end_column": 5697,
              "start_byte": 5641,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.evaluate_skip_marks",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "\"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerr",
              "start_line": 166,
              "start_column": 5700,
              "end_line": 0,
              "end_column": 6627,
              "start_byte": 5700,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Xfail",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
              "start_line": 194,
              "start_column": 6630,
              "end_line": 0,
              "end_column": 6885,
              "start_byte": 6630,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.Xfail.reason",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "n because\n        # of exi",
              "start_line": 198,
              "start_column": 6729,
              "end_line": 4,
              "end_column": 6755,
              "start_byte": 6729,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.Xfail.run",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": " directories with the sa",
              "start_line": 199,
              "start_column": 6760,
              "end_line": 4,
              "end_column": 6784,
              "start_byte": 6760,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.Xfail.strict",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "me we're trying to\n        ",
              "start_line": 200,
              "start_column": 6789,
              "end_line": 4,
              "end_column": 6816,
              "start_byte": 6789,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.Xfail.raises",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
              "start_line": 201,
              "start_column": 6821,
              "end_line": 4,
              "end_column": 6885,
              "start_byte": 6821,
              "end_byte": 4
            },
            {
              "name": "_pytest.skipping.evaluate_xfail_marks",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
              "start_line": 204,
              "start_column": 6888,
              "end_line": 0,
              "end_column": 7789,
              "start_byte": 6888,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.xfailed_key",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 230,
              "start_column": 7865,
              "end_line": 0,
              "end_column": 7906,
              "start_byte": 7865,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 233,
              "start_column": 7909,
              "end_line": 0,
              "end_column": 8290,
              "start_byte": 7909,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_call",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 244,
              "start_column": 8293,
              "end_line": 0,
              "end_column": 8868,
              "start_byte": 8293,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 261,
              "start_column": 8871,
              "end_line": 0,
              "end_column": 9945,
              "start_byte": 8871,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_report_teststatus",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 289,
              "start_column": 9948,
              "end_line": 0,
              "end_column": 10224,
              "start_byte": 9948,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal._folded_skips",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1318,
              "start_column": 47439,
              "end_line": 0,
              "end_column": 48734,
              "start_byte": 47439,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal._get_raw_skip_reason",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 1394,
              "start_column": 50198,
              "end_line": 0,
              "end_column": 50865,
              "start_byte": 50198,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest._is_skipped",
              "file_path": "src/_pytest/outcomes.py",
              "file_content": "",
              "start_line": 403,
              "start_column": 13805,
              "end_line": 0,
              "end_column": 13971,
              "start_byte": 13805,
              "end_byte": 0
            }
          ],
          "root_caused_symbols": [
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 802,
              "start_column": 26043,
              "end_line": 4,
              "end_column": 26214,
              "start_byte": 26043,
              "end_byte": 4
            },
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 807,
              "start_column": 26220,
              "end_line": 4,
              "end_column": 26389,
              "start_byte": 26220,
              "end_byte": 4
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 481,
              "start_column": 16683,
              "end_line": 0,
              "end_column": 17027,
              "start_byte": 16683,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 491,
              "start_column": 17030,
              "end_line": 0,
              "end_column": 17204,
              "start_byte": 17030,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 704,
              "start_column": 25635,
              "end_line": 4,
              "end_column": 25950,
              "start_byte": 25635,
              "end_byte": 4
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 712,
              "start_column": 25956,
              "end_line": 4,
              "end_column": 26165,
              "start_byte": 25956,
              "end_byte": 4
            },
            {
              "name": "_pytest.runner.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
              "start_line": 153,
              "start_column": 4972,
              "end_line": 0,
              "end_column": 5102,
              "start_byte": 4972,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n   ",
              "start_line": 158,
              "start_column": 5105,
              "end_line": 0,
              "end_column": 5640,
              "start_byte": 5105,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.evaluate_skip_marks",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
              "start_line": 166,
              "start_column": 5700,
              "end_line": 0,
              "end_column": 6627,
              "start_byte": 5700,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
              "start_line": 233,
              "start_column": 7909,
              "end_line": 0,
              "end_column": 8290,
              "start_byte": 7909,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # The test run may have added an xfail mark dynamically.\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)",
              "start_line": 244,
              "start_column": 8293,
              "end_line": 0,
              "end_column": 8868,
              "start_byte": 8293,
              "end_byte": 0
            },
            {
              "name": "_pytest.threadexception.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
              "start_line": 75,
              "start_column": 2442,
              "end_line": 0,
              "end_column": 2596,
              "start_byte": 2442,
              "end_byte": 0
            },
            {
              "name": "_pytest.threadexception.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "t them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/",
              "start_line": 80,
              "start_column": 2599,
              "end_line": 0,
              "end_column": 2753,
              "start_byte": 2599,
              "end_byte": 0
            },
            {
              "name": "_pytest.unraisableexception.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
              "start_line": 80,
              "start_column": 2705,
              "end_line": 0,
              "end_column": 2864,
              "start_byte": 2705,
              "end_byte": 0
            },
            {
              "name": "_pytest.unraisableexception.pytest_runtest_call",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ect) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    c",
              "start_line": 85,
              "start_column": 2867,
              "end_line": 0,
              "end_column": 3025,
              "start_byte": 2867,
              "end_byte": 0
            },
            {
              "name": "_pytest.nose.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
              "start_line": 8,
              "start_column": 236,
              "end_line": 0,
              "end_column": 1174,
              "start_byte": 236,
              "end_byte": 0
            }
          ]
        },
        "symbols_evaluate": {
          "scratch_pad": "1.1 The user is describing an issue with skipping tests in pytest for a module that uses features only available in more recent versions of Python (3.8 and above). When trying to skip these tests on older versions, they are running into issues with how pytest handles the `skip` functionality at the module level.\n\n1.2 The issue arises because the `skip` function from pytest is not supposed to be used at the module level without the `allow_module_level=True` parameter. However, the error message does not make this clear and suggests using `pytest.mark.skip` or `pytest.mark.skipif`, which leads to the tests attempting to run anyway and failing on syntax errors.\n\n1.3 The user proposes that either the documentation or the error messages be improved to clearly indicate how to handle skips at the module level, or that a new API (`skip_module`) be introduced to simplify this process.\n\n1.4 The expectation should be to skip tests entirely at the module level in a seamless way that is clearly documented or easy to use.\n\n1.5 The issue could be resolved by updating the error messages in pytest to include information about the `allow_module_level` parameter when the `skip` function is misused, and/or by introducing a new API that encapsulates this behavior in a user-friendly manner.\n\n2. Importantly though, the solution needs to be accurately reflected in the parts of the codebase where skipping and handling of skips is defined or documented.\n\n\n2.1 Reviewing the symbols, most are hook implementations or involve exception handling/logging.\n2.2 as this only affects on module level skipping and errors associated with it not being clear enough.\n2.3 the `evaluate_skip_marks` function is highly relevant since it evaluates whether a skip or skipif marker should cause the given test/item to be skipped at runtime.\n2.4 the `skip` function itself should be reviewed to ensure adequate handling of the `allow_module_level` flag and to clarify its documentation.\n\nIdentified Place to make updates: \n1. \"_pytest.skipping.evaluate_skip_marks\" function should support the proposed solution or needs enhancement on explaining the module level skipping.\n2. \"_pytest.outcomes.skip\" function's handling of `allow_module_level` flag seems central to the issue.",
          "symbols": [
            {
              "name": "_pytest.skipping.evaluate_skip_marks",
              "file_path": "src/_pytest/skipping.py",
              "reason": "This function evaluates skip marks and could potentially be updated to handle or message about module-level skips more effectively or provide the necessary hooks for a new API."
            },
            {
              "name": "_pytest.outcomes.skip",
              "file_path": "src/_pytest/outcomes.py",
              "reason": "The skip function defines how skips are triggered, including the `allow_module_level` parameter that needs clearer documentation or alteration as per the issue described."
            }
          ],
          "more_info": [],
          "is_done": false,
          "symbols_scratch_pad": [
            "_pytest.capture.CaptureManager.pytest_runtest_setup in src/_pytest/skipping.py: No content provided; seems like a hook that deals with setup, but not directly related to skipping or error messaging. Ignored for relevance to the issue.",
            "_pytest.capture.CaptureManager.pytest_runtest_call in src/_pytest/skipping.py: Similar to above; No content provided. Hook related to the test call stage. Not directly relevant to skipping functionality.",
            "_pytest.hookspec.pytest_runtest_setup in src/_pytest/skipping.py: No content. Likely a hookspec for setup. Not directly related to skipping or error messaging.",
            "_pytest.hookspec.pytest_runtest_call in src/_pytest/skipping.py: No content. Likely a hookspec for call. Not directly related to skipping or error messaging.",
            "_pytest.logging.LoggingPlugin.pytest_runtest_setup in src/_pytest/skipping.py: No content. Likely deals with logging during setup. Not directly relevant unless skipping logs are affected.",
            "_pytest.logging.LoggingPlugin.pytest_runtest_call in src/_pytest/skipping.py: No content. Likely deals with logging during calls. Not relevant to the skip logic or message.",
            "_pytest.runner.pytest_runtest_setup in src/_pytest/skipping.py: Contains some logic but doesn't seem related to skipping functionality. Likely handles setup complexities.",
            "_pytest.runner.pytest_runtest_call in src/_pytest/skipping.py: Deals with reasons and results for failure, likely of little interest unless related to skip messages.",
            "_pytest.skipping.evaluate_skip_marks in outcomes.py: Highly relevant. Evaluates skips, needs review for module-level or improve document explanation.",
            "_pytest.skipping.pytest_runtest_setup in skipping.py: Manages how skips are raised, could have a role in how messages are formulated for module-level skips.",
            "_pytest.outcomes.skip: Defines skip functionality, including module-level skipping. Relevant to issues raised.",
            "...other symbols: mainly related to error logging and pytest hooks, largely peripheral to the specific skipping module issue noted."
          ]
        }
      },
      "response": {
        "file_paths": [
          "src/_pytest/skipping.py",
          "src/_pytest/outcomes.py"
        ],
        "symbols": [
          "_pytest.skipping.evaluate_skip_marks",
          "_pytest.outcomes.skip"
        ],
        "code_spans": {
          "_pytest.mark.ParameterSet.param": {
            "name": "_pytest.mark.ParameterSet.param",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@classmethod\n    def param(\n        cls,\n        *values: object,\n        marks: Union[\"MarkDecorator\", Collection[Union[\"MarkDecorator\", \"Mark\"]]] = (),\n        id: Optional[str] = None,\n    ) -> \"ParameterSet\":\n        if isinstance(marks, MarkDecorator):\n            marks = (marks,)\n        else:\n            assert isinstance(marks, collections.abc.Collection)\n\n        if id is not None:\n            if not isinstance(id, str):\n                raise TypeError(f\"Expected id to be a string, got {type(id)}: {id!r}\")\n            id = ascii_escaped(id)\n        return cls(values, marks, id)",
            "start_line": 87,
            "start_column": 2272,
            "end_line": 4,
            "end_column": 2865,
            "start_byte": 2272,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator": {
            "name": "_pytest.mark.MarkDecorator",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@attr.s(init=False, auto_attribs=True)\nclass MarkDecorator:\n    \"\"\"A decorator for applying a mark on test functions and classes.\n\n    ``MarkDecorators`` are created with ``pytest.mark``::\n\n        mark1 = pytest.mark.NAME              # Simple MarkDecorator\n        mark2 = pytest.mark.NAME(name1=value) # Parametrized MarkDecorator\n\n    and can then be applied as decorators to test functions::\n\n        @mark2\n        def test_function():\n            pass\n\n    When a ``MarkDecorator`` is called, it does the following:\n\n    1. If called with a single class as its only positional argument and no\n       additional keyword arguments, it attaches the mark to the class so it\n       gets applied automatically to all test cases found in that class.\n\n    2. If called with a single function as its only positional argument and\n       no additional keyword arguments, it attaches the mark to the function,\n       containing all the arguments already stored internally in the\n       ``MarkDecorator``.\n\n    3. When called in any other case, it returns a new ``MarkDecorator``\n       instance with the original ``MarkDecorator``'s content updated with\n       the arguments passed to this call.\n\n    Note: The rules above prevent a ``MarkDecorator`` from storing only a\n    single function or class reference as its positional argument with no\n    additional keyword or positional arguments. You can work around this by\n    using `with_args()`.\n    \"\"\"\n\n    mark: Mark\n\n    def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark\n\n    @property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name\n\n    @property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args\n\n    @property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs\n\n    @property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)\n\n    def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"\n\n    def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)\n\n    # Type ignored because the overloads overlap with an incompatible\n    # return type. Not much we can do about that. Thankfully mypy picks\n    # the first match so it works out even if we break the rules.\n    @overload\n    def __call__(self, arg: Markable) -> Markable:  # type: ignore[misc]\n        pass\n\n    @overload\n    def __call__(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        pass\n\n    def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
            "start_line": 271,
            "start_column": 8979,
            "end_line": 0,
            "end_column": 12391,
            "start_byte": 8979,
            "end_byte": 0
          },
          "_pytest.mark.MarkDecorator.mark": {
            "name": "_pytest.mark.MarkDecorator.mark",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "mark: Mark",
            "start_line": 307,
            "start_column": 10433,
            "end_line": 4,
            "end_column": 10443,
            "start_byte": 10433,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.__init__": {
            "name": "_pytest.mark.MarkDecorator.__init__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __init__(self, mark: Mark, *, _ispytest: bool = False) -> None:\n        \"\"\":meta private:\"\"\"\n        check_ispytest(_ispytest)\n        self.mark = mark",
            "start_line": 309,
            "start_column": 10449,
            "end_line": 4,
            "end_column": 10604,
            "start_byte": 10449,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.name": {
            "name": "_pytest.mark.MarkDecorator.name",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def name(self) -> str:\n        \"\"\"Alias for mark.name.\"\"\"\n        return self.mark.name",
            "start_line": 314,
            "start_column": 10610,
            "end_line": 4,
            "end_column": 10711,
            "start_byte": 10610,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.args": {
            "name": "_pytest.mark.MarkDecorator.args",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def args(self) -> Tuple[Any, ...]:\n        \"\"\"Alias for mark.args.\"\"\"\n        return self.mark.args",
            "start_line": 319,
            "start_column": 10717,
            "end_line": 4,
            "end_column": 10830,
            "start_byte": 10717,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.kwargs": {
            "name": "_pytest.mark.MarkDecorator.kwargs",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def kwargs(self) -> Mapping[str, Any]:\n        \"\"\"Alias for mark.kwargs.\"\"\"\n        return self.mark.kwargs",
            "start_line": 324,
            "start_column": 10836,
            "end_line": 4,
            "end_column": 10957,
            "start_byte": 10836,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.markname": {
            "name": "_pytest.mark.MarkDecorator.markname",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "@property\n    def markname(self) -> str:\n        \"\"\":meta private:\"\"\"\n        return self.name  # for backward-compat (2.4.1 had this attr)",
            "start_line": 329,
            "start_column": 10963,
            "end_line": 4,
            "end_column": 11102,
            "start_byte": 10963,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.__repr__": {
            "name": "_pytest.mark.MarkDecorator.__repr__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __repr__(self) -> str:\n        return f\"<MarkDecorator {self.mark!r}>\"",
            "start_line": 334,
            "start_column": 11108,
            "end_line": 4,
            "end_column": 11182,
            "start_byte": 11108,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.with_args": {
            "name": "_pytest.mark.MarkDecorator.with_args",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def with_args(self, *args: object, **kwargs: object) -> \"MarkDecorator\":\n        \"\"\"Return a MarkDecorator with extra arguments added.\n\n        Unlike calling the MarkDecorator, with_args() can be used even\n        if the sole argument is a callable/class.\n        \"\"\"\n        mark = Mark(self.name, args, kwargs, _ispytest=True)\n        return MarkDecorator(self.mark.combined_with(mark), _ispytest=True)",
            "start_line": 337,
            "start_column": 11188,
            "end_line": 4,
            "end_column": 11593,
            "start_byte": 11188,
            "end_byte": 4
          },
          "_pytest.mark.MarkDecorator.__call__": {
            "name": "_pytest.mark.MarkDecorator.__call__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __call__(self, *args: object, **kwargs: object):\n        \"\"\"Call the MarkDecorator.\"\"\"\n        if args and not kwargs:\n            func = args[0]\n            is_class = inspect.isclass(func)\n            if len(args) == 1 and (istestfunc(func) or is_class):\n                store_mark(func, self.mark)\n                return func\n        return self.with_args(*args, **kwargs)",
            "start_line": 357,
            "start_column": 12012,
            "end_line": 4,
            "end_column": 12391,
            "start_byte": 12012,
            "end_byte": 4
          },
          "_pytest.mark.MarkGenerator.__getattr__": {
            "name": "_pytest.mark.MarkGenerator.__getattr__",
            "file_path": "src/_pytest/mark/structures.py",
            "file_content": "def __getattr__(self, name: str) -> MarkDecorator:\n        \"\"\"Generate a new :class:`MarkDecorator` with the given name.\"\"\"\n        if name[0] == \"_\":\n            raise AttributeError(\"Marker name must NOT start with underscore\")\n\n        if self._config is not None:\n            # We store a set of markers as a performance optimisation - if a mark\n            # name is in the set we definitely know it, but a mark may be known and\n            # not in the set.  We therefore start by updating the set!\n            if name not in self._markers:\n                for line in self._config.getini(\"markers\"):\n                    # example lines: \"skipif(condition): skip the given test if...\"\n                    # or \"hypothesis: tests which use Hypothesis\", so to get the\n                    # marker name we split on both `:` and `(`.\n                    marker = line.split(\":\")[0].split(\"(\")[0].strip()\n                    self._markers.add(marker)\n\n            # If the name is not in the set of known marks after updating,\n            # then it really is time to issue a warning or an error.\n            if name not in self._markers:\n                if self._config.option.strict_markers or self._config.option.strict:\n                    fail(\n                        f\"{name!r} not found in `markers` configuration option\",\n                        pytrace=False,\n                    )\n\n                # Raise a specific error for common misspellings of \"parametrize\".\n                if name in [\"parameterize\", \"parametrise\", \"parameterise\"]:\n                    __tracebackhide__ = True\n                    fail(f\"Unknown '{name}' mark, did you mean 'parametrize'?\")\n\n                warnings.warn(\n                    \"Unknown pytest.mark.%s - is this a typo?  You can register \"\n                    \"custom marks to avoid this warning - for details, see \"\n                    \"https://docs.pytest.org/en/stable/how-to/mark.html\" % name,\n                    PytestUnknownMarkWarning,\n                    2,\n                )\n\n        return MarkDecorator(Mark(name, (), {}, _ispytest=True), _ispytest=True)",
            "start_line": 498,
            "start_column": 16601,
            "end_line": 4,
            "end_column": 18718,
            "start_byte": 16601,
            "end_byte": 4
          },
          "_pytest.config.Config.getvalueorskip": {
            "name": "_pytest.config.Config.getvalueorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1522,
            "start_column": 53901,
            "end_line": 4,
            "end_column": 54055,
            "start_byte": 53901,
            "end_byte": 4
          },
          "_pytest.config.Config._warn_about_skipped_plugins": {
            "name": "_pytest.config.Config._warn_about_skipped_plugins",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1547,
            "start_column": 54888,
            "end_line": 4,
            "end_column": 55169,
            "start_byte": 54888,
            "end_byte": 4
          },
          "_pytest.doctest._check_all_skipped": {
            "name": "_pytest.doctest._check_all_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 440,
            "start_column": 14834,
            "end_line": 0,
            "end_column": 15165,
            "start_byte": 14834,
            "end_byte": 0
          },
          "_pytest.junitxml._NodeReporter.append_collect_skipped": {
            "name": "_pytest.junitxml._NodeReporter.append_collect_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 216,
            "start_column": 7806,
            "end_line": 4,
            "end_column": 7947,
            "start_byte": 7806,
            "end_byte": 4
          },
          "_pytest.junitxml._NodeReporter.append_skipped": {
            "name": "_pytest.junitxml._NodeReporter.append_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 235,
            "start_column": 8519,
            "end_line": 4,
            "end_column": 9421,
            "start_byte": 8519,
            "end_byte": 4
          },
          "_pytest.outcomes.Skipped": {
            "name": "_pytest.outcomes.Skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "class Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 50,
            "start_column": 1592,
            "end_line": 0,
            "end_column": 2297,
            "start_byte": 1592,
            "end_byte": 0
          },
          "_pytest.outcomes.Skipped.__module__": {
            "name": "_pytest.outcomes.Skipped.__module__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "__module__ = \"builtins\"",
            "start_line": 53,
            "start_column": 1750,
            "end_line": 4,
            "end_column": 1773,
            "start_byte": 1750,
            "end_byte": 4
          },
          "_pytest.outcomes.Skipped.__init__": {
            "name": "_pytest.outcomes.Skipped.__init__",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location",
            "start_line": 55,
            "start_column": 1779,
            "end_line": 4,
            "end_column": 2297,
            "start_byte": 1779,
            "end_byte": 4
          },
          "_pytest.outcomes.skip": {
            "name": "_pytest.outcomes.skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)",
            "start_line": 122,
            "start_column": 3684,
            "end_line": 0,
            "end_column": 4711,
            "start_byte": 3684,
            "end_byte": 0
          },
          "_pytest.outcomes.importorskip": {
            "name": "_pytest.outcomes.importorskip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "def importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attribute must be at\n        least this minimal version, otherwise the test is still skipped.\n    :param str reason:\n        If given, this reason is shown as the message when the module cannot\n        be imported.\n\n    :returns:\n        The imported module. This should be assigned to its canonical name.\n\n    Example::\n\n        docutils = pytest.importorskip(\"docutils\")\n    \"\"\"\n    import warnings\n\n    __tracebackhide__ = True\n    compile(modname, \"\", \"eval\")  # to catch syntaxerrors\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilter(\"ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod",
            "start_line": 179,
            "start_column": 5761,
            "end_line": 0,
            "end_column": 7676,
            "start_byte": 5761,
            "end_byte": 0
          },
          "_pytest.pathlib.symlink_or_skip": {
            "name": "_pytest.pathlib.symlink_or_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 428,
            "start_column": 13871,
            "end_line": 0,
            "end_column": 14118,
            "start_byte": 13871,
            "end_byte": 0
          },
          "_pytest.python.async_warn_and_skip": {
            "name": "_pytest.python.async_warn_and_skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "are a test to be xfailed under certain conditions\n        like known bugs or missing features.\n    \"\"\"\n    __tracebackhide__ = True\n    raise XFailed(reason)\n\n\ndef importorskip(\n    modname: str, minversion: Optional[str] = None, reason: Optional[str] = None\n) -> Any:\n    \"\"\"Import and return the requested module ``modname``, or skip the\n    current test if the module cannot be imported.\n\n    :param str modname:\n        The name of the module to import.\n    :param str minversion:\n        If given, the imported module's ``__version__`` attri",
            "start_line": 163,
            "start_column": 5601,
            "end_line": 0,
            "end_column": 6147,
            "start_byte": 5601,
            "end_byte": 0
          },
          "_pytest.reports.BaseReport.skipped": {
            "name": "_pytest.reports.BaseReport.skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be",
            "start_line": 151,
            "start_column": 4247,
            "end_line": 4,
            "end_column": 4328,
            "start_byte": 4247,
            "end_byte": 4
          },
          "_pytest.skipping.pytest_addoption": {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ic\n\n    Protocol = Generic\n\n\nclass OutcomeException(BaseException):\n    \"\"\"OutcomeException and its subclass instances indicate and contain info\n    about test and collection outcomes.\"\"\"\n\n    def __init__(self, msg: Optional[str] = None, pytrace: bool = True) -> None:\n        if msg is not None and not isinstance(msg, str):\n            error_msg = (  # type: ignore[unreachable]\n                \"{} expected string as 'msg' parameter, got '{}' instead.\\n\"\n                \"Perhaps you meant to",
            "start_line": 26,
            "start_column": 664,
            "end_line": 0,
            "end_column": 1160,
            "start_byte": 664,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_configure": {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "e a mark?\"\n            )\n            raise TypeError(error_msg.format(type(self).__name__, type(msg).__name__))\n        BaseException.__init__(self, msg)\n        self.msg = msg\n        self.pytrace = pytrace\n\n    def __repr__(self) -> str:\n        if self.msg is not None:\n            return self.msg\n        return f\"<{self.__class__.__name__} instance>\"\n\n    __str__ = __repr__\n\n\nTEST_OUTCOME = (OutcomeException, Exception)\n\n\nclass Skipped(OutcomeException):\n    # XXX hackish: on 3k we fake to live in the builtins\n    # in order to have Skipped exception printing shorter/nicer\n    __module__ = \"builtins\"\n\n    def __init__(\n        self,\n        msg: Optional[str] = None,\n        pytrace: bool = True,\n        allow_module_level: bool = False,\n        *,\n        _use_item_location: bool = False,\n    ) -> None:\n        OutcomeException.__init__(self, msg=msg, pytrace=pytrace)\n        self.allow_module_level = allow_module_level\n        # If true, the skip location is reported as the item's location,\n        # instead of the place that raises the exception/calls skip().\n        self._use_item_location = _use_item_location\n\n\nclass Failed(OutcomeException):\n    \"\"\"Raised from an explicit call to pytest.fail().\"\"\"\n\n    __module__ = \"builtins\"\n\n\nclass Exit(Exception):\n    \"\"\"Raised for immediate program exits (no tracebacks/summaries).\"\"\"\n\n    def __init__(\n        self, msg: str = \"unknown reason\", returncode: Optional[int] = None\n    ) -> None:\n        self.msg = msg\n        self.returncode = returncode\n        super().__init__(msg)\n\n\n# Elaborate hack to work around https://github.com/python/mypy/issues/2087.\n# Ideally ",
            "start_line": 45,
            "start_column": 1163,
            "end_line": 0,
            "end_column": 2803,
            "start_byte": 1163,
            "end_byte": 0
          },
          "_pytest.skipping.evaluate_condition": {
            "name": "_pytest.skipping.evaluate_condition",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ld just be `exit.Exception = Exit` etc.\n\n_F = TypeVar(\"_F\", bound=Callable[..., object])\n_ET = TypeVar(\"_ET\", bound=Type[BaseException])\n\n\nclass _WithException(Protocol[_F, _ET]):\n    Exception: _ET\n    __call__: _F\n\n\ndef _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n\n    return decorate\n\n\n# Exposed helper methods.\n\n\n@_with_exception(Exit)\ndef exit(msg: str, returncode: Optional[int] = None) -> \"NoReturn\":\n    \"\"\"Exit testing process.\n\n    :param str msg: Message to display upon exit.\n    :param int returncode: Return code to be used when exiting pytest.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Exit(msg, returncode)\n\n\n@_with_exception(Skipped)\ndef skip(msg: str = \"\", *, allow_module_level: bool = False) -> \"NoReturn\":\n    \"\"\"Skip an executing test with the given message.\n\n    This function should be called only during testing (setup, call or teardown) or\n    during collection by using the ``allow_module_level`` flag.  This function can\n    be called in doctests as well.\n\n    :param bool allow_module_level:\n        Allows this function to be called at module level, skipping the rest\n        of the module. Defaults to False.\n\n    .. note::\n        It is better to use the :ref:`pytest.mark.skipif ref` marker when\n        possible to declare a test to be skipped under certain conditions\n        like mismatching platforms or dependencies.\n        Similarly, use the ``# doctest: +SKIP`` directive (see `doctest.SKIP\n        <https://docs.python.org/3/library/how-to/doctest.html#doctest.SKIP>`_)\n        to skip a doctest statically.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Skipped(msg=msg, allow_module_level=allow_module_level)\n\n\n@_with_exception(Failed)\ndef fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n    \"\"\"Explicitly fail an executing test with the given message.\n\n    :param str msg:\n        The message to show the user as reason for the failure.\n    :param bool pytrace:\n        If False, msg represents the full failure information and no\n        python traceback will be reported.\n    \"\"\"\n    __tracebackhide__ = True\n    raise Failed(msg=msg, pytrace=pytrace)\n\n\nclass XFailed(Failed):\n    \"\"\"Raised from an explicit call to pytest.xfail().\"\"\"\n\n\n@_with_exception(XFailed)\ndef xfail(reason: str = \"\") -> \"NoReturn\":\n    \"\"\"Imperatively xfail an executing test or setup function with the given reason.\n\n    This function should be called only during testing (setup, call or teardown).\n\n    .. note::\n        It is better to use the :ref:",
            "start_line": 84,
            "start_column": 2806,
            "end_line": 0,
            "end_column": 5541,
            "start_byte": 2806,
            "end_byte": 0
          },
          "_pytest.skipping.Skip": {
            "name": "_pytest.skipping.Skip",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "test.mark.xfail ref` marker when\n        possible to declare a test to be xfailed under certain conditions\n        like known bugs or missing features.\n ",
            "start_line": 159,
            "start_column": 5544,
            "end_line": 0,
            "end_column": 5697,
            "start_byte": 5544,
            "end_byte": 0
          },
          "_pytest.skipping.Skip.reason": {
            "name": "_pytest.skipping.Skip.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "onditions\n        like known bugs or missing features.\n ",
            "start_line": 163,
            "start_column": 5641,
            "end_line": 4,
            "end_column": 5697,
            "start_byte": 5641,
            "end_byte": 4
          },
          "_pytest.skipping.evaluate_skip_marks": {
            "name": "_pytest.skipping.evaluate_skip_marks",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    \"\"\"Evaluate skip and skipif marks on item, returning Skip if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"skipif\"):\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Skip(reason)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n\n    for mark in item.iter_markers(name=\"skip\"):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + \" - maybe you meant pytest.mark.skipif?\") from None\n\n    return None",
            "start_line": 166,
            "start_column": 5700,
            "end_line": 0,
            "end_column": 6627,
            "start_byte": 5700,
            "end_byte": 0
          },
          "_pytest.skipping.Xfail": {
            "name": "_pytest.skipping.Xfail",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "\n\n    with warnings.catch_warnings():\n        # Make sure to ignore ImportWarnings that might happen because\n        # of existing directories with the same name we're trying to\n        # import but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 194,
            "start_column": 6630,
            "end_line": 0,
            "end_column": 6885,
            "start_byte": 6630,
            "end_byte": 0
          },
          "_pytest.skipping.Xfail.reason": {
            "name": "_pytest.skipping.Xfail.reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "n because\n        # of exi",
            "start_line": 198,
            "start_column": 6729,
            "end_line": 4,
            "end_column": 6755,
            "start_byte": 6729,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail.run": {
            "name": "_pytest.skipping.Xfail.run",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": " directories with the sa",
            "start_line": 199,
            "start_column": 6760,
            "end_line": 4,
            "end_column": 6784,
            "start_byte": 6760,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail.strict": {
            "name": "_pytest.skipping.Xfail.strict",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "me we're trying to\n        ",
            "start_line": 200,
            "start_column": 6789,
            "end_line": 4,
            "end_column": 6816,
            "start_byte": 6789,
            "end_byte": 4
          },
          "_pytest.skipping.Xfail.raises": {
            "name": "_pytest.skipping.Xfail.raises",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ort but without a __init__.py file.\n        warnings.simplefilte",
            "start_line": 201,
            "start_column": 6821,
            "end_line": 4,
            "end_column": 6885,
            "start_byte": 6821,
            "end_byte": 4
          },
          "_pytest.skipping.evaluate_xfail_marks": {
            "name": "_pytest.skipping.evaluate_xfail_marks",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "ignore\")\n        try:\n            __import__(modname)\n        except ImportError as exc:\n            if reason is None:\n                reason = f\"could not import {modname!r}: {exc}\"\n            raise Skipped(reason, allow_module_level=True) from None\n    mod = sys.modules[modname]\n    if minversion is None:\n        return mod\n    verattr = getattr(mod, \"__version__\", None)\n    if minversion is not None:\n        # Imported lazily to improve start-up time.\n        from packaging.version import Version\n\n        if verattr is None or Version(verattr) < Version(minversion):\n            raise Skipped(\n                \"module %r has __version__ %r, required is: %r\"\n                % (modname, verattr, minversion),\n                allow_module_level=True,\n            )\n    return mod\n",
            "start_line": 204,
            "start_column": 6888,
            "end_line": 0,
            "end_column": 7789,
            "start_byte": 6888,
            "end_byte": 0
          },
          "_pytest.skipping.xfailed_key": {
            "name": "_pytest.skipping.xfailed_key",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 230,
            "start_column": 7865,
            "end_line": 0,
            "end_column": 7906,
            "start_byte": 7865,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        raise skip.Exception(skipped.reason, _use_item_location=True)\n\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)",
            "start_line": 233,
            "start_column": 7909,
            "end_line": 0,
            "end_column": 8290,
            "start_byte": 7909,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_call": {
            "name": "_pytest.skipping.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if xfailed and not item.config.option.runxfail and not xfailed.run:\n        xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # The test run may have added an xfail mark dynamically.\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)",
            "start_line": 244,
            "start_column": 8293,
            "end_line": 0,
            "end_column": 8868,
            "start_byte": 8293,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 261,
            "start_column": 8871,
            "end_line": 0,
            "end_column": 9945,
            "start_byte": 8871,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_report_teststatus": {
            "name": "_pytest.skipping.pytest_report_teststatus",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 289,
            "start_column": 9948,
            "end_line": 0,
            "end_column": 10224,
            "start_byte": 9948,
            "end_byte": 0
          },
          "_pytest.terminal._folded_skips": {
            "name": "_pytest.terminal._folded_skips",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1318,
            "start_column": 47439,
            "end_line": 0,
            "end_column": 48734,
            "start_byte": 47439,
            "end_byte": 0
          },
          "_pytest.terminal._get_raw_skip_reason": {
            "name": "_pytest.terminal._get_raw_skip_reason",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 1394,
            "start_column": 50198,
            "end_line": 0,
            "end_column": 50865,
            "start_byte": 50198,
            "end_byte": 0
          },
          "_pytest.unittest._is_skipped": {
            "name": "_pytest.unittest._is_skipped",
            "file_path": "src/_pytest/outcomes.py",
            "file_content": "",
            "start_line": 403,
            "start_column": 13805,
            "end_line": 0,
            "end_column": 13971,
            "start_byte": 13805,
            "end_byte": 0
          },
          "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 802,
            "start_column": 26043,
            "end_line": 4,
            "end_column": 26214,
            "start_byte": 26043,
            "end_byte": 4
          },
          "_pytest.capture.CaptureManager.pytest_runtest_call": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 807,
            "start_column": 26220,
            "end_line": 4,
            "end_column": 26389,
            "start_byte": 26220,
            "end_byte": 4
          },
          "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 481,
            "start_column": 16683,
            "end_line": 0,
            "end_column": 17027,
            "start_byte": 16683,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_runtest_call": {
            "name": "_pytest.hookspec.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 491,
            "start_column": 17030,
            "end_line": 0,
            "end_column": 17204,
            "start_byte": 17030,
            "end_byte": 0
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 704,
            "start_column": 25635,
            "end_line": 4,
            "end_column": 25950,
            "start_byte": 25635,
            "end_byte": 4
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_call": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 712,
            "start_column": 25956,
            "end_line": 4,
            "end_column": 26165,
            "start_byte": 25956,
            "end_byte": 4
          },
          "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".format_exception_only(type(exc), exc),\n            ]\n            fail(\"\\n\".join(msglines), pytrace=False)\n\n    reason = mark.kwar",
            "start_line": 153,
            "start_column": 4972,
            "end_line": 0,
            "end_column": 5102,
            "start_byte": 4972,
            "end_byte": 0
          },
          "_pytest.runner.pytest_runtest_call": {
            "name": "_pytest.runner.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "get(\"reason\", None)\n    if reason is None:\n        if isinstance(condition, str):\n            reason = \"condition: \" + condition\n        else:\n            # XXX better be checked at collection time\n            msg = (\n                \"Error evaluating %r: \" % mark.name\n                + \"you need to specify reason=STRING when using booleans as conditions.\"\n            )\n            fail(msg, pytrace=False)\n\n    return result, reason\n\n\n@attr.s(slots=True, frozen=True)\nclass Skip:\n    \"\"\"The result of evaluate_skip_marks().\"\"\"\n\n   ",
            "start_line": 158,
            "start_column": 5105,
            "end_line": 0,
            "end_column": 5640,
            "start_byte": 5105,
            "end_byte": 0
          },
          "_pytest.threadexception.pytest_runtest_setup": {
            "name": "_pytest.threadexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "er reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can ",
            "start_line": 75,
            "start_column": 2442,
            "end_line": 0,
            "end_column": 2596,
            "start_byte": 2442,
            "end_byte": 0
          },
          "_pytest.threadexception.pytest_runtest_call": {
            "name": "_pytest.threadexception.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "t them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/stable/",
            "start_line": 80,
            "start_column": 2599,
            "end_line": 0,
            "end_column": 2753,
            "start_byte": 2599,
            "end_byte": 0
          },
          "_pytest.unraisableexception.pytest_runtest_setup": {
            "name": "_pytest.unraisableexception.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\",\n    )\n\n\ndef evaluate_condition(item: Item, mark: Mark, condition: ",
            "start_line": 80,
            "start_column": 2705,
            "end_line": 0,
            "end_column": 2864,
            "start_byte": 2705,
            "end_byte": 0
          },
          "_pytest.unraisableexception.pytest_runtest_call": {
            "name": "_pytest.unraisableexception.pytest_runtest_call",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ect) -> Tuple[bool, str]:\n    \"\"\"Evaluate a single skipif/xfail condition.\n\n    If an old-style string condition is given, it is eval()'d, otherwise the\n    c",
            "start_line": 85,
            "start_column": 2867,
            "end_line": 0,
            "end_column": 3025,
            "start_byte": 2867,
            "end_byte": 0
          },
          "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "import Type\n\nimport attr\n\nfrom _pytest.config import Config\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.reports import BaseReport\nfrom _pytest.runner import CallInfo\nfrom _pytest.store import StoreKey\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_",
            "start_line": 8,
            "start_column": 236,
            "end_line": 0,
            "end_column": 1174,
            "start_byte": 236,
            "end_byte": 0
          }
        }
      }
    }
  ]
}