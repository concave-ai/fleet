{
  "total_cost": 0,
  "task_id": "pytest-dev__pytest-7236",
  "model": "gpt-4o-2024-08-06",
  "spans": [
    {
      "name": "KeywordExtract",
      "metadata": {
        "start": "2024-08-17T22:37:47.357331",
        "usage": "1390.5320167541504 ms"
      },
      "request": {
        "repo_tokens": [
          "suspend_fixture",
          "visit_Attribute",
          "Function",
          "_py_ext_re",
          "_seen",
          "filename_arg",
          "Exception",
          "name2fixturedefs",
          "_create",
          "CollectErrorRepr",
          "resultlog",
          "get_unpacked_marks",
          "_locationline",
          "pytest_runtest_teardown",
          "_disable_output_capturing_for_darwin",
          "issequence",
          "_get_marks",
          "getini",
          "DOCTEST_REPORT_CHOICE_NONE",
          "isnosetest",
          "make_hook_recorder",
          "DEFAULT_LOG_FORMAT",
          "_addexcinfo",
          "node_reporter",
          "markname",
          "at_level",
          "notset",
          "astcache",
          "__all__",
          "warning_types",
          "_legal_ranges",
          "get_scope_package",
          "param",
          "BINOP_MAP",
          "getbasetemp",
          "Traceback",
          "post_mortem",
          "_compare_eq_verbose",
          "get_scope_node",
          "deactivate_fixture",
          "_in_suspended",
          "_get_active_fixturedef",
          "create_new_paste",
          "CHECKER_CLASS",
          "__getitem__",
          "fix_cache_order",
          "LogXML",
          "AND",
          "linecomp",
          "_format_boolop",
          "getfuncargnames",
          "pytest_make_parametrize_id",
          "str",
          "_T",
          "_ensure_basetemp",
          "add_report_section",
          "_get_teardown_reports",
          "ReprLocals",
          "ihook",
          "approx",
          "Config",
          "_highlight",
          "iter_markers",
          "setupplan",
          "nodes",
          "tracebackcutdir",
          "set_log_path",
          "pytest_keyboard_interrupt",
          "cached_eval",
          "passed",
          "_REPORTCHARS_DEFAULT",
          "map_fixname_class",
          "_warn_already_imported",
          "classnamefilter",
          "USAGE_MSG",
          "MarkEvaluator",
          "__repr__",
          "pytest_runtest_call",
          "getpluginversioninfo",
          "recursionindex",
          "_cachedir",
          "showversion",
          "nodeid",
          "_istrue",
          "collect_one_node",
          "typename",
          "capfdbinary",
          "start",
          "format",
          "DOCTEST_REPORT_CHOICE_NDIFF",
          "function",
          "putaround",
          "mangle_test_address",
          "getcalls",
          "pytest_exception_interact",
          "own_mark_names",
          "evalxfail_key",
          "pytest_addhooks",
          "freeze_support",
          "_update_current_test_var",
          "PrintHelp",
          "PYTEST_TAG",
          "repr_locals",
          "illegal_xml_re",
          "outcomes",
          "FakeCollectModule",
          "_tryconvertpyarg",
          "SysCapture",
          "pytest_runtest_logreport",
          "newinstance",
          "_call_assertion_pass",
          "getargs",
          "_surrounding_parens_on_own_lines",
          "get_default_arg_names",
          "AssertionState",
          "_prepare_content",
          "makedir",
          "set_when",
          "PytestWarning",
          "FixtureFunctionMarker",
          "repr_failure",
          "makeini",
          "_ensure_unconfigure",
          "flow_marker",
          "extraline",
          "Instance",
          "build_summary_stats_line",
          "__module__",
          "cache_dir_from_config",
          "FSHookProxy",
          "PytestUnknownMarkWarning",
          "stop_capturing",
          "_colorama_workaround",
          "resume_fixture",
          "get_lines_after",
          "extract_from",
          "tb",
          "LineMatcher_fixture",
          "__hash__",
          "pytest_collection_modifyitems",
          "spawn",
          "_assertion_supported",
          "_notin_text",
          "parts",
          "_getnextfixturedef",
          "getpathnode",
          "CallInfo",
          "Package",
          "find_spec",
          "pytest_itemcollected",
          "sep",
          "_has_param_ids",
          "_determine_main_color",
          "terminal",
          "pytest_collect_file",
          "OptionGroup",
          "addoption",
          "scopename2class",
          "cleanup_numbered_dir",
          "pytest_collection_finish",
          "isinitpath",
          "_AnyPurePath",
          "Node",
          "consider_env",
          "STRING_TYPES",
          "get_open_files",
          "_typ_map",
          "from_item_and_call",
          "compile_",
          "prog",
          "_recurse",
          "runpython_c",
          "pytest_pyfunc_call",
          "_teardown_with_finalization",
          "_given_basetemp",
          "flush",
          "pytest_warning_captured",
          "__contains__",
          "_resolve_arg_ids",
          "stream",
          "FDCapture",
          "OK",
          "setdefault",
          "countoutcomes",
          "WarningsChecker",
          "nofuncargs",
          "_get_stderr_fileno",
          "_get_allow_bytes_flag",
          "get_timeout_config_value",
          "close",
          "mode",
          "_get_fixturestack",
          "iter_markers_with_node",
          "with_args",
          "MarkMatcher",
          "UsageError",
          "delenv",
          "runpytest_inprocess",
          "_argcomplete",
          "_get_multicapture",
          "File",
          "fail_fixturefunc",
          "not_expr",
          "AssertionRewriter",
          "FaultHandlerHooks",
          "IDENT",
          "__next__",
          "_getglobals",
          "StoreKey",
          "PytestAssertRewriteWarning",
          "check_xfail_no_run",
          "_readline_workaround",
          "initialnames",
          "runtest",
          "cut",
          "mark_rewrite",
          "pytest_sessionfinish",
          "ResultLog",
          "_getreprcrash",
          "DEFAULT_MAX_LINES",
          "_markers",
          "autouse",
          "istestclass",
          "genitems",
          "excinfo",
          "old_mark_config_key",
          "write_ensure_prefix",
          "_for_parametrize",
          "pytest_cmdline_preparse",
          "CaptureResult",
          "addfinalizer",
          "node",
          "markup",
          "_is_last_item",
          "_capturing_for_request",
          "append_error",
          "pytest_unconfigure",
          "TeeCaptureIO",
          "mkdir",
          "_should_truncate_item",
          "logging",
          "_report_keyboardinterrupt",
          "_processopt",
          "DOCTEST_REPORT_CHOICE_CDIFF",
          "capstdout",
          "_issue_warning_captured",
          "resolve_fixture_function",
          "_make_xunit_fixture",
          "_warn_about_missing_assertion",
          "getmodpath",
          "LPAREN",
          "_prunetraceback",
          "pytest_internalerror",
          "_update_message",
          "ischildnode",
          "_should_rewrite",
          "print_teardown_sections",
          "visit_Compare",
          "LsofFdLeakChecker",
          "fail_marker",
          "getgroup",
          "safe_getattr",
          "TEST_OUTCOME",
          "_validate_args",
          "parse",
          "_teardown_towards",
          "pytest",
          "locals",
          "_matchnodes",
          "syspathinsert",
          "lineno",
          "finish",
          "__bool__",
          "_check_scope",
          "_should_repr_global_name",
          "RESULT_LOG",
          "Testdir",
          "PdbInvoke",
          "makeconftest",
          "global_and_fixture_disabled",
          "pytest_assertrepr_compare",
          "args",
          "_config_for_test",
          "copy_example",
          "scope2props",
          "try_makedirs",
          "getfixturedefs",
          "warn",
          "writeorg",
          "_py36_windowsconsoleio_workaround",
          "setitem",
          "findsource",
          "_handle_teardown_sections",
          "pop_outerr_to_orig",
          "parametrize",
          "FuncFixtureInfo",
          "getimfunc",
          "parse_num",
          "Store",
          "_recursive_debug",
          "_compare_eq_cls",
          "MoreQuietAction",
          "_assert_start_repr",
          "traceback",
          "deselect_by_mark",
          "_suspend",
          "_inject_setup_module_fixture",
          "_yield_comparisons",
          "processoption",
          "_main",
          "Source",
          "_getscopeitem",
          "_parse_lsof_output",
          "addError",
          "call_fixture_func",
          "compile",
          "_names",
          "scope2index",
          "get_last_failed_paths",
          "emit",
          "_excinfo",
          "matching_platform",
          "and_expr",
          "accept",
          "_collectfile",
          "_config",
          "repr_instance",
          "path_matches_patterns",
          "_ellipsize",
          "_get_report_choice",
          "_inject_setup_function_fixture",
          "matchreport",
          "doctest",
          "DoctestTextfile",
          "teardown_exact",
          "FixtureLookupErrorRepr",
          "getstatement",
          "get_public_names",
          "set_location",
          "pytest_plugin_registered",
          "python",
          "_compute_fixture_value",
          "_split_lines",
          "_getobj",
          "num_mock_patch_args",
          "PercentStyleMultiline",
          "_check_all_skipped",
          "NODE_USE_FROM_PARENT",
          "_start",
          "runitem",
          "records",
          "get_log_level_for_setting",
          "call_runtest_hook",
          "disabled",
          "_postmortem_traceback",
          "helpconfig",
          "default_plugins",
          "ReprFileLocation",
          "USAGE_ERROR",
          "find_suffixes",
          "clear",
          "_non_numeric_type_error",
          "_get_checker",
          "getcrashentry",
          "REGEX_TYPE",
          "RPAREN",
          "re_match_lines_random",
          "_format_repr_exception",
          "setattr",
          "add_attribute",
          "get_closest_marker",
          "pytest_runtest_setup",
          "eval",
          "finish_recording",
          "DontReadFromInput",
          "report_collect",
          "is_potential_nosetest",
          "getfixturevalue",
          "_find_parametrized_scope",
          "isatty",
          "ReprEntry",
          "_compilecounter",
          "_is_setup_py",
          "pytest_fixture_setup",
          "assertion",
          "pytest_fixture_post_finalizer",
          "count_towards_summary",
          "rm_rf",
          "_callfinalizers",
          "for_config",
          "_early_rewrite_bailout",
          "pytest_load_initial_conftests",
          "_init_runner_class",
          "get_records",
          "buffer",
          "Skipped",
          "Frame",
          "_running_on_ci",
          "call_optional",
          "isdict",
          "_genfunctions",
          "context",
          "_legal_chars",
          "head_line",
          "WarningsRecorder",
          "combined_with",
          "store",
          "Scanner",
          "append_collect_skipped",
          "strip",
          "reports",
          "_makeitem",
          "_set_main_color",
          "MarkGenerator",
          "wrap_session",
          "pytest_runtest_protocol",
          "gethookproxy",
          "deprecated",
          "testdir",
          "pytest_terminal_summary",
          "result",
          "EMPTY_PARAMETERSET_OPTION",
          "names_closure",
          "gethookrecorder",
          "fromdictargs",
          "__slots__",
          "isattrs",
          "pytest_report_header",
          "catch_log_handler_key",
          "pytest_runtestloop",
          "rewrite_asserts",
          "README_CONTENT",
          "builtin",
          "is_rewrite_disabled",
          "finalize",
          "pytest_sessionstart",
          "_assert_expr_to_lineno",
          "get_line_width",
          "id",
          "RE_IMPORT_ERROR_NAME",
          "_ensure_removed_sysmodule",
          "yield_fixture",
          "capsys",
          "_S",
          "parseconfig",
          "LEVELNAME_FMT_REGEX",
          "addSkip",
          "_inject_setup_teardown_fixtures",
          "getfailedcollections",
          "_version",
          "prepare",
          "_getpytestargs",
          "_log_text",
          "TracebackEntry",
          "get_direct_param_fixture_func",
          "_with_exception",
          "CaptureManager",
          "saferepr",
          "parsefactories",
          "get_sections",
          "_log",
          "get_statement_startend2",
          "getoption",
          "getparam",
          "idmaker",
          "_F",
          "makepyfile",
          "spawn_pytest",
          "location",
          "pytestPDB",
          "_PLUGGY_DIR",
          "NoMatch",
          "getreports",
          "assertstate_key",
          "short_test_summary",
          "_ensure_immutable_ids",
          "_param_ids_generated",
          "cached_result",
          "pytest_leave_pdb",
          "set",
          "_W",
          "add_property",
          "ids",
          "truncate_if_required",
          "fslocation",
          "version_tuple",
          "_log_cli_enabled",
          "_get_verbose_word",
          "_done",
          "width_of_current_line",
          "inline_runsource",
          "maybe_delete_a_numbered_dir",
          "_parse_parametrize_parameters",
          "record_testsuite_property",
          "_get_progress_information_message",
          "getsource",
          "conftest_options",
          "fullsource",
          "python_api",
          "_trace",
          "_init_pdb",
          "_color_for_type",
          "rex_outcome",
          "_param_ids_from",
          "addFailure",
          "reportinfo",
          "fixture",
          "_get_pos",
          "_ET",
          "_NodeReporter",
          "patchsysdict",
          "MINUS_K_COLON",
          "DEPRECATED_EXTERNAL_PLUGINS",
          "pastebinfile_key",
          "SysPathsSnapshot",
          "CFG_PYTEST_SECTION",
          "activate_fixture",
          "_fillfixtures",
          "_code",
          "getstatementrange_ast",
          "_testcase",
          "HookRecorder",
          "char_width",
          "async_warn_and_skip",
          "_report_kwargs_from_json",
          "getitem",
          "_width_of_current_line",
          "getfslineno",
          "filter_traceback",
          "__missing__",
          "_expecting_failure",
          "to_xml",
          "teardown_nose",
          "_get_global_properties_node",
          "suspend",
          "parseoutcomes",
          "_makepath",
          "getlocation",
          "_is_marked_for_rewrite",
          "_write_report_lines_from_hooks",
          "_sys_snapshot",
          "Failed",
          "item_capture",
          "_idval",
          "_pytest",
          "determine_setup",
          "_getentrysource",
          "format_explanation",
          "reorder_items",
          "suspend_capturing",
          "_pop_and_teardown",
          "catch_warnings_for_item",
          "pytest_assertion_pass",
          "wasvalid",
          "_perform_collect",
          "expression",
          "add_stats",
          "getcall",
          "getstatementrange",
          "_addoption_instance",
          "_set_opt_strings",
          "_saved",
          "showheader",
          "builtin_plugins",
          "__array_ufunc__",
          "_write_source",
          "_truncate_by_char_count",
          "ParameterSet",
          "_import_plugin_specs",
          "FixtureManager",
          "cache_key",
          "tmpdir",
          "__version__",
          "_E",
          "params",
          "TestReport",
          "Parser",
          "evaluate",
          "TerminalRepr",
          "chain",
          "catching_logs",
          "xfail",
          "addini",
          "getparent",
          "FIXTURE_POSITIONAL_ARGUMENTS",
          "frame",
          "ensure_reset_dir",
          "matchnodes",
          "resultlog_key",
          "CACHEDIR_TAG_CONTENT",
          "iscoroutinefunction",
          "indent",
          "_folded_skips",
          "DOCTEST_REPORT_CHOICES",
          "exitstatus",
          "listchain",
          "write_sep",
          "getslaveinfoline",
          "FastFilesCompleter",
          "source",
          "_validate_if_using_arg_names",
          "raises",
          "evalcache_key",
          "reject",
          "wrap_pytest_function_for_tracing",
          "reorder_items_atscope",
          "Metafunc",
          "reprcrash",
          "main",
          "TYPE_CHECKING",
          "_find_spec",
          "resume_global_capture",
          "_report_unserialization_failure",
          "append",
          "chdir",
          "statement",
          "parseconfigure",
          "fixturenames",
          "catch_log_records_key",
          "_is_doctest",
          "Class",
          "type",
          "warning_record_to_str",
          "OutcomeException",
          "_CACHE_PREFIX_DIRS",
          "message",
          "reset",
          "reprtraceback",
          "summary_warnings",
          "_esctable",
          "format_session_duration",
          "_iter_rewritable_modules",
          "readlines",
          "helper",
          "INTERRUPTED",
          "_resolve_arg_value_types",
          "mktemp",
          "FSCollector",
          "getvalueorskip",
          "_wrapped_pdb_cls",
          "_no_match_line",
          "Cache",
          "delitem",
          "explanation_param",
          "get_lock_path",
          "summary_passes",
          "tmp_path_factory",
          "_initrequest",
          "_setupstate",
          "_get_plugin_specs_as_list",
          "_non_printable_ascii_translate_table",
          "inline_genitems",
          "_format_args",
          "TokenType",
          "NFPlugin",
          "failed",
          "getinicfg",
          "CwdSnapshot",
          "UnformattedWarning",
          "_check_non_top_pytest_plugins",
          "import_plugin",
          "__ne__",
          "FDCaptureBinary",
          "makefile",
          "_getconftestmodules",
          "LFPlugin",
          "_approx_scalar",
          "ApproxNumpy",
          "_getini",
          "_mark_plugins_for_rewrite",
          "hookspec",
          "PYC_TAIL",
          "_bytes_to_ascii",
          "__dir__",
          "tmp_path",
          "FixtureRequest",
          "capstderr",
          "relline",
          "cls",
          "PYC_EXT",
          "parse_known_args",
          "from_config",
          "fullwidth",
          "CollectError",
          "DEFAULT_LOG_DATE_FORMAT",
          "normalize_mark_list",
          "OR",
          "getfailures",
          "_write_entry_lines",
          "make_numbered_dir",
          "pytest_collectreport",
          "NodeMeta",
          "addExpectedFailure",
          "essential_plugins",
          "add_global_property",
          "_match_lines",
          "_remove_ansi_escape_sequences",
          "LogCaptureHandler",
          "_try_load_conftest",
          "_get_override_ini_value",
          "_is_numpy_array",
          "_inject_setup_class_fixture",
          "isset",
          "_is_capturing",
          "Module",
          "exconly",
          "longrepr",
          "_format_assertmsg",
          "names",
          "repr_excinfo",
          "_idvalset",
          "_PluggyPlugin",
          "__attrs_post_init__",
          "PytestArg",
          "DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE",
          "register_cleanup_lock_removal",
          "__take_sys_modules_snapshot",
          "collect",
          "importorskip",
          "ExitCode",
          "NoCapture",
          "visit_Assert",
          "PytestUnhandledCoroutineWarning",
          "LogCaptureFixture",
          "get_exconly",
          "HelpAction",
          "repr",
          "_addoption",
          "_format_action_invocation",
          "addSuccess",
          "restore",
          "_rewrite_test",
          "_get_auto_indent",
          "parse_setoption",
          "unexpectedsuccess_key",
          "compat",
          "ishidden",
          "find_prefixed",
          "create_module",
          "SafeRepr",
          "run",
          "MarkDecorator",
          "SetupState",
          "try_cleanup",
          "pytester",
          "JUNIT_XML_DEFAULT_FAMILY",
          "add_marker",
          "register_assert_rewrite",
          "_truncate_recursive_traceback",
          "consider_conftest",
          "_ALLOW_MARKERS",
          "stop",
          "_getindent",
          "showfixtures",
          "_get_direct_parametrize_args",
          "getplugin",
          "funcargs",
          "Exit",
          "read",
          "template",
          "summary_stats",
          "TerminalWriter",
          "_assertion_pass",
          "addsection",
          "AlwaysDispatchingPrettyPrinter",
          "RUNNER_CLASS",
          "messages",
          "_recursive_list_map",
          "_prepareconfig",
          "_get_flag_lookup",
          "repr_traceback_entry",
          "add_cleanup",
          "_ascii_escaped_by_config",
          "FUNCARGNAMES",
          "showlocals",
          "_printcollecteditems",
          "LineMatcher",
          "stop_global_capturing",
          "scopeproperty",
          "pytest_runtest_logfinish",
          "collect_by_name",
          "PytestConfigWarning",
          "_truncate_explanation",
          "PYTESTER_COPY_EXAMPLE",
          "NO_TESTS_COLLECTED",
          "encoding",
          "ensure_newline",
          "exec_module",
          "COLLECT_FAKEMODULE_ATTRIBUTES",
          "__init__",
          "lines",
          "pytest_cmdline_main",
          "start_global_capturing",
          "ApproxBase",
          "fillfixtures",
          "ColoredLevelFormatter",
          "exit",
          "read_global_capture",
          "co_equal",
          "_importconftest",
          "ParseError",
          "_compare_eq_set",
          "_tmppath_factory",
          "teardown",
          "warnings",
          "showlongtestinfo",
          "_validate_usepdb_cls",
          "_getfailureheadline",
          "_read_pyc",
          "value",
          "getexplanation",
          "set_trace",
          "write",
          "_parse_filter",
          "handler",
          "PyCollector",
          "_show_fixture_action",
          "PyobjMixin",
          "_compare_eq_sequence",
          "no_re_match_line",
          "repr_args",
          "_ANSI_ESCAPE_SEQ",
          "check_interactive_exception",
          "DEFAULT_RELATIVE_TOLERANCE",
          "pytest_cmdline_parse",
          "get_dirs_from_args",
          "_get_allow_unicode_flag",
          "ApproxSequencelike",
          "_node_location_to_relpath",
          "_diff_text",
          "assert_contains",
          "cmdline",
          "istext",
          "_from_json",
          "SubRequest",
          "TERMINALWRITER_WRITER",
          "get_parametrized_fixture_keys",
          "hookimpl",
          "BaseReport",
          "ExceptionRepr",
          "getnode",
          "install_importhook",
          "__exit__",
          "pytest_deselected",
          "_split_explanation",
          "PYTEST_COLLECT_MODULE",
          "fnmatch_lines_random",
          "_consider_importhook",
          "MINUS_K_DASH",
          "MultipleDoctestFailures",
          "__version_tuple__",
          "runtestprotocol",
          "caplog",
          "Token",
          "FILLFUNCARGS",
          "reprentries",
          "Mark",
          "assert_outcomes",
          "ConftestImportFailure",
          "argnames",
          "get_real_func",
          "check_strict_xfail",
          "_dump_lines",
          "show_fixtures_per_test",
          "done",
          "ApproxDecimal",
          "pytest_collect_directory",
          "_repr_failure_py",
          "from_current",
          "_setup_fixtures",
          "path",
          "verbosity",
          "errisinstance",
          "get_empty_parameterset_mark",
          "deprecated_call",
          "istestfunction",
          "_matches_prefix_or_glob_option",
          "pytest_report_teststatus",
          "RunResult",
          "listextrakeywords",
          "isparseable",
          "NOT",
          "_checkversion",
          "_resume",
          "for_later",
          "DOCTEST_REPORT_CHOICE_UDIFF",
          "consider_module",
          "fixtures",
          "from_item",
          "write_line",
          "_get_runner",
          "_get_pdb_wrapper_class",
          "when",
          "tbfilter",
          "CaptureFixture",
          "consider_pluginarg",
          "_fixturemanager",
          "getcfg",
          "_get_continue_on_failure",
          "invocation_dir",
          "is_generator",
          "append_skipped",
          "delattr",
          "getitems",
          "_LiveLoggingNullHandler",
          "lex",
          "funcargnames",
          "_translate_non_printable",
          "scopemismatch",
          "_splitnode",
          "invalidraise",
          "funcnamefilter",
          "syspath_prepend",
          "EMPTY_BUFFER",
          "_reprcompare",
          "MODULE_NOT_FOUND_ERROR",
          "readline",
          "repr_traceback",
          "exists",
          "pytest_pycollect_makemodule",
          "raiseerror",
          "_add_simple",
          "__test__",
          "visit_Name",
          "pytest_report_from_serializable",
          "truncate_locals",
          "nextitem",
          "update_testcase_duration",
          "notify_exception",
          "_eval_scope_callable",
          "_strtobool",
          "formatrepr",
          "_compare_eq_dict",
          "pytest_collectstart",
          "setuponly",
          "Item",
          "_striptext",
          "_PY_DIR",
          "from_exc_info",
          "is_globally_capturing",
          "_visit_filter",
          "ReprFailDoctest",
          "tmpdir_factory",
          "_warn_incompatibility_with_xunit2",
          "_getautousenames",
          "StepwisePlugin",
          "dir",
          "freeze_includes",
          "kwargs",
          "CollectReport",
          "listnames",
          "_result",
          "pytest_enter_pdb",
          "_pyfuncitem",
          "name",
          "junitxml",
          "getrawcode",
          "reprlocals",
          "_teardown_yield_fixture",
          "fill_unfilled",
          "rewrite",
          "MyOptionParser",
          "_LiveLoggingStreamHandler",
          "list",
          "applymarker",
          "_mk_tmp",
          "version",
          "_factorytraceback",
          "UnitTestCase",
          "create_terminal_writer",
          "fnmatch_lines",
          "hasplugin",
          "scope",
          "SEP",
          "_get_number_flag",
          "writer",
          "re_match_lines",
          "_report_to_json",
          "runner",
          "_basetemp",
          "append_collect_error",
          "is_capturing",
          "LOCK_TIMEOUT",
          "tolerance",
          "assign",
          "style",
          "_do_configure",
          "showhelp",
          "_create_formatter",
          "_getcrashline",
          "snap",
          "TempdirFactory",
          "PytestCollectionWarning",
          "skip",
          "_check_if_assertion_pass_impl",
          "_preparse",
          "make_properties_node",
          "hasnew",
          "istrue",
          "get",
          "resume_capturing",
          "maybe_wrap_pytest_function_for_tracing",
          "__delitem__",
          "entrysep",
          "reprfuncargs",
          "get_fslocation_from_item",
          "section",
          "_match_lines_random",
          "_fail",
          "ReprTraceback",
          "deindent",
          "inline_run",
          "bin_xml_escape",
          "_schedule_finalizers",
          "_bestrelpath_cache",
          "is_true",
          "hasopt",
          "LFPluginCollSkipfiles",
          "UNARY_MAP",
          "make_numbered_dir_with_cleanup",
          "pytest_collection",
          "EncodedFile",
          "MARK_GEN",
          "unittest",
          "Argument",
          "hasinit",
          "getfixtureinfo",
          "FormattedExcinfo",
          "_init_checker_class",
          "get_terminal_writer",
          "maketxtfile",
          "resolve",
          "out",
          "on_rm_rf_error",
          "addinivalue_line",
          "TestCaseFunction",
          "DEFAULT_ABSOLUTE_TOLERANCE",
          "_write_progress_information_filling_space",
          "ReprTracebackNative",
          "_saferepr",
          "PytestDeprecationWarning",
          "BASE_TYPE",
          "_getparser",
          "_PytestWrapper",
          "_showfixtures_main",
          "_color_for_type_default",
          "_get_main_color",
          "PdbTrace",
          "pop",
          "set_session",
          "get_option_ini",
          "scopenum_function",
          "_get_assertion_exprs",
          "_opentestcase",
          "no_fnmatch_line",
          "line",
          "_call_reprcompare",
          "register",
          "recwarn",
          "AssertionRewritingHook",
          "istestfunc",
          "popcall",
          "_ensure_supporting_files",
          "_getlines",
          "visit_Call",
          "mark",
          "DummyRewriteHook",
          "pytest_report_to_serializable",
          "show_test_item",
          "execute",
          "INTERNAL_ERROR",
          "_check_type",
          "write_fspath_result",
          "REPORT_COLLECTING_RESOLUTION",
          "rex_session_duration",
          "capture",
          "_force_symlink",
          "undo",
          "config",
          "_exec_lsof",
          "mkpydir",
          "from_parent",
          "ReprExceptionInfo",
          "get_location",
          "__eq__",
          "_to_json",
          "push_format_context",
          "suspend_global_capture",
          "xml_key",
          "TeeSysCapture",
          "_make_plural",
          "obj",
          "IGNORE_PAM",
          "ensure_deletable",
          "ExceptionInfo",
          "stopTest",
          "_matchfactories",
          "get_source",
          "popen",
          "T",
          "Notset",
          "fail",
          "debugging",
          "TempPathFactory",
          "_determine_show_progress_info",
          "capfd",
          "skipped_by_mark_key",
          "_repr_style",
          "_get_increasing_order",
          "families",
          "_traceback",
          "expr",
          "record_property",
          "getmodulecol",
          "_check_initialpaths_for_relpath",
          "visit_BinOp",
          "safe_isclass",
          "_write_content",
          "__len__",
          "append_pass",
          "SysCaptureBinary",
          "attrs",
          "_pluginmanager",
          "getrepr",
          "runpytest",
          "setenv",
          "pytest_make_collect_report",
          "__setitem__",
          "__enter__",
          "set_level",
          "_patch_unwrap_mock_aware",
          "ReprEntryNative",
          "record_tuples",
          "store_mark",
          "COLLECT_DIRECTORY_HOOK",
          "_runtest_for",
          "_format_lines",
          "_check_record_param_type",
          "filter",
          "NOTSET",
          "addUnexpectedSuccess",
          "Interrupted",
          "__iter__",
          "cleanup_candidates",
          "TESTS_FAILED",
          "LoggingPlugin",
          "_validate_ids",
          "write_docstring",
          "isiterable",
          "_in_venv",
          "visit_UnaryOp",
          "PytestPluginManager",
          "ParsedCall",
          "get_terminal_width",
          "startTest",
          "visit_Starred",
          "MultiCapture",
          "ReprFuncArgs",
          "pytestconfig",
          "InvocationParams",
          "ascii_escaped",
          "KeywordMatcher",
          "merge_family",
          "__str__",
          "set_repr_style",
          "append_failure",
          "LineComp",
          "KNOWN_TYPES",
          "readouterr",
          "parse_hookspec_opts",
          "get_data",
          "reprfileloc",
          "pytest_configure",
          "_import_pdb_cls",
          "pytest_runtest_makereport",
          "display",
          "getfirstlinesource",
          "parse_args",
          "monkeypatch",
          "_CACHE_PREFIX_VALUES",
          "DEFAULT_MAX_CHARS",
          "assertoutcome",
          "_is_mocked",
          "fileno",
          "_legal_xml_re",
          "_checkargnotcontained",
          "parse_hookimpl_opts",
          "start_capturing",
          "_getconftest_pathlist",
          "_rget_with_confmod",
          "pytest_generate_tests",
          "NodeKeywords",
          "create_cleanup_lock",
          "summary_errors",
          "plugins",
          "variable",
          "_enter_pdb",
          "_outrep_summary",
          "pytest_report_collectionfinish",
          "fspath",
          "FILE_OR_DIR",
          "_WithException",
          "clear_cache",
          "_importtestmodule",
          "consider_preparse",
          "FixtureDef",
          "TerminalReporter",
          "_plugin_nameversions",
          "longreprtext",
          "FunctionDefinition",
          "PseudoFixtureDef",
          "capsysbinary",
          "console_main",
          "pastebin",
          "runpython",
          "_iter_all_modules",
          "DoctestItem",
          "pos",
          "_finalize",
          "getvalue",
          "resume",
          "ArgumentError",
          "getfixturemarker",
          "_call_with_optional_argument",
          "parse_known_and_unknown_args",
          "CaptureIO",
          "get_cache_dir",
          "exec_",
          "record_testreport",
          "_add_stats",
          "fnmatch_ex",
          "cwd_relative_nodeid",
          "extract_suffixes",
          "TimeoutExpired",
          "should_do_markup",
          "MonkeyPatch",
          "match",
          "_set_initial_conftests",
          "setmulti2",
          "_collect",
          "_io",
          "cacheprovider",
          "from_call",
          "write_captured_output",
          "get_common_ancestor",
          "PytestCacheWarning",
          "get_user",
          "record_xml_attribute",
          "pop_format_context",
          "CLOSE_STDIN",
          "error",
          "pathlib",
          "skipped",
          "_addfinalizer",
          "LFPluginCollWrapper",
          "ExceptionChainRepr",
          "directory_arg",
          "runpytest_subprocess",
          "CallSpec2",
          "log_outcome",
          "pytest_addoption",
          "_ensure_relative_to_basetemp",
          "safeformat",
          "nose",
          "sections",
          "copy",
          "_parsearg",
          "pytest_ignore_collect",
          "_format",
          "EOF",
          "Session",
          "pytest_pycollect_makeitem",
          "prune_dependency_tree",
          "cache",
          "Code",
          "Junit",
          "session",
          "check_testcase_implements_trial_reporter",
          "module",
          "assert_contains_lines",
          "generic_visit",
          "fault_handler_stderr_key",
          "derive_importpath",
          "perform_collect",
          "setup",
          "annotated_getattr",
          "get_plugin_manager",
          "ApproxMapping",
          "_PYTEST_DIR",
          "__array_priority__",
          "_pformat_dispatch",
          "wrap_function_to_error_out_if_called_directly",
          "get_config",
          "__call__",
          "instance",
          "get_optionflags",
          "pytest_doctest_prepare_content",
          "visit_BoolOp",
          "duration",
          "simple",
          "getfixtureclosure",
          "skipping",
          "getreportopt",
          "write_log_entry",
          "Collector",
          "add_funcarg_pseudo_fixture_def",
          "_state",
          "isdatacls",
          "doctest_namespace",
          "ApproxScalar",
          "cacheshow",
          "is_async_function",
          "_get_line_with_reprcrash_message",
          "resolve_from_str",
          "WarningReport",
          "summary_failures",
          "__getattr__",
          "_gethookproxy",
          "call_and_report",
          "showfspath",
          "abspath",
          "RaisesContext",
          "_initini",
          "stepwise",
          "MatcherAdapter",
          "_makefile",
          "toterminal",
          "_getvaluepath",
          "warns",
          "SysModulesSnapshot",
          "teardown_all",
          "get_real_method",
          "faulthandler",
          "DropShorterLongHelpFormatter",
          "LOGLEVEL_COLOROPTS",
          "_parse_parametrize_args",
          "listoutcomes",
          "exprinfo",
          "text",
          "D",
          "_try_repr_or_str",
          "DoctestModule",
          "_write_pyc_fp",
          "XFailed",
          "Expression",
          "category",
          "_inject_setup_method_fixture",
          "PytestExperimentalApiWarning",
          "FixtureLookupError",
          "pytest_runtest_logstart",
          "keywords",
          "assertrepr_compare",
          "scopes",
          "deselect_by_keyword",
          "_get_first_non_fixture_func",
          "_compare_eq_iterable",
          "_show_fixtures_per_test"
        ]
      },
      "response": {
        "keywords": [
          "pytest_runtest_teardown",
          "pytest_exception_interact",
          "pytest_runtest_makereport",
          "Function",
          "Node",
          "Session",
          "TestReport",
          "pytest_reporting",
          "pytest_teardown",
          "pytest_pdb"
        ]
      }
    },
    {
      "name": "FilesKeywordSearch",
      "metadata": {
        "start": "2024-08-17T22:37:48.747880",
        "usage": "172.14608192443848 ms"
      },
      "request": {
        "tokens": [
          "pytest_runtest_teardown",
          "pytest_exception_interact",
          "pytest_runtest_makereport",
          "Function",
          "Node",
          "Session",
          "TestReport",
          "pytest_reporting",
          "pytest_teardown",
          "pytest_pdb"
        ]
      },
      "response": {
        "results": {
          "pytest_runtest_makereport": [
            "src/_pytest/unittest.py",
            "src/_pytest/runner.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/skipping.py"
          ],
          "Session": [
            "src/pytest/__init__.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/nodes.py",
            "src/_pytest/cacheprovider.py",
            "src/pytest/collect.py",
            "src/_pytest/python.py",
            "src/_pytest/terminal.py",
            "src/_pytest/main.py",
            "src/_pytest/warnings.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/pytester.py",
            "src/_pytest/hookspec.py"
          ],
          "TestReport": [
            "src/_pytest/runner.py",
            "src/_pytest/terminal.py",
            "src/_pytest/pytester.py",
            "src/_pytest/reports.py",
            "src/_pytest/hookspec.py"
          ],
          "Function": [
            "src/pytest/__init__.py",
            "src/pytest/collect.py",
            "src/_pytest/python.py",
            "src/_pytest/nose.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/unittest.py",
            "src/_pytest/_argcomplete.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/debugging.py"
          ],
          "Node": [
            "src/_pytest/nodes.py",
            "src/_pytest/runner.py",
            "src/_pytest/python.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/reports.py",
            "src/_pytest/logging.py",
            "src/_pytest/hookspec.py"
          ],
          "pytest_runtest_teardown": [
            "src/_pytest/runner.py",
            "src/_pytest/capture.py",
            "src/_pytest/logging.py",
            "src/_pytest/hookspec.py"
          ],
          "pytest_exception_interact": [
            "src/_pytest/hookspec.py",
            "src/_pytest/runner.py",
            "src/_pytest/debugging.py",
            "src/_pytest/faulthandler.py"
          ]
        }
      }
    },
    {
      "name": "FilesFilter",
      "metadata": {
        "start": "2024-08-17T22:37:48.920034",
        "usage": "5393.410205841064 ms"
      },
      "request": {
        "keyword_search_results": {
          "pytest_runtest_makereport": [
            "src/_pytest/unittest.py",
            "src/_pytest/runner.py",
            "src/_pytest/hookspec.py",
            "src/_pytest/skipping.py"
          ],
          "Session": [
            "src/pytest/__init__.py",
            "src/_pytest/assertion/__init__.py",
            "src/_pytest/nodes.py",
            "src/_pytest/cacheprovider.py",
            "src/pytest/collect.py",
            "src/_pytest/python.py",
            "src/_pytest/terminal.py",
            "src/_pytest/main.py",
            "src/_pytest/warnings.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/pytester.py",
            "src/_pytest/hookspec.py"
          ],
          "TestReport": [
            "src/_pytest/runner.py",
            "src/_pytest/terminal.py",
            "src/_pytest/pytester.py",
            "src/_pytest/reports.py",
            "src/_pytest/hookspec.py"
          ],
          "Function": [
            "src/pytest/__init__.py",
            "src/pytest/collect.py",
            "src/_pytest/python.py",
            "src/_pytest/nose.py",
            "src/_pytest/mark/__init__.py",
            "src/_pytest/unittest.py",
            "src/_pytest/_argcomplete.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/debugging.py"
          ],
          "Node": [
            "src/_pytest/nodes.py",
            "src/_pytest/runner.py",
            "src/_pytest/python.py",
            "src/_pytest/fixtures.py",
            "src/_pytest/reports.py",
            "src/_pytest/logging.py",
            "src/_pytest/hookspec.py"
          ],
          "pytest_runtest_teardown": [
            "src/_pytest/runner.py",
            "src/_pytest/capture.py",
            "src/_pytest/logging.py",
            "src/_pytest/hookspec.py"
          ],
          "pytest_exception_interact": [
            "src/_pytest/hookspec.py",
            "src/_pytest/runner.py",
            "src/_pytest/debugging.py",
            "src/_pytest/faulthandler.py"
          ]
        }
      },
      "response": {
        "scratch_pad": "The reported issue highlights a problem where the tearDown method is executed on skipped tests when using the --pdb option in pytest version 5.4.2.\n\nKey components of this problem:\n- `tearDown` method: This is called after a test method has been run, and the issue is that it's being executed even when a test is marked as skipped.\n- `--pdb` option: This causes pytest to run in post-mortem debugging mode when an error is encountered.\n- Between pytest versions 5.4.1 and 5.4.2, there is a change that affects this behavior.\n\nIn the `keyword_search_results`, looking for relevant files:\n\n1. **src/_pytest/runner.py**:\n   - This file appears in both `pytest_runtest_makereport` and `pytest_runtest_teardown`. I suspect it holds significant logic about running and tearing down tests, which might be modified to change the behavior as described by the issue.\n\n2. **src/_pytest/unittest.py**:\n   - This file appears in `pytest_runtest_makereport` and is directly related to unittest integration, which is the framework used in the example.\n\n3. **src/_pytest/debugging.py**:\n   - This file appears for `pytest_exception_interact`. It may involve handling the interaction with the debugger (--pdb), possibly affecting why the tearDown is invoked.\n   \n4. **src/_pytest/skipping.py**:\n  - This file is related to the skipping mechanism, which might have been altered in version 5.4.2 to incorrectly handle --pdb cases.\n\nThese files are likely to contain the changes between the two versions (5.4.1 and 5.4.2) that resulted in this issue.",
        "file_paths": [
          "src/_pytest/runner.py",
          "src/_pytest/unittest.py",
          "src/_pytest/debugging.py",
          "src/_pytest/skipping.py"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-17T22:37:54.313485",
        "usage": "6845.907926559448 ms"
      },
      "request": {
        "file_path": "src/_pytest/runner.py"
      },
      "response": {
        "scratch_pad": "1. The issue reported is that in pytest version 5.4.2, when using `--pdb`, the `tearDown` method is executed even for skipped tests which was not the case in version 5.4.1. \n\n2. The execution of `tearDown` during skipped tests with `--pdb` seems to be related to the newer version of pytest and the behavior change may root from how pytest interprets or interacts with the unittest framework in conjunction with pdb.\n\n3. The file `src/_pytest/runner.py` implements the core runtest protocol and related functionalities for pytest.\n\n4. Observations from the file content:\n   - The method `runtestprotocol()` orchestrates the execution of setup, call (test), and teardown phases.\n   - The methods `pytest_runtest_setup()`, `pytest_runtest_call()`, and `pytest_runtest_teardown()` demarcate these phases and are invoked respectively.\n   - `call_and_report()` manages hook calls and error handling, and also checks and triggers interactive debugging when `--pdb` is activated.\n   - The function `check_interactive_exception()` skips interactive mode when exceptions are of type `Skipped` or `BdbQuit`, indicating a consideration for skipped tests already exists. \n\n5. Considering that `call_runtest_hook(item, when)` in `runtestprotocol` unconditionally attempts teardown even if a test is skipped, it might be relevant.\n6. Given the execution style and handling of interactive exception, the teardown shouldn't be called for skipped tests with `pdb`. Thus, the behavior change might be due to conditional checking or handling within `runtestprotocol` or within how calls/errors are managed with pdb.\n7. Further analysis should encompass the call-order affected by PDB settings and how Skipped exceptions are being bypassed or processed differently leading to teardown execution.\n\n8. It is likely that adjustments are required in how `runtest_protocol` or `call_and_report` control teardown for pdb/skipped scenarios.\n\n9. Sections and symbols primarily relevant include `runtestprotocol`, `call_and_report`, `check_interactive_exception`. These are key to managing the test flow and any interactive debugging engagement.",
        "relevant": true,
        "root_cause": true,
        "relevant_files": [],
        "relevant_symbols": [
          "runtestprotocol",
          "call_and_report",
          "pytest_runtest_teardown",
          "check_interactive_exception"
        ],
        "root_cause_symbols": [
          "runtestprotocol",
          "call_and_report"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-17T22:38:01.159408",
        "usage": "9065.035820007324 ms"
      },
      "request": {
        "file_path": "src/_pytest/unittest.py"
      },
      "response": {
        "scratch_pad": "1. **Understanding the User Issue:**\n   - The user reports an issue where the `tearDown` method is executed on skipped tests when running tests with the `--pdb` option in pytest version 5.4.2, while it was not executed in version 5.4.1. \n   - The test should be skipped due to the unittest's `@unittest.skip` decorator, but the presence of `--pdb`, which is used for debugging, is affecting this behavior by still invoking the `tearDown`.\n   - The expectation is that skipped tests should not execute the `tearDown` method even when `--pdb` is used.\n\n2. **Analyzing the File Content:**\n   - The file `src/_pytest/unittest.py` seems to be related to the integration of unittest with pytest.\n   - Within the `TestCaseFunction` class, the `runtest()` method is critical as it sets `self._explicit_tearDown` to the actual `tearDown` method, effectively deferring its calling when `--pdb` is enabled.\n   - The code uses `setattr(self._testcase, \"tearDown\", lambda *args: None)` when `self.config.getoption(\"usepdb\")` is true, suggesting a deliberate step to avoid calling `tearDown` prematurely during debugging.\n\n3. **Matching the Problematic Behavior with Code:**\n   - `_make_xunit_fixture` is injecting fixtures that manage `setUp` and `tearDown`, which could relate to why `tearDown` is called.\n   - In `runtest()` of `TestCaseFunction`, an explicit defer of `tearDown` is done when `--pdb` is used to prevent premature cleanup by `tearDown`. However, during skipped tests, this still seems to fall through, possibly due to `tearDown` fixtures being postponed rather than skipped altogether.\n   - `pytest_runtest_makereport` may further relate by substituting exception info with `pytest.skip`, which should correctly account for skipping.\n\n4. **Determination of Relevance and Root Cause:**\n   - This file explains the unittest test running logic including the use of `--pdb`.\n   - Relevant to potential causes of `tearDown` being triggered since controlling execution timing of `setUp`/`tearDown` is crucial here.\n\n5. **Consequence and Recommendations:**\n   - This file is highly relevant to the issue because it exhibits control over the test lifecycle including `tearDown`.\n   - Fix could involve ensuring that skip conditions also bypass fixture invocations entirely, especially during `--pdb` execution.\n\nThe symbols and classes relevant to the root cause have been identified based on this analysis.",
        "relevant": true,
        "root_cause": true,
        "relevant_files": [],
        "relevant_symbols": [
          "UnitTestCase.collect",
          "_make_xunit_fixture",
          "TestCaseFunction.runtest",
          "pytest_runtest_makereport"
        ],
        "root_cause_symbols": [
          "TestCaseFunction.runtest",
          "_make_xunit_fixture"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-17T22:38:10.224461",
        "usage": "8003.630876541138 ms"
      },
      "request": {
        "file_path": "src/_pytest/debugging.py"
      },
      "response": {
        "scratch_pad": "1. **Understanding the Issue**:\n   - The issue reported revolves around `unittest.TestCase.tearDown` being executed on skipped tests when using `--pdb` with pytest.\n   - The expectation from the user is that for skipped tests, `tearDown` should not be executed, even when `--pdb` is active.\n   - The user mentions that this behavior differs from previous version 5.4.1, where the test was skipped, and `tearDown` was not executed.\n\n2. **Possible Cause**:\n   - The execution of `tearDown` could be due to changes in how pytest with `--pdb` handles exceptions or test results, possibly ignoring the skip status.\n\n3. **Analyzing the Code**:\n   - The file `src/_pytest/debugging.py` contains functions related to handling the `--pdb` option, such as `set_trace`, `_enter_pdb`, and `post_mortem`.\n   - The `pytest_exception_interact` function appears to handle exceptions and may invoke PDB when errors occur.\n   - There is potential involvement with capturing mechanisms which might interfere or cause `tearDown` to execute due to pdb's post-mortem or interactive debugging features.\n\n4. **Mapping the Symbols**:\n   - `pytest_exception_interact`: This function is relevant because it is directly involved with interacting via PDB upon exceptions, which is central to the issue.\n   - `_enter_pdb`: Because entering into PDB possibly involves `tearDown` executing due to unresolved or mishandled skip logic.\n\n5. **Root Cause**:\n   - The root cause appears related to how `--pdb` modifies the normal execution flow of pytest, and the logic when handling skips may not be correctly implemented.\n   - It seems likely that interactions in `_enter_pdb` or `pytest_exception_interact` functions may not account for skipped tests correctly when `--pdb` is active.",
        "relevant": true,
        "root_cause": true,
        "relevant_files": [],
        "relevant_symbols": [
          "pytest_exception_interact",
          "_enter_pdb"
        ],
        "root_cause_symbols": [
          "pytest_exception_interact",
          "_enter_pdb"
        ]
      }
    },
    {
      "name": "FileEvaluate",
      "metadata": {
        "start": "2024-08-17T22:38:18.228101",
        "usage": "12392.606019973755 ms"
      },
      "request": {
        "file_path": "src/_pytest/skipping.py"
      },
      "response": {
        "scratch_pad": "1. Analyze User Instructions:\n1.1 The user reports an issue where the `tearDown()` method in a `unittest.TestCase` is executed for tests that are marked as skipped when using the `--pdb` option. Normally, when a test is skipped, setUp and tearDown methods should not be executed.\n1.2 The reproduction steps provided involve creating a test case that is always skipped using the `@unittest.skip` decorator and running the test suite with `--pdb`. The problem occurs when `--pdb` is used, as it leads to the execution of tearDown, which should not happen if the test is skipped.\n1.3 It seems the issue was caused by changes between pytest versions 5.4.1 and 5.4.2. Specifically, pytest might not be correctly handling the skip marker in conjunction with the `--pdb` flag.\n1.4 The user expects the test to be completely skipped, even with the `--pdb` option, as it was in the previous version of pytest (5.4.1). This expectation is not met in the current version.\n1.5 Possible solutions could include:\n   - Reviewing how pytest interacts with unittest's skip marker in the presence of the `--pdb` flag to ensure that the control flow does not reach tearDown.\n   - Investigating any changes made to the pytest skipping logic or pdb handling between versions 5.4.1 and 5.4.2 that could have led to this behavior.\n\n2. Make file evaluation:\n2.1 The file is `src/_pytest/skipping.py`, which deals with skipping-related functionality in pytest.\n2.2 Initial relevant symbols include:\n   - `pytest_runtest_setup(item)`: Sets up the test, determining whether to skip.\n   - `skipped_by_mark_key`: Used to identify if an item is skipped by a marker.\n   - `check_xfail_no_run(item)`: Handles xfail(no_run=True) which might relate to skipping logic.\n2.3 The logic in `pytest_runtest_setup` pipeline evaluates and sets skips based on markers, but it doesn't directly handle pdb interaction.\n2.4 The `pytest_runtest_makereport` hook might influence the behavior since it's responsible for assigning test outcomes.\n2.5 There are no apparent direct interactions with pdb within this file. However, the place to start looking would be where the skipping and reporting are managed.\n2.6 This file manages how tests are skipped and reported in pytest, but it is unclear if it is directly responsible for the pdb-related issue without further context on pdb management logic within pytest.\n\nDecision:\n- Mark this file as relevant, since it handles skipping logic which is crucial to the reported behavior.\n- This file may not be the root cause because we don't see interaction with pdb here. Further files related to test execution and pdb might need evaluation.",
        "relevant": true,
        "root_cause": false,
        "relevant_files": [
          "src/_pytest/debugging.py",
          "src/_pytest/runner.py"
        ],
        "relevant_symbols": [
          "pytest_addoption",
          "pytest_configure",
          "pytest_runtest_setup",
          "pytest_runtest_makereport",
          "pytest_runtest_makereport"
        ],
        "root_cause_symbols": []
      }
    },
    {
      "name": "FileSymbolSearch",
      "metadata": {
        "start": "2024-08-17T22:38:30.620727",
        "usage": "175.2340793609619 ms"
      },
      "request": {
        "files": [
          {
            "file_path": "src/_pytest/runner.py",
            "relevant_symbol_keys": [
              "runtestprotocol",
              "call_and_report",
              "pytest_runtest_teardown",
              "check_interactive_exception"
            ],
            "root_cause_symbol_keys": [
              "runtestprotocol",
              "call_and_report"
            ]
          },
          {
            "file_path": "src/_pytest/unittest.py",
            "relevant_symbol_keys": [
              "UnitTestCase.collect",
              "_make_xunit_fixture",
              "TestCaseFunction.runtest",
              "pytest_runtest_makereport"
            ],
            "root_cause_symbol_keys": [
              "TestCaseFunction.runtest",
              "_make_xunit_fixture"
            ]
          },
          {
            "file_path": "src/_pytest/debugging.py",
            "relevant_symbol_keys": [
              "pytest_exception_interact",
              "_enter_pdb"
            ],
            "root_cause_symbol_keys": [
              "pytest_exception_interact",
              "_enter_pdb"
            ]
          },
          {
            "file_path": "src/_pytest/skipping.py",
            "relevant_symbol_keys": [
              "pytest_addoption",
              "pytest_configure",
              "pytest_runtest_setup",
              "pytest_runtest_makereport",
              "pytest_runtest_makereport"
            ],
            "root_cause_symbol_keys": []
          }
        ],
        "with_content": true
      },
      "response": {
        "related_symbols": [
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "    duration=duration,\n            when=when,\n            result=result,\n            excinfo=excinfo,\n        )\n\n    def __repr__(self):\n        if",
            "start_line": 255,
            "start_column": 8576,
            "end_line": 4,
            "end_column": 8723,
            "start_byte": 8576,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "raised an error\n        for col in self.stack:\n            if hasattr(col, \"_prepare_exc\"):\n                exc = col._prepare_exc\n                raise exc\n        for col in needed_collectors[len(self.stack) :]:\n            self.stack.append(col)\n            try:\n                col.setup()\n            except TEST_OUTCOME as e:\n                col._prepare_exc = e\n                raise e\n\n\ndef ",
            "start_line": 386,
            "start_column": 12830,
            "end_line": 0,
            "end_column": 13229,
            "start_byte": 12830,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "",
            "start_line": 662,
            "start_column": 23491,
            "end_line": 4,
            "end_column": 23770,
            "start_byte": 23491,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def pytest_runtest_teardown(item, nextitem):\n    _update_current_test_var(item, \"teardown\")\n    item.session._setupstate.teardown_exact(item, nextitem)\n    _update_current_test_var(item, None)",
            "start_line": 150,
            "start_column": 4467,
            "end_line": 0,
            "end_column": 4659,
            "start_byte": 4467,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.check_interactive_exception",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def check_interactive_exception(call, report):\n    return call.excinfo and not (\n        hasattr(report, \"wasxfail\")\n        or call.excinfo.errisinstance(Skipped)\n        or call.excinfo.errisinstance(bdb.BdbQuit)\n    )",
            "start_line": 200,
            "start_column": 5979,
            "end_line": 0,
            "end_column": 6199,
            "start_byte": 5979,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "",
            "start_line": 396,
            "start_column": 13232,
            "end_line": 0,
            "end_column": 13535,
            "start_byte": 13232,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "try:\n                del call.result\n            except AttributeError:\n                pass\n\n ",
            "start_line": 284,
            "start_column": 8918,
            "end_line": 0,
            "end_column": 9013,
            "start_byte": 8918,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "     self._explicit_tearDown()\n            self._explicit_tearDown = None\n        self._testcase = None\n        self._obj = None\n\n    def startTest(self, testcase):\n        pass\n\n    def _addexcinfo(self, rawexcinfo):\n        # unwrap potential exception info (see twisted trial support below)\n        rawexcinfo = getattr(rawexcinfo, \"_rawexcinfo\", rawexcinfo)\n        try:\n            excinfo = _pytest._code.ExceptionInfo(rawexcinfo)\n            # invoke the attributes to trigger storing the traceback\n            # trial causes some issue there\n            excinfo.value\n            excinfo.traceback\n        except TypeError:\n            try:\n                try:\n                    values = traceback.format_exception(*rawexcinfo)\n                    values.insert(\n                        0,\n                        \"NOTE: Incompatible Exception Representation, \"\n                        \"displaying natively:\\n\\n\",\n                    )\n                    fail(\"\".join(values), pytrace=False)\n                except (fail.Exception, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    fail(\n                        \"ERROR: Unknown Incompatible Exception \"\n                        \"representation:\\n%r\" % (rawexcinfo,),\n                        pytrace=False,\n                    )\n            except KeyboardInterrupt:\n                raise\n            except fail.Exception:\n                excinfo = _pytest._code.ExceptionInfo.from_current()\n        self.__dict__.setdefault(\"_excinfo\", []).append(excinfo)\n\n    def addError(self, testcase, rawexcinfo):\n        try:\n            if isinstance(rawexcinfo[1], exit.Exception):\n                exit(rawexcinfo[1].msg)\n        except TypeError:\n            pass\n        self._addexcinfo(rawexcinfo)\n\n    def addFailure(self, testcase, rawexcinfo):\n        self._addexcinfo(rawexcinfo)\n\n    def addSkip(self, testcase, reason):\n        try:\n            skip(reason)\n        except skip.Exception:\n            self._store[skipped_by_mark_k",
            "start_line": 125,
            "start_column": 4192,
            "end_line": 0,
            "end_column": 6240,
            "start_byte": 4192,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.UnitTestCase.collect",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def collect(self):\n        from unittest import TestLoader\n\n        cls = self.obj\n        if not getattr(cls, \"__test__\", True):\n            return\n\n        skipped = getattr(cls, \"__unittest_skip__\", False)\n        if not skipped:\n            self._inject_setup_teardown_fixtures(cls)\n            self._inject_setup_class_fixture()\n\n        self.session._fixturemanager.parsefactories(self, unittest=True)\n        loader = TestLoader()\n        foundsomething = False\n        for name in loader.getTestCaseNames(self.obj):\n            x = getattr(self.obj, name)\n            if not getattr(x, \"__test__\", True):\n                continue\n            funcobj = getimfunc(x)\n            yield TestCaseFunction.from_parent(self, name=name, callobj=funcobj)\n            foundsomething = True\n\n        if not foundsomething:\n            runtest = getattr(self.obj, \"runTest\", None)\n            if runtest is not None:\n                ut = sys.modules.get(\"twisted.trial.unittest\", None)\n                if ut is None or runtest != ut.TestCase.runTest:\n                    # TODO: callobj consistency\n                    yield TestCaseFunction.from_parent(self, name=\"runTest\")",
            "start_line": 36,
            "start_column": 1112,
            "end_line": 4,
            "end_column": 2283,
            "start_byte": 1112,
            "end_byte": 4
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_makereport(item, call):\n    if isinstance(item, TestCaseFunction):\n        if item._excinfo:\n            call.excinfo = item._excinfo.pop(0)\n            try:\n                del call.result\n            except AttributeError:\n                pass\n\n    unittest = sys.modules.get(\"unittest\")\n    if unittest and call.excinfo and call.excinfo.errisinstance(unittest.SkipTest):\n        # let's substitute the excinfo with a pytest.skip one\n        call2 = CallInfo.from_call(\n            lambda: pytest.skip(str(call.excinfo.value)), call.when\n        )\n        call.excinfo = call2.excinfo",
            "start_line": 243,
            "start_column": 8721,
            "end_line": 0,
            "end_column": 9351,
            "start_byte": 8721,
            "end_byte": 0
          },
          {
            "name": "_pytest.assertion.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "  \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n       ",
            "start_line": 20,
            "start_column": 497,
            "end_line": 0,
            "end_column": 1273,
            "start_byte": 497,
            "end_byte": 0
          },
          {
            "name": "_pytest.cacheprovider.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 384,
            "start_column": 14196,
            "end_line": 0,
            "end_column": 15963,
            "start_byte": 14196,
            "end_byte": 0
          },
          {
            "name": "_pytest.cacheprovider.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 449,
            "start_column": 16127,
            "end_line": 0,
            "end_column": 16376,
            "start_byte": 16127,
            "end_byte": 0
          },
          {
            "name": "_pytest.capture.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "e,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambd",
            "start_line": 28,
            "start_column": 587,
            "end_line": 0,
            "end_column": 1052,
            "start_byte": 587,
            "end_byte": 0
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 245,
            "start_column": 8284,
            "end_line": 4,
            "end_column": 8425,
            "start_byte": 8284,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.PytestPluginManager.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 409,
            "start_column": 12624,
            "end_line": 4,
            "end_column": 13256,
            "start_byte": 12624,
            "end_byte": 4
          },
          {
            "name": "_pytest.debugging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addiniva",
            "start_line": 21,
            "start_column": 539,
            "end_line": 0,
            "end_column": 1237,
            "start_byte": 539,
            "end_byte": 0
          },
          {
            "name": "_pytest.debugging.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(conditio",
            "start_line": 45,
            "start_column": 1240,
            "end_line": 0,
            "end_column": 2049,
            "start_byte": 1240,
            "end_byte": 0
          },
          {
            "name": "_pytest.doctest.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "  \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" ",
            "start_line": 54,
            "start_column": 1486,
            "end_line": 0,
            "end_column": 2904,
            "start_byte": 1486,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "es import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsucce",
            "start_line": 12,
            "start_column": 157,
            "end_line": 0,
            "end_column": 374,
            "start_byte": 157,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test f",
            "start_line": 20,
            "start_column": 377,
            "end_line": 0,
            "end_column": 1315,
            "start_byte": 377,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "s\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win3",
            "start_line": 48,
            "start_column": 1495,
            "end_line": 4,
            "end_column": 1768,
            "start_byte": 1495,
            "end_byte": 4
          },
          {
            "name": "_pytest.fixtures.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 1202,
            "start_column": 45360,
            "end_line": 0,
            "end_column": 45548,
            "start_byte": 45360,
            "end_byte": 0
          },
          {
            "name": "_pytest.helpconfig.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_set",
            "start_line": 38,
            "start_column": 1168,
            "end_line": 0,
            "end_column": 2499,
            "start_byte": 1168,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            s",
            "start_line": 50,
            "start_column": 1425,
            "end_line": 0,
            "end_column": 3012,
            "start_byte": 1425,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n ",
            "start_line": 88,
            "start_column": 3015,
            "end_line": 0,
            "end_column": 3498,
            "start_byte": 3015,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 378,
            "start_column": 12657,
            "end_line": 0,
            "end_column": 12745,
            "start_byte": 12657,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 396,
            "start_column": 13232,
            "end_line": 0,
            "end_column": 13535,
            "start_byte": 13232,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 365,
            "start_column": 12037,
            "end_line": 0,
            "end_column": 13376,
            "start_byte": 12037,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 410,
            "start_column": 13379,
            "end_line": 0,
            "end_column": 14153,
            "start_byte": 13379,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "report.skipped:\n            return \"xfailed\", \"x\", \"XFAIL\"\n        elif report.passed:\n            return \"xpassed\", \"X\", \"XPASS\"\n",
            "start_line": 182,
            "start_column": 6378,
            "end_line": 0,
            "end_column": 8992,
            "start_byte": 6378,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 471,
            "start_column": 16133,
            "end_line": 0,
            "end_column": 16268,
            "start_byte": 16133,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 649,
            "start_column": 23082,
            "end_line": 4,
            "end_column": 23306,
            "start_byte": 23082,
            "end_byte": 4
          },
          {
            "name": "_pytest.main.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and eval",
            "start_line": 40,
            "start_column": 989,
            "end_line": 0,
            "end_column": 5037,
            "start_byte": 989,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yi",
            "start_line": 53,
            "start_column": 1694,
            "end_line": 0,
            "end_column": 3297,
            "start_byte": 1694,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 250,
            "start_column": 7889,
            "end_line": 0,
            "end_column": 8340,
            "start_byte": 7889,
            "end_byte": 0
          },
          {
            "name": "_pytest.pastebin.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ort xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"run",
            "start_line": 12,
            "start_column": 231,
            "end_line": 0,
            "end_column": 559,
            "start_byte": 231,
            "end_byte": 0
          },
          {
            "name": "_pytest.pastebin.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "il\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(rea",
            "start_line": 25,
            "start_column": 562,
            "end_line": 0,
            "end_column": 1376,
            "start_byte": 562,
            "end_byte": 0
          },
          {
            "name": "_pytest.pytester.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "eason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n       ",
            "start_line": 55,
            "start_column": 1374,
            "end_line": 0,
            "end_column": 1985,
            "start_byte": 1374,
            "end_byte": 0
          },
          {
            "name": "_pytest.pytester.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"",
            "start_line": 80,
            "start_column": 1988,
            "end_line": 0,
            "end_column": 2364,
            "start_byte": 1988,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ine(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n      ",
            "start_line": 59,
            "start_column": 1875,
            "end_line": 0,
            "end_column": 3550,
            "start_byte": 1875,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skippe",
            "start_line": 129,
            "start_column": 4062,
            "end_line": 0,
            "end_column": 4962,
            "start_byte": 4062,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "n.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_lin",
            "start_line": 34,
            "start_column": 933,
            "end_line": 0,
            "end_column": 1244,
            "start_byte": 933,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "alxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_s",
            "start_line": 125,
            "start_column": 3822,
            "end_line": 0,
            "end_column": 3940,
            "start_byte": 3822,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 284,
            "start_column": 8918,
            "end_line": 0,
            "end_column": 9013,
            "start_byte": 8918,
            "end_byte": 0
          },
          {
            "name": "_pytest.setuponly.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgrou",
            "start_line": 4,
            "start_column": 58,
            "end_line": 0,
            "end_column": 457,
            "start_byte": 58,
            "end_byte": 0
          },
          {
            "name": "_pytest.setupplan.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark",
            "start_line": 3,
            "start_column": 16,
            "end_line": 0,
            "end_column": 293,
            "start_byte": 16,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
            "start_line": 14,
            "start_column": 401,
            "end_line": 0,
            "end_column": 881,
            "start_byte": 401,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )",
            "start_line": 33,
            "start_column": 884,
            "end_line": 0,
            "end_column": 2449,
            "start_byte": 884,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 73,
            "start_column": 2452,
            "end_line": 0,
            "end_column": 3179,
            "start_byte": 2452,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 125,
            "start_column": 4192,
            "end_line": 0,
            "end_column": 6240,
            "start_byte": 4192,
            "end_byte": 0
          },
          {
            "name": "_pytest.stepwise.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group",
            "start_line": 3,
            "start_column": 16,
            "end_line": 0,
            "end_column": 479,
            "start_byte": 16,
            "end_byte": 0
          },
          {
            "name": "_pytest.stepwise.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "doption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"",
            "start_line": 20,
            "start_column": 482,
            "end_line": 0,
            "end_column": 604,
            "start_byte": 482,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n",
            "start_line": 78,
            "start_column": 1896,
            "end_line": 0,
            "end_column": 4639,
            "start_byte": 1896,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "     else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        ",
            "start_line": 173,
            "start_column": 4642,
            "end_line": 0,
            "end_column": 5061,
            "start_byte": 4642,
            "end_byte": 0
          },
          {
            "name": "_pytest.tmpdir.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif cal",
            "start_line": 137,
            "start_column": 4708,
            "end_line": 0,
            "end_column": 5308,
            "start_byte": 4708,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 243,
            "start_column": 8721,
            "end_line": 0,
            "end_column": 9351,
            "start_byte": 8721,
            "end_byte": 0
          },
          {
            "name": "_pytest.warnings.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"ma",
            "start_line": 51,
            "start_column": 1531,
            "end_line": 0,
            "end_column": 1989,
            "start_byte": 1531,
            "end_byte": 0
          },
          {
            "name": "_pytest.warnings.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific except",
            "start_line": 68,
            "start_column": 1992,
            "end_line": 0,
            "end_column": 2242,
            "start_byte": 1992,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "r\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n      ",
            "start_line": 6,
            "start_column": 136,
            "end_line": 0,
            "end_column": 548,
            "start_byte": 136,
            "end_byte": 0
          },
          {
            "name": "_pytest.resultlog.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"-",
            "start_line": 13,
            "start_column": 185,
            "end_line": 0,
            "end_column": 501,
            "start_byte": 185,
            "end_byte": 0
          },
          {
            "name": "_pytest.resultlog.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "nxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(",
            "start_line": 25,
            "start_column": 504,
            "end_line": 0,
            "end_column": 1195,
            "start_byte": 504,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.runner.runtestprotocol",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def runtestprotocol(item, log=True, nextitem=None):\n    hasrequest = hasattr(item, \"_request\")\n    if hasrequest and not item._request:\n        item._initrequest()\n    rep = call_and_report(item, \"setup\", log)\n    reports = [rep]\n    if rep.passed:\n        if item.config.getoption(\"setupshow\", False):\n            show_test_item(item)\n        if not item.config.getoption(\"setuponly\", False):\n            reports.append(call_and_report(item, \"call\", log))\n    reports.append(call_and_report(item, \"teardown\", log, nextitem=nextitem))\n    # after all teardown hooks have been called\n    # want funcargs and request info to go away\n    if hasrequest:\n        item._request = False\n        item.funcargs = None\n    return reports",
            "start_line": 93,
            "start_column": 2710,
            "end_line": 0,
            "end_column": 3437,
            "start_byte": 2710,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.call_and_report",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def call_and_report(\n    item, when: \"Literal['setup', 'call', 'teardown']\", log=True, **kwds\n):\n    call = call_runtest_hook(item, when, **kwds)\n    hook = item.ihook\n    report = hook.pytest_runtest_makereport(item=item, call=call)\n    if log:\n        hook.pytest_runtest_logreport(report=report)\n    if check_interactive_exception(call, report):\n        hook.pytest_exception_interact(node=item, call=call, report=report)\n    return report",
            "start_line": 187,
            "start_column": 5534,
            "end_line": 0,
            "end_column": 5976,
            "start_byte": 5534,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest._make_xunit_fixture",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def _make_xunit_fixture(obj, setup_name, teardown_name, scope, pass_self):\n    setup = getattr(obj, setup_name, None)\n    teardown = getattr(obj, teardown_name, None)\n    if setup is None and teardown is None:\n        return None\n\n    @pytest.fixture(scope=scope, autouse=True)\n    def fixture(self, request):\n        if getattr(self, \"__unittest_skip__\", None):\n            reason = self.__unittest_skip_why__\n            pytest.skip(reason)\n        if setup is not None:\n            if pass_self:\n                setup(self, request.function)\n            else:\n                setup()\n        yield\n        if teardown is not None:\n            if pass_self:\n                teardown(self, request.function)\n            else:\n                teardown()\n\n    return fixture",
            "start_line": 83,
            "start_column": 2910,
            "end_line": 0,
            "end_column": 3683,
            "start_byte": 2910,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.TestCaseFunction.runtest",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def runtest(self):\n        from _pytest.debugging import maybe_wrap_pytest_function_for_tracing\n\n        maybe_wrap_pytest_function_for_tracing(self)\n\n        # let the unittest framework handle async functions\n        if is_async_function(self.obj):\n            self._testcase(self)\n        else:\n            # when --pdb is given, we want to postpone calling tearDown() otherwise\n            # when entering the pdb prompt, tearDown() would have probably cleaned up\n            # instance variables, which makes it difficult to debug\n            # arguably we could always postpone tearDown(), but this changes the moment where the\n            # TestCase instance interacts with the results object, so better to only do it\n            # when absolutely needed\n            if self.config.getoption(\"usepdb\"):\n                self._explicit_tearDown = self._testcase.tearDown\n                setattr(self._testcase, \"tearDown\", lambda *args: None)\n\n            # we need to update the actual bound method with self.obj, because\n            # wrap_pytest_function_for_tracing replaces self.obj by a wrapper\n            setattr(self._testcase, self.name, self.obj)\n            try:\n                self._testcase(result=self)\n            finally:\n                delattr(self._testcase, self.name)",
            "start_line": 207,
            "start_column": 7152,
            "end_line": 4,
            "end_column": 8447,
            "start_byte": 7152,
            "end_byte": 4
          },
          {
            "name": "_pytest.debugging.PdbInvoke.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "def pytest_exception_interact(self, node, call, report):\n        capman = node.config.pluginmanager.getplugin(\"capturemanager\")\n        if capman:\n            capman.suspend_global_capture(in_=True)\n            out, err = capman.read_global_capture()\n            sys.stdout.write(out)\n            sys.stdout.write(err)\n        _enter_pdb(node, call.excinfo, report)",
            "start_line": 257,
            "start_column": 8431,
            "end_line": 4,
            "end_column": 8796,
            "start_byte": 8431,
            "end_byte": 4
          },
          {
            "name": "_pytest.debugging._enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "def _enter_pdb(node, excinfo, rep):\n    # XXX we re-use the TerminalReporter's terminalwriter\n    # because this seems to avoid some encoding related troubles\n    # for not completely clear reasons.\n    tw = node.config.pluginmanager.getplugin(\"terminalreporter\")._tw\n    tw.line()\n\n    showcapture = node.config.option.showcapture\n\n    for sectionname, content in (\n        (\"stdout\", rep.capstdout),\n        (\"stderr\", rep.capstderr),\n        (\"log\", rep.caplog),\n    ):\n        if showcapture in (sectionname, \"all\") and content:\n            tw.sep(\">\", \"captured \" + sectionname)\n            if content[-1:] == \"\\n\":\n                content = content[:-1]\n            tw.line(content)\n\n    tw.sep(\">\", \"traceback\")\n    rep.toterminal(tw)\n    tw.sep(\">\", \"entering PDB\")\n    tb = _postmortem_traceback(excinfo)\n    rep._pdbshown = True\n    post_mortem(tb)\n    return rep",
            "start_line": 304,
            "start_column": 10146,
            "end_line": 0,
            "end_column": 11019,
            "start_byte": 10146,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "           import pdb\n\n            pdb_cls = pdb.Pdb\n\n        wrapped_cls = cls._get_pdb_wrapper_class(pdb_cls, capman)\n        cls._wrapped_pdb_cls = (usepdb_cls, wrapped_cls)\n        return wrapped_cls\n\n    @classmethod\n    def _",
            "start_line": 96,
            "start_column": 3425,
            "end_line": 4,
            "end_column": 3656,
            "start_byte": 3425,
            "end_byte": 4
          },
          {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "b_wrapper_class(cls, pdb_cls, capman):\n        import _pytest.config\n\n        class PytestPdbWrapper(pdb_cls):\n            _pytest_capman = capman\n            _continued = False\n\n            def do_debug(self, arg):\n                cls._recursive_debug += 1\n",
            "start_line": 104,
            "start_column": 3662,
            "end_line": 4,
            "end_column": 3920,
            "start_byte": 3662,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 22873,
            "end_line": 0,
            "end_column": 23146,
            "start_byte": 22873,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "",
            "start_line": 684,
            "start_column": 23149,
            "end_line": 0,
            "end_column": 23438,
            "start_byte": 23149,
            "end_byte": 0
          }
        ]
      }
    },
    {
      "name": "SymbolsEvaluate",
      "metadata": {
        "start": "2024-08-17T22:38:30.795968",
        "usage": "7237.873792648315 ms"
      },
      "request": {
        "related_symbols": [
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "    duration=duration,\n            when=when,\n            result=result,\n            excinfo=excinfo,\n        )\n\n    def __repr__(self):\n        if",
            "start_line": 255,
            "start_column": 8576,
            "end_line": 4,
            "end_column": 8723,
            "start_byte": 8576,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "raised an error\n        for col in self.stack:\n            if hasattr(col, \"_prepare_exc\"):\n                exc = col._prepare_exc\n                raise exc\n        for col in needed_collectors[len(self.stack) :]:\n            self.stack.append(col)\n            try:\n                col.setup()\n            except TEST_OUTCOME as e:\n                col._prepare_exc = e\n                raise e\n\n\ndef ",
            "start_line": 386,
            "start_column": 12830,
            "end_line": 0,
            "end_column": 13229,
            "start_byte": 12830,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "",
            "start_line": 662,
            "start_column": 23491,
            "end_line": 4,
            "end_column": 23770,
            "start_byte": 23491,
            "end_byte": 4
          },
          {
            "name": "_pytest.runner.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def pytest_runtest_teardown(item, nextitem):\n    _update_current_test_var(item, \"teardown\")\n    item.session._setupstate.teardown_exact(item, nextitem)\n    _update_current_test_var(item, None)",
            "start_line": 150,
            "start_column": 4467,
            "end_line": 0,
            "end_column": 4659,
            "start_byte": 4467,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.check_interactive_exception",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def check_interactive_exception(call, report):\n    return call.excinfo and not (\n        hasattr(report, \"wasxfail\")\n        or call.excinfo.errisinstance(Skipped)\n        or call.excinfo.errisinstance(bdb.BdbQuit)\n    )",
            "start_line": 200,
            "start_column": 5979,
            "end_line": 0,
            "end_column": 6199,
            "start_byte": 5979,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "",
            "start_line": 396,
            "start_column": 13232,
            "end_line": 0,
            "end_column": 13535,
            "start_byte": 13232,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "try:\n                del call.result\n            except AttributeError:\n                pass\n\n ",
            "start_line": 284,
            "start_column": 8918,
            "end_line": 0,
            "end_column": 9013,
            "start_byte": 8918,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "     self._explicit_tearDown()\n            self._explicit_tearDown = None\n        self._testcase = None\n        self._obj = None\n\n    def startTest(self, testcase):\n        pass\n\n    def _addexcinfo(self, rawexcinfo):\n        # unwrap potential exception info (see twisted trial support below)\n        rawexcinfo = getattr(rawexcinfo, \"_rawexcinfo\", rawexcinfo)\n        try:\n            excinfo = _pytest._code.ExceptionInfo(rawexcinfo)\n            # invoke the attributes to trigger storing the traceback\n            # trial causes some issue there\n            excinfo.value\n            excinfo.traceback\n        except TypeError:\n            try:\n                try:\n                    values = traceback.format_exception(*rawexcinfo)\n                    values.insert(\n                        0,\n                        \"NOTE: Incompatible Exception Representation, \"\n                        \"displaying natively:\\n\\n\",\n                    )\n                    fail(\"\".join(values), pytrace=False)\n                except (fail.Exception, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    fail(\n                        \"ERROR: Unknown Incompatible Exception \"\n                        \"representation:\\n%r\" % (rawexcinfo,),\n                        pytrace=False,\n                    )\n            except KeyboardInterrupt:\n                raise\n            except fail.Exception:\n                excinfo = _pytest._code.ExceptionInfo.from_current()\n        self.__dict__.setdefault(\"_excinfo\", []).append(excinfo)\n\n    def addError(self, testcase, rawexcinfo):\n        try:\n            if isinstance(rawexcinfo[1], exit.Exception):\n                exit(rawexcinfo[1].msg)\n        except TypeError:\n            pass\n        self._addexcinfo(rawexcinfo)\n\n    def addFailure(self, testcase, rawexcinfo):\n        self._addexcinfo(rawexcinfo)\n\n    def addSkip(self, testcase, reason):\n        try:\n            skip(reason)\n        except skip.Exception:\n            self._store[skipped_by_mark_k",
            "start_line": 125,
            "start_column": 4192,
            "end_line": 0,
            "end_column": 6240,
            "start_byte": 4192,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.UnitTestCase.collect",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def collect(self):\n        from unittest import TestLoader\n\n        cls = self.obj\n        if not getattr(cls, \"__test__\", True):\n            return\n\n        skipped = getattr(cls, \"__unittest_skip__\", False)\n        if not skipped:\n            self._inject_setup_teardown_fixtures(cls)\n            self._inject_setup_class_fixture()\n\n        self.session._fixturemanager.parsefactories(self, unittest=True)\n        loader = TestLoader()\n        foundsomething = False\n        for name in loader.getTestCaseNames(self.obj):\n            x = getattr(self.obj, name)\n            if not getattr(x, \"__test__\", True):\n                continue\n            funcobj = getimfunc(x)\n            yield TestCaseFunction.from_parent(self, name=name, callobj=funcobj)\n            foundsomething = True\n\n        if not foundsomething:\n            runtest = getattr(self.obj, \"runTest\", None)\n            if runtest is not None:\n                ut = sys.modules.get(\"twisted.trial.unittest\", None)\n                if ut is None or runtest != ut.TestCase.runTest:\n                    # TODO: callobj consistency\n                    yield TestCaseFunction.from_parent(self, name=\"runTest\")",
            "start_line": 36,
            "start_column": 1112,
            "end_line": 4,
            "end_column": 2283,
            "start_byte": 1112,
            "end_byte": 4
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_makereport(item, call):\n    if isinstance(item, TestCaseFunction):\n        if item._excinfo:\n            call.excinfo = item._excinfo.pop(0)\n            try:\n                del call.result\n            except AttributeError:\n                pass\n\n    unittest = sys.modules.get(\"unittest\")\n    if unittest and call.excinfo and call.excinfo.errisinstance(unittest.SkipTest):\n        # let's substitute the excinfo with a pytest.skip one\n        call2 = CallInfo.from_call(\n            lambda: pytest.skip(str(call.excinfo.value)), call.when\n        )\n        call.excinfo = call2.excinfo",
            "start_line": 243,
            "start_column": 8721,
            "end_line": 0,
            "end_column": 9351,
            "start_byte": 8721,
            "end_byte": 0
          },
          {
            "name": "_pytest.assertion.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "  \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n       ",
            "start_line": 20,
            "start_column": 497,
            "end_line": 0,
            "end_column": 1273,
            "start_byte": 497,
            "end_byte": 0
          },
          {
            "name": "_pytest.cacheprovider.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 384,
            "start_column": 14196,
            "end_line": 0,
            "end_column": 15963,
            "start_byte": 14196,
            "end_byte": 0
          },
          {
            "name": "_pytest.cacheprovider.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 449,
            "start_column": 16127,
            "end_line": 0,
            "end_column": 16376,
            "start_byte": 16127,
            "end_byte": 0
          },
          {
            "name": "_pytest.capture.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "e,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambd",
            "start_line": 28,
            "start_column": 587,
            "end_line": 0,
            "end_column": 1052,
            "start_byte": 587,
            "end_byte": 0
          },
          {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 245,
            "start_column": 8284,
            "end_line": 4,
            "end_column": 8425,
            "start_byte": 8284,
            "end_byte": 4
          },
          {
            "name": "_pytest.config.PytestPluginManager.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 409,
            "start_column": 12624,
            "end_line": 4,
            "end_column": 13256,
            "start_byte": 12624,
            "end_byte": 4
          },
          {
            "name": "_pytest.debugging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addiniva",
            "start_line": 21,
            "start_column": 539,
            "end_line": 0,
            "end_column": 1237,
            "start_byte": 539,
            "end_byte": 0
          },
          {
            "name": "_pytest.debugging.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(conditio",
            "start_line": 45,
            "start_column": 1240,
            "end_line": 0,
            "end_column": 2049,
            "start_byte": 1240,
            "end_byte": 0
          },
          {
            "name": "_pytest.doctest.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "  \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" ",
            "start_line": 54,
            "start_column": 1486,
            "end_line": 0,
            "end_column": 2904,
            "start_byte": 1486,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "es import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsucce",
            "start_line": 12,
            "start_column": 157,
            "end_line": 0,
            "end_column": 374,
            "start_byte": 157,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test f",
            "start_line": 20,
            "start_column": 377,
            "end_line": 0,
            "end_column": 1315,
            "start_byte": 377,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "s\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win3",
            "start_line": 48,
            "start_column": 1495,
            "end_line": 4,
            "end_column": 1768,
            "start_byte": 1495,
            "end_byte": 4
          },
          {
            "name": "_pytest.fixtures.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 1202,
            "start_column": 45360,
            "end_line": 0,
            "end_column": 45548,
            "start_byte": 45360,
            "end_byte": 0
          },
          {
            "name": "_pytest.helpconfig.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_set",
            "start_line": 38,
            "start_column": 1168,
            "end_line": 0,
            "end_column": 2499,
            "start_byte": 1168,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            s",
            "start_line": 50,
            "start_column": 1425,
            "end_line": 0,
            "end_column": 3012,
            "start_byte": 1425,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n ",
            "start_line": 88,
            "start_column": 3015,
            "end_line": 0,
            "end_column": 3498,
            "start_byte": 3015,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 378,
            "start_column": 12657,
            "end_line": 0,
            "end_column": 12745,
            "start_byte": 12657,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 396,
            "start_column": 13232,
            "end_line": 0,
            "end_column": 13535,
            "start_byte": 13232,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 365,
            "start_column": 12037,
            "end_line": 0,
            "end_column": 13376,
            "start_byte": 12037,
            "end_byte": 0
          },
          {
            "name": "_pytest.junitxml.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 410,
            "start_column": 13379,
            "end_line": 0,
            "end_column": 14153,
            "start_byte": 13379,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "report.skipped:\n            return \"xfailed\", \"x\", \"XFAIL\"\n        elif report.passed:\n            return \"xpassed\", \"X\", \"XPASS\"\n",
            "start_line": 182,
            "start_column": 6378,
            "end_line": 0,
            "end_column": 8992,
            "start_byte": 6378,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 471,
            "start_column": 16133,
            "end_line": 0,
            "end_column": 16268,
            "start_byte": 16133,
            "end_byte": 0
          },
          {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 649,
            "start_column": 23082,
            "end_line": 4,
            "end_column": 23306,
            "start_byte": 23082,
            "end_byte": 4
          },
          {
            "name": "_pytest.main.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and eval",
            "start_line": 40,
            "start_column": 989,
            "end_line": 0,
            "end_column": 5037,
            "start_byte": 989,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yi",
            "start_line": 53,
            "start_column": 1694,
            "end_line": 0,
            "end_column": 3297,
            "start_byte": 1694,
            "end_byte": 0
          },
          {
            "name": "_pytest.mark.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 250,
            "start_column": 7889,
            "end_line": 0,
            "end_column": 8340,
            "start_byte": 7889,
            "end_byte": 0
          },
          {
            "name": "_pytest.pastebin.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ort xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"run",
            "start_line": 12,
            "start_column": 231,
            "end_line": 0,
            "end_column": 559,
            "start_byte": 231,
            "end_byte": 0
          },
          {
            "name": "_pytest.pastebin.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "il\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(rea",
            "start_line": 25,
            "start_column": 562,
            "end_line": 0,
            "end_column": 1376,
            "start_byte": 562,
            "end_byte": 0
          },
          {
            "name": "_pytest.pytester.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "eason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n       ",
            "start_line": 55,
            "start_column": 1374,
            "end_line": 0,
            "end_column": 1985,
            "start_byte": 1374,
            "end_byte": 0
          },
          {
            "name": "_pytest.pytester.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"",
            "start_line": 80,
            "start_column": 1988,
            "end_line": 0,
            "end_column": 2364,
            "start_byte": 1988,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ine(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n      ",
            "start_line": 59,
            "start_column": 1875,
            "end_line": 0,
            "end_column": 3550,
            "start_byte": 1875,
            "end_byte": 0
          },
          {
            "name": "_pytest.python.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skippe",
            "start_line": 129,
            "start_column": 4062,
            "end_line": 0,
            "end_column": 4962,
            "start_byte": 4062,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "n.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_lin",
            "start_line": 34,
            "start_column": 933,
            "end_line": 0,
            "end_column": 1244,
            "start_byte": 933,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "alxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_s",
            "start_line": 125,
            "start_column": 3822,
            "end_line": 0,
            "end_column": 3940,
            "start_byte": 3822,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 284,
            "start_column": 8918,
            "end_line": 0,
            "end_column": 9013,
            "start_byte": 8918,
            "end_byte": 0
          },
          {
            "name": "_pytest.setuponly.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgrou",
            "start_line": 4,
            "start_column": 58,
            "end_line": 0,
            "end_column": 457,
            "start_byte": 58,
            "end_byte": 0
          },
          {
            "name": "_pytest.setupplan.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark",
            "start_line": 3,
            "start_column": 16,
            "end_line": 0,
            "end_column": 293,
            "start_byte": 16,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
            "start_line": 14,
            "start_column": 401,
            "end_line": 0,
            "end_column": 881,
            "start_byte": 401,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )",
            "start_line": 33,
            "start_column": 884,
            "end_line": 0,
            "end_column": 2449,
            "start_byte": 884,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 73,
            "start_column": 2452,
            "end_line": 0,
            "end_column": 3179,
            "start_byte": 2452,
            "end_byte": 0
          },
          {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 125,
            "start_column": 4192,
            "end_line": 0,
            "end_column": 6240,
            "start_byte": 4192,
            "end_byte": 0
          },
          {
            "name": "_pytest.stepwise.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group",
            "start_line": 3,
            "start_column": 16,
            "end_line": 0,
            "end_column": 479,
            "start_byte": 16,
            "end_byte": 0
          },
          {
            "name": "_pytest.stepwise.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "doption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"",
            "start_line": 20,
            "start_column": 482,
            "end_line": 0,
            "end_column": 604,
            "start_byte": 482,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n",
            "start_line": 78,
            "start_column": 1896,
            "end_line": 0,
            "end_column": 4639,
            "start_byte": 1896,
            "end_byte": 0
          },
          {
            "name": "_pytest.terminal.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "     else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        ",
            "start_line": 173,
            "start_column": 4642,
            "end_line": 0,
            "end_column": 5061,
            "start_byte": 4642,
            "end_byte": 0
          },
          {
            "name": "_pytest.tmpdir.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif cal",
            "start_line": 137,
            "start_column": 4708,
            "end_line": 0,
            "end_column": 5308,
            "start_byte": 4708,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 243,
            "start_column": 8721,
            "end_line": 0,
            "end_column": 9351,
            "start_byte": 8721,
            "end_byte": 0
          },
          {
            "name": "_pytest.warnings.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"ma",
            "start_line": 51,
            "start_column": 1531,
            "end_line": 0,
            "end_column": 1989,
            "start_byte": 1531,
            "end_byte": 0
          },
          {
            "name": "_pytest.warnings.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific except",
            "start_line": 68,
            "start_column": 1992,
            "end_line": 0,
            "end_column": 2242,
            "start_byte": 1992,
            "end_byte": 0
          },
          {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "r\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n      ",
            "start_line": 6,
            "start_column": 136,
            "end_line": 0,
            "end_column": 548,
            "start_byte": 136,
            "end_byte": 0
          },
          {
            "name": "_pytest.resultlog.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"-",
            "start_line": 13,
            "start_column": 185,
            "end_line": 0,
            "end_column": 501,
            "start_byte": 185,
            "end_byte": 0
          },
          {
            "name": "_pytest.resultlog.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "nxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(",
            "start_line": 25,
            "start_column": 504,
            "end_line": 0,
            "end_column": 1195,
            "start_byte": 504,
            "end_byte": 0
          }
        ],
        "root_caused_symbols": [
          {
            "name": "_pytest.runner.runtestprotocol",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def runtestprotocol(item, log=True, nextitem=None):\n    hasrequest = hasattr(item, \"_request\")\n    if hasrequest and not item._request:\n        item._initrequest()\n    rep = call_and_report(item, \"setup\", log)\n    reports = [rep]\n    if rep.passed:\n        if item.config.getoption(\"setupshow\", False):\n            show_test_item(item)\n        if not item.config.getoption(\"setuponly\", False):\n            reports.append(call_and_report(item, \"call\", log))\n    reports.append(call_and_report(item, \"teardown\", log, nextitem=nextitem))\n    # after all teardown hooks have been called\n    # want funcargs and request info to go away\n    if hasrequest:\n        item._request = False\n        item.funcargs = None\n    return reports",
            "start_line": 93,
            "start_column": 2710,
            "end_line": 0,
            "end_column": 3437,
            "start_byte": 2710,
            "end_byte": 0
          },
          {
            "name": "_pytest.runner.call_and_report",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def call_and_report(\n    item, when: \"Literal['setup', 'call', 'teardown']\", log=True, **kwds\n):\n    call = call_runtest_hook(item, when, **kwds)\n    hook = item.ihook\n    report = hook.pytest_runtest_makereport(item=item, call=call)\n    if log:\n        hook.pytest_runtest_logreport(report=report)\n    if check_interactive_exception(call, report):\n        hook.pytest_exception_interact(node=item, call=call, report=report)\n    return report",
            "start_line": 187,
            "start_column": 5534,
            "end_line": 0,
            "end_column": 5976,
            "start_byte": 5534,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest._make_xunit_fixture",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def _make_xunit_fixture(obj, setup_name, teardown_name, scope, pass_self):\n    setup = getattr(obj, setup_name, None)\n    teardown = getattr(obj, teardown_name, None)\n    if setup is None and teardown is None:\n        return None\n\n    @pytest.fixture(scope=scope, autouse=True)\n    def fixture(self, request):\n        if getattr(self, \"__unittest_skip__\", None):\n            reason = self.__unittest_skip_why__\n            pytest.skip(reason)\n        if setup is not None:\n            if pass_self:\n                setup(self, request.function)\n            else:\n                setup()\n        yield\n        if teardown is not None:\n            if pass_self:\n                teardown(self, request.function)\n            else:\n                teardown()\n\n    return fixture",
            "start_line": 83,
            "start_column": 2910,
            "end_line": 0,
            "end_column": 3683,
            "start_byte": 2910,
            "end_byte": 0
          },
          {
            "name": "_pytest.unittest.TestCaseFunction.runtest",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def runtest(self):\n        from _pytest.debugging import maybe_wrap_pytest_function_for_tracing\n\n        maybe_wrap_pytest_function_for_tracing(self)\n\n        # let the unittest framework handle async functions\n        if is_async_function(self.obj):\n            self._testcase(self)\n        else:\n            # when --pdb is given, we want to postpone calling tearDown() otherwise\n            # when entering the pdb prompt, tearDown() would have probably cleaned up\n            # instance variables, which makes it difficult to debug\n            # arguably we could always postpone tearDown(), but this changes the moment where the\n            # TestCase instance interacts with the results object, so better to only do it\n            # when absolutely needed\n            if self.config.getoption(\"usepdb\"):\n                self._explicit_tearDown = self._testcase.tearDown\n                setattr(self._testcase, \"tearDown\", lambda *args: None)\n\n            # we need to update the actual bound method with self.obj, because\n            # wrap_pytest_function_for_tracing replaces self.obj by a wrapper\n            setattr(self._testcase, self.name, self.obj)\n            try:\n                self._testcase(result=self)\n            finally:\n                delattr(self._testcase, self.name)",
            "start_line": 207,
            "start_column": 7152,
            "end_line": 4,
            "end_column": 8447,
            "start_byte": 7152,
            "end_byte": 4
          },
          {
            "name": "_pytest.debugging.PdbInvoke.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "def pytest_exception_interact(self, node, call, report):\n        capman = node.config.pluginmanager.getplugin(\"capturemanager\")\n        if capman:\n            capman.suspend_global_capture(in_=True)\n            out, err = capman.read_global_capture()\n            sys.stdout.write(out)\n            sys.stdout.write(err)\n        _enter_pdb(node, call.excinfo, report)",
            "start_line": 257,
            "start_column": 8431,
            "end_line": 4,
            "end_column": 8796,
            "start_byte": 8431,
            "end_byte": 4
          },
          {
            "name": "_pytest.debugging._enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "def _enter_pdb(node, excinfo, rep):\n    # XXX we re-use the TerminalReporter's terminalwriter\n    # because this seems to avoid some encoding related troubles\n    # for not completely clear reasons.\n    tw = node.config.pluginmanager.getplugin(\"terminalreporter\")._tw\n    tw.line()\n\n    showcapture = node.config.option.showcapture\n\n    for sectionname, content in (\n        (\"stdout\", rep.capstdout),\n        (\"stderr\", rep.capstderr),\n        (\"log\", rep.caplog),\n    ):\n        if showcapture in (sectionname, \"all\") and content:\n            tw.sep(\">\", \"captured \" + sectionname)\n            if content[-1:] == \"\\n\":\n                content = content[:-1]\n            tw.line(content)\n\n    tw.sep(\">\", \"traceback\")\n    rep.toterminal(tw)\n    tw.sep(\">\", \"entering PDB\")\n    tb = _postmortem_traceback(excinfo)\n    rep._pdbshown = True\n    post_mortem(tb)\n    return rep",
            "start_line": 304,
            "start_column": 10146,
            "end_line": 0,
            "end_column": 11019,
            "start_byte": 10146,
            "end_byte": 0
          },
          {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "           import pdb\n\n            pdb_cls = pdb.Pdb\n\n        wrapped_cls = cls._get_pdb_wrapper_class(pdb_cls, capman)\n        cls._wrapped_pdb_cls = (usepdb_cls, wrapped_cls)\n        return wrapped_cls\n\n    @classmethod\n    def _",
            "start_line": 96,
            "start_column": 3425,
            "end_line": 4,
            "end_column": 3656,
            "start_byte": 3425,
            "end_byte": 4
          },
          {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "b_wrapper_class(cls, pdb_cls, capman):\n        import _pytest.config\n\n        class PytestPdbWrapper(pdb_cls):\n            _pytest_capman = capman\n            _continued = False\n\n            def do_debug(self, arg):\n                cls._recursive_debug += 1\n",
            "start_line": 104,
            "start_column": 3662,
            "end_line": 4,
            "end_column": 3920,
            "start_byte": 3662,
            "end_byte": 4
          },
          {
            "name": "_pytest.hookspec.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 22873,
            "end_line": 0,
            "end_column": 23146,
            "start_byte": 22873,
            "end_byte": 0
          },
          {
            "name": "_pytest.hookspec.pytest_enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "",
            "start_line": 684,
            "start_column": 23149,
            "end_line": 0,
            "end_column": 23438,
            "start_byte": 23149,
            "end_byte": 0
          }
        ]
      },
      "response": {
        "scratch_pad": "1. The issue describes that when running tests using pytest with the `--pdb` option, the `tearDown` method is executed even for tests that are marked as `skip`. This behavior occurs in pytest version 5.4.2 but not in version 5.4.1.\n\n2. To investigate the issue, the following steps will be taken:\n  - Examine the symbols provided to identify where the `tearDown` method might be getting called.\n  - Look for conditions related to skipping tests and the `--pdb` option.\n  - Determine any changes that need to be made to avoid `tearDown` being called for skipped tests when `--pdb` is used.",
        "symbols": [
          {
            "name": "_pytest.runner.runtestprotocol",
            "file_path": "src/_pytest/runner.py",
            "reason": "This portion controls the flow of running tests, including calling the teardown phase. It does not seem to conditionally skip the teardown based on skip markers when --pdb is enabled."
          },
          {
            "name": "_pytest.runner.call_and_report",
            "file_path": "src/_pytest/runner.py",
            "reason": "This function dispatches to `call_runtest_hook`, which may include `setup`, `call`, or `teardown`. There's potential that teardown should be omitted when `--pdb` is set and the test is skipped."
          },
          {
            "name": "_pytest.unittest.TestCaseFunction.runtest",
            "file_path": "src/_pytest/unittest.py",
            "reason": "The modification related to pdb usage is evident in the comment section about deferring `tearDown` when --pdb is used. This poses as the potential candidate of the error when combined logic for pdb doesn't handle skips."
          }
        ],
        "more_info": [],
        "is_done": false,
        "symbols_scratch_pad": [
          "The control of executing the 'setup', 'call', 'teardown' sequence is in this function. The teardown is unconditional after the setup passes. So if a test is skipped at setup, currently teardown will occur if no modifications handle this specifically.",
          "This function is the wrapper to call hooks like 'setup', 'call', and 'teardown' contained under 'runtestprotocol'. There may need to be additional logic to prevent teardown on skipping the test with pdb.",
          "If the pdb flag is on, there's logic for deferment of tearing down the test class entirely. Its indications are to substitute the real tearDown with a noop in scenarios demanding pdb tracing. So, any logic circumventing pdb should consider skips too."
        ]
      }
    },
    {
      "name": "SymbolSummary",
      "metadata": {
        "start": "2024-08-17T22:38:38.033901",
        "usage": "5.439043045043945 ms"
      },
      "request": {
        "symbols": {
          "related_symbols": [
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_teardown",
              "file_path": "src/_pytest/runner.py",
              "file_content": "    duration=duration,\n            when=when,\n            result=result,\n            excinfo=excinfo,\n        )\n\n    def __repr__(self):\n        if",
              "start_line": 255,
              "start_column": 8576,
              "end_line": 4,
              "end_column": 8723,
              "start_byte": 8576,
              "end_byte": 4
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_teardown",
              "file_path": "src/_pytest/runner.py",
              "file_content": "raised an error\n        for col in self.stack:\n            if hasattr(col, \"_prepare_exc\"):\n                exc = col._prepare_exc\n                raise exc\n        for col in needed_collectors[len(self.stack) :]:\n            self.stack.append(col)\n            try:\n                col.setup()\n            except TEST_OUTCOME as e:\n                col._prepare_exc = e\n                raise e\n\n\ndef ",
              "start_line": 386,
              "start_column": 12830,
              "end_line": 0,
              "end_column": 13229,
              "start_byte": 12830,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_teardown",
              "file_path": "src/_pytest/runner.py",
              "file_content": "",
              "start_line": 662,
              "start_column": 23491,
              "end_line": 4,
              "end_column": 23770,
              "start_byte": 23491,
              "end_byte": 4
            },
            {
              "name": "_pytest.runner.pytest_runtest_teardown",
              "file_path": "src/_pytest/runner.py",
              "file_content": "def pytest_runtest_teardown(item, nextitem):\n    _update_current_test_var(item, \"teardown\")\n    item.session._setupstate.teardown_exact(item, nextitem)\n    _update_current_test_var(item, None)",
              "start_line": 150,
              "start_column": 4467,
              "end_line": 0,
              "end_column": 4659,
              "start_byte": 4467,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.check_interactive_exception",
              "file_path": "src/_pytest/runner.py",
              "file_content": "def check_interactive_exception(call, report):\n    return call.excinfo and not (\n        hasattr(report, \"wasxfail\")\n        or call.excinfo.errisinstance(Skipped)\n        or call.excinfo.errisinstance(bdb.BdbQuit)\n    )",
              "start_line": 200,
              "start_column": 5979,
              "end_line": 0,
              "end_column": 6199,
              "start_byte": 5979,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_makereport",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "",
              "start_line": 396,
              "start_column": 13232,
              "end_line": 0,
              "end_column": 13535,
              "start_byte": 13232,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_makereport",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "try:\n                del call.result\n            except AttributeError:\n                pass\n\n ",
              "start_line": 284,
              "start_column": 8918,
              "end_line": 0,
              "end_column": 9013,
              "start_byte": 8918,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "     self._explicit_tearDown()\n            self._explicit_tearDown = None\n        self._testcase = None\n        self._obj = None\n\n    def startTest(self, testcase):\n        pass\n\n    def _addexcinfo(self, rawexcinfo):\n        # unwrap potential exception info (see twisted trial support below)\n        rawexcinfo = getattr(rawexcinfo, \"_rawexcinfo\", rawexcinfo)\n        try:\n            excinfo = _pytest._code.ExceptionInfo(rawexcinfo)\n            # invoke the attributes to trigger storing the traceback\n            # trial causes some issue there\n            excinfo.value\n            excinfo.traceback\n        except TypeError:\n            try:\n                try:\n                    values = traceback.format_exception(*rawexcinfo)\n                    values.insert(\n                        0,\n                        \"NOTE: Incompatible Exception Representation, \"\n                        \"displaying natively:\\n\\n\",\n                    )\n                    fail(\"\".join(values), pytrace=False)\n                except (fail.Exception, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    fail(\n                        \"ERROR: Unknown Incompatible Exception \"\n                        \"representation:\\n%r\" % (rawexcinfo,),\n                        pytrace=False,\n                    )\n            except KeyboardInterrupt:\n                raise\n            except fail.Exception:\n                excinfo = _pytest._code.ExceptionInfo.from_current()\n        self.__dict__.setdefault(\"_excinfo\", []).append(excinfo)\n\n    def addError(self, testcase, rawexcinfo):\n        try:\n            if isinstance(rawexcinfo[1], exit.Exception):\n                exit(rawexcinfo[1].msg)\n        except TypeError:\n            pass\n        self._addexcinfo(rawexcinfo)\n\n    def addFailure(self, testcase, rawexcinfo):\n        self._addexcinfo(rawexcinfo)\n\n    def addSkip(self, testcase, reason):\n        try:\n            skip(reason)\n        except skip.Exception:\n            self._store[skipped_by_mark_k",
              "start_line": 125,
              "start_column": 4192,
              "end_line": 0,
              "end_column": 6240,
              "start_byte": 4192,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest.UnitTestCase.collect",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "def collect(self):\n        from unittest import TestLoader\n\n        cls = self.obj\n        if not getattr(cls, \"__test__\", True):\n            return\n\n        skipped = getattr(cls, \"__unittest_skip__\", False)\n        if not skipped:\n            self._inject_setup_teardown_fixtures(cls)\n            self._inject_setup_class_fixture()\n\n        self.session._fixturemanager.parsefactories(self, unittest=True)\n        loader = TestLoader()\n        foundsomething = False\n        for name in loader.getTestCaseNames(self.obj):\n            x = getattr(self.obj, name)\n            if not getattr(x, \"__test__\", True):\n                continue\n            funcobj = getimfunc(x)\n            yield TestCaseFunction.from_parent(self, name=name, callobj=funcobj)\n            foundsomething = True\n\n        if not foundsomething:\n            runtest = getattr(self.obj, \"runTest\", None)\n            if runtest is not None:\n                ut = sys.modules.get(\"twisted.trial.unittest\", None)\n                if ut is None or runtest != ut.TestCase.runTest:\n                    # TODO: callobj consistency\n                    yield TestCaseFunction.from_parent(self, name=\"runTest\")",
              "start_line": 36,
              "start_column": 1112,
              "end_line": 4,
              "end_column": 2283,
              "start_byte": 1112,
              "end_byte": 4
            },
            {
              "name": "_pytest.unittest.pytest_runtest_makereport",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_makereport(item, call):\n    if isinstance(item, TestCaseFunction):\n        if item._excinfo:\n            call.excinfo = item._excinfo.pop(0)\n            try:\n                del call.result\n            except AttributeError:\n                pass\n\n    unittest = sys.modules.get(\"unittest\")\n    if unittest and call.excinfo and call.excinfo.errisinstance(unittest.SkipTest):\n        # let's substitute the excinfo with a pytest.skip one\n        call2 = CallInfo.from_call(\n            lambda: pytest.skip(str(call.excinfo.value)), call.when\n        )\n        call.excinfo = call2.excinfo",
              "start_line": 243,
              "start_column": 8721,
              "end_line": 0,
              "end_column": 9351,
              "start_byte": 8721,
              "end_byte": 0
            },
            {
              "name": "_pytest.assertion.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "  \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n       ",
              "start_line": 20,
              "start_column": 497,
              "end_line": 0,
              "end_column": 1273,
              "start_byte": 497,
              "end_byte": 0
            },
            {
              "name": "_pytest.cacheprovider.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 384,
              "start_column": 14196,
              "end_line": 0,
              "end_column": 15963,
              "start_byte": 14196,
              "end_byte": 0
            },
            {
              "name": "_pytest.cacheprovider.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 449,
              "start_column": 16127,
              "end_line": 0,
              "end_column": 16376,
              "start_byte": 16127,
              "end_byte": 0
            },
            {
              "name": "_pytest.capture.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "e,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambd",
              "start_line": 28,
              "start_column": 587,
              "end_line": 0,
              "end_column": 1052,
              "start_byte": 587,
              "end_byte": 0
            },
            {
              "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 245,
              "start_column": 8284,
              "end_line": 4,
              "end_column": 8425,
              "start_byte": 8284,
              "end_byte": 4
            },
            {
              "name": "_pytest.config.PytestPluginManager.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 409,
              "start_column": 12624,
              "end_line": 4,
              "end_column": 13256,
              "start_byte": 12624,
              "end_byte": 4
            },
            {
              "name": "_pytest.debugging.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addiniva",
              "start_line": 21,
              "start_column": 539,
              "end_line": 0,
              "end_column": 1237,
              "start_byte": 539,
              "end_byte": 0
            },
            {
              "name": "_pytest.debugging.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(conditio",
              "start_line": 45,
              "start_column": 1240,
              "end_line": 0,
              "end_column": 2049,
              "start_byte": 1240,
              "end_byte": 0
            },
            {
              "name": "_pytest.doctest.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "  \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" ",
              "start_line": 54,
              "start_column": 1486,
              "end_line": 0,
              "end_column": 2904,
              "start_byte": 1486,
              "end_byte": 0
            },
            {
              "name": "_pytest.faulthandler.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "es import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsucce",
              "start_line": 12,
              "start_column": 157,
              "end_line": 0,
              "end_column": 374,
              "start_byte": 157,
              "end_byte": 0
            },
            {
              "name": "_pytest.faulthandler.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test f",
              "start_line": 20,
              "start_column": 377,
              "end_line": 0,
              "end_column": 1315,
              "start_byte": 377,
              "end_byte": 0
            },
            {
              "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "s\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win3",
              "start_line": 48,
              "start_column": 1495,
              "end_line": 4,
              "end_column": 1768,
              "start_byte": 1495,
              "end_byte": 4
            },
            {
              "name": "_pytest.fixtures.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 1202,
              "start_column": 45360,
              "end_line": 0,
              "end_column": 45548,
              "start_byte": 45360,
              "end_byte": 0
            },
            {
              "name": "_pytest.helpconfig.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": ".Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_set",
              "start_line": 38,
              "start_column": 1168,
              "end_line": 0,
              "end_column": 2499,
              "start_byte": 1168,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            s",
              "start_line": 50,
              "start_column": 1425,
              "end_line": 0,
              "end_column": 3012,
              "start_byte": 1425,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n ",
              "start_line": 88,
              "start_column": 3015,
              "end_line": 0,
              "end_column": 3498,
              "start_byte": 3015,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 378,
              "start_column": 12657,
              "end_line": 0,
              "end_column": 12745,
              "start_byte": 12657,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 396,
              "start_column": 13232,
              "end_line": 0,
              "end_column": 13535,
              "start_byte": 13232,
              "end_byte": 0
            },
            {
              "name": "_pytest.junitxml.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 365,
              "start_column": 12037,
              "end_line": 0,
              "end_column": 13376,
              "start_byte": 12037,
              "end_byte": 0
            },
            {
              "name": "_pytest.junitxml.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 410,
              "start_column": 13379,
              "end_line": 0,
              "end_column": 14153,
              "start_byte": 13379,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "report.skipped:\n            return \"xfailed\", \"x\", \"XFAIL\"\n        elif report.passed:\n            return \"xpassed\", \"X\", \"XPASS\"\n",
              "start_line": 182,
              "start_column": 6378,
              "end_line": 0,
              "end_column": 8992,
              "start_byte": 6378,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 471,
              "start_column": 16133,
              "end_line": 0,
              "end_column": 16268,
              "start_byte": 16133,
              "end_byte": 0
            },
            {
              "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 649,
              "start_column": 23082,
              "end_line": 4,
              "end_column": 23306,
              "start_byte": 23082,
              "end_byte": 4
            },
            {
              "name": "_pytest.main.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and eval",
              "start_line": 40,
              "start_column": 989,
              "end_line": 0,
              "end_column": 5037,
              "start_byte": 989,
              "end_byte": 0
            },
            {
              "name": "_pytest.mark.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yi",
              "start_line": 53,
              "start_column": 1694,
              "end_line": 0,
              "end_column": 3297,
              "start_byte": 1694,
              "end_byte": 0
            },
            {
              "name": "_pytest.mark.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 250,
              "start_column": 7889,
              "end_line": 0,
              "end_column": 8340,
              "start_byte": 7889,
              "end_byte": 0
            },
            {
              "name": "_pytest.pastebin.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ort xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"run",
              "start_line": 12,
              "start_column": 231,
              "end_line": 0,
              "end_column": 559,
              "start_byte": 231,
              "end_byte": 0
            },
            {
              "name": "_pytest.pastebin.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "il\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(rea",
              "start_line": 25,
              "start_column": 562,
              "end_line": 0,
              "end_column": 1376,
              "start_byte": 562,
              "end_byte": 0
            },
            {
              "name": "_pytest.pytester.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "eason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n       ",
              "start_line": 55,
              "start_column": 1374,
              "end_line": 0,
              "end_column": 1985,
              "start_byte": 1374,
              "end_byte": 0
            },
            {
              "name": "_pytest.pytester.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"",
              "start_line": 80,
              "start_column": 1988,
              "end_line": 0,
              "end_column": 2364,
              "start_byte": 1988,
              "end_byte": 0
            },
            {
              "name": "_pytest.python.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ine(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n      ",
              "start_line": 59,
              "start_column": 1875,
              "end_line": 0,
              "end_column": 3550,
              "start_byte": 1875,
              "end_byte": 0
            },
            {
              "name": "_pytest.python.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "ail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skippe",
              "start_line": 129,
              "start_column": 4062,
              "end_line": 0,
              "end_column": 4962,
              "start_byte": 4062,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "n.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_lin",
              "start_line": 34,
              "start_column": 933,
              "end_line": 0,
              "end_column": 1244,
              "start_byte": 933,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "alxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_s",
              "start_line": 125,
              "start_column": 3822,
              "end_line": 0,
              "end_column": 3940,
              "start_byte": 3822,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 284,
              "start_column": 8918,
              "end_line": 0,
              "end_column": 9013,
              "start_byte": 8918,
              "end_byte": 0
            },
            {
              "name": "_pytest.setuponly.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgrou",
              "start_line": 4,
              "start_column": 58,
              "end_line": 0,
              "end_column": 457,
              "start_byte": 58,
              "end_byte": 0
            },
            {
              "name": "_pytest.setupplan.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark",
              "start_line": 3,
              "start_column": 16,
              "end_line": 0,
              "end_column": 293,
              "start_byte": 16,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
              "start_line": 14,
              "start_column": 401,
              "end_line": 0,
              "end_column": 881,
              "start_byte": 401,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "def pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )",
              "start_line": 33,
              "start_column": 884,
              "end_line": 0,
              "end_column": 2449,
              "start_byte": 884,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
              "start_line": 73,
              "start_column": 2452,
              "end_line": 0,
              "end_column": 3179,
              "start_byte": 2452,
              "end_byte": 0
            },
            {
              "name": "_pytest.skipping.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        rep.longrepr = str(filename), line + 1, reason",
              "start_line": 125,
              "start_column": 4192,
              "end_line": 0,
              "end_column": 6240,
              "start_byte": 4192,
              "end_byte": 0
            },
            {
              "name": "_pytest.stepwise.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group",
              "start_line": 3,
              "start_column": 16,
              "end_line": 0,
              "end_column": 479,
              "start_byte": 16,
              "end_byte": 0
            },
            {
              "name": "_pytest.stepwise.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "doption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"",
              "start_line": 20,
              "start_column": 482,
              "end_line": 0,
              "end_column": 604,
              "start_byte": 482,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n",
              "start_line": 78,
              "start_column": 1896,
              "end_line": 0,
              "end_column": 4639,
              "start_byte": 1896,
              "end_byte": 0
            },
            {
              "name": "_pytest.terminal.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "     else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        ",
              "start_line": 173,
              "start_column": 4642,
              "end_line": 0,
              "end_column": 5061,
              "start_byte": 4642,
              "end_byte": 0
            },
            {
              "name": "_pytest.tmpdir.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif cal",
              "start_line": 137,
              "start_column": 4708,
              "end_line": 0,
              "end_column": 5308,
              "start_byte": 4708,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest.pytest_runtest_makereport",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "",
              "start_line": 243,
              "start_column": 8721,
              "end_line": 0,
              "end_column": 9351,
              "start_byte": 8721,
              "end_byte": 0
            },
            {
              "name": "_pytest.warnings.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": " the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"ma",
              "start_line": 51,
              "start_column": 1531,
              "end_line": 0,
              "end_column": 1989,
              "start_byte": 1531,
              "end_byte": 0
            },
            {
              "name": "_pytest.warnings.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific except",
              "start_line": 68,
              "start_column": 1992,
              "end_line": 0,
              "end_column": 2242,
              "start_byte": 1992,
              "end_byte": 0
            },
            {
              "name": "_pytest.nose.pytest_runtest_setup",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "r\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n      ",
              "start_line": 6,
              "start_column": 136,
              "end_line": 0,
              "end_column": 548,
              "start_byte": 136,
              "end_byte": 0
            },
            {
              "name": "_pytest.resultlog.pytest_addoption",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"-",
              "start_line": 13,
              "start_column": 185,
              "end_line": 0,
              "end_column": 501,
              "start_byte": 185,
              "end_byte": 0
            },
            {
              "name": "_pytest.resultlog.pytest_configure",
              "file_path": "src/_pytest/skipping.py",
              "file_content": "nxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(",
              "start_line": 25,
              "start_column": 504,
              "end_line": 0,
              "end_column": 1195,
              "start_byte": 504,
              "end_byte": 0
            }
          ],
          "root_caused_symbols": [
            {
              "name": "_pytest.runner.runtestprotocol",
              "file_path": "src/_pytest/runner.py",
              "file_content": "def runtestprotocol(item, log=True, nextitem=None):\n    hasrequest = hasattr(item, \"_request\")\n    if hasrequest and not item._request:\n        item._initrequest()\n    rep = call_and_report(item, \"setup\", log)\n    reports = [rep]\n    if rep.passed:\n        if item.config.getoption(\"setupshow\", False):\n            show_test_item(item)\n        if not item.config.getoption(\"setuponly\", False):\n            reports.append(call_and_report(item, \"call\", log))\n    reports.append(call_and_report(item, \"teardown\", log, nextitem=nextitem))\n    # after all teardown hooks have been called\n    # want funcargs and request info to go away\n    if hasrequest:\n        item._request = False\n        item.funcargs = None\n    return reports",
              "start_line": 93,
              "start_column": 2710,
              "end_line": 0,
              "end_column": 3437,
              "start_byte": 2710,
              "end_byte": 0
            },
            {
              "name": "_pytest.runner.call_and_report",
              "file_path": "src/_pytest/runner.py",
              "file_content": "def call_and_report(\n    item, when: \"Literal['setup', 'call', 'teardown']\", log=True, **kwds\n):\n    call = call_runtest_hook(item, when, **kwds)\n    hook = item.ihook\n    report = hook.pytest_runtest_makereport(item=item, call=call)\n    if log:\n        hook.pytest_runtest_logreport(report=report)\n    if check_interactive_exception(call, report):\n        hook.pytest_exception_interact(node=item, call=call, report=report)\n    return report",
              "start_line": 187,
              "start_column": 5534,
              "end_line": 0,
              "end_column": 5976,
              "start_byte": 5534,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest._make_xunit_fixture",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "def _make_xunit_fixture(obj, setup_name, teardown_name, scope, pass_self):\n    setup = getattr(obj, setup_name, None)\n    teardown = getattr(obj, teardown_name, None)\n    if setup is None and teardown is None:\n        return None\n\n    @pytest.fixture(scope=scope, autouse=True)\n    def fixture(self, request):\n        if getattr(self, \"__unittest_skip__\", None):\n            reason = self.__unittest_skip_why__\n            pytest.skip(reason)\n        if setup is not None:\n            if pass_self:\n                setup(self, request.function)\n            else:\n                setup()\n        yield\n        if teardown is not None:\n            if pass_self:\n                teardown(self, request.function)\n            else:\n                teardown()\n\n    return fixture",
              "start_line": 83,
              "start_column": 2910,
              "end_line": 0,
              "end_column": 3683,
              "start_byte": 2910,
              "end_byte": 0
            },
            {
              "name": "_pytest.unittest.TestCaseFunction.runtest",
              "file_path": "src/_pytest/unittest.py",
              "file_content": "def runtest(self):\n        from _pytest.debugging import maybe_wrap_pytest_function_for_tracing\n\n        maybe_wrap_pytest_function_for_tracing(self)\n\n        # let the unittest framework handle async functions\n        if is_async_function(self.obj):\n            self._testcase(self)\n        else:\n            # when --pdb is given, we want to postpone calling tearDown() otherwise\n            # when entering the pdb prompt, tearDown() would have probably cleaned up\n            # instance variables, which makes it difficult to debug\n            # arguably we could always postpone tearDown(), but this changes the moment where the\n            # TestCase instance interacts with the results object, so better to only do it\n            # when absolutely needed\n            if self.config.getoption(\"usepdb\"):\n                self._explicit_tearDown = self._testcase.tearDown\n                setattr(self._testcase, \"tearDown\", lambda *args: None)\n\n            # we need to update the actual bound method with self.obj, because\n            # wrap_pytest_function_for_tracing replaces self.obj by a wrapper\n            setattr(self._testcase, self.name, self.obj)\n            try:\n                self._testcase(result=self)\n            finally:\n                delattr(self._testcase, self.name)",
              "start_line": 207,
              "start_column": 7152,
              "end_line": 4,
              "end_column": 8447,
              "start_byte": 7152,
              "end_byte": 4
            },
            {
              "name": "_pytest.debugging.PdbInvoke.pytest_exception_interact",
              "file_path": "src/_pytest/debugging.py",
              "file_content": "def pytest_exception_interact(self, node, call, report):\n        capman = node.config.pluginmanager.getplugin(\"capturemanager\")\n        if capman:\n            capman.suspend_global_capture(in_=True)\n            out, err = capman.read_global_capture()\n            sys.stdout.write(out)\n            sys.stdout.write(err)\n        _enter_pdb(node, call.excinfo, report)",
              "start_line": 257,
              "start_column": 8431,
              "end_line": 4,
              "end_column": 8796,
              "start_byte": 8431,
              "end_byte": 4
            },
            {
              "name": "_pytest.debugging._enter_pdb",
              "file_path": "src/_pytest/debugging.py",
              "file_content": "def _enter_pdb(node, excinfo, rep):\n    # XXX we re-use the TerminalReporter's terminalwriter\n    # because this seems to avoid some encoding related troubles\n    # for not completely clear reasons.\n    tw = node.config.pluginmanager.getplugin(\"terminalreporter\")._tw\n    tw.line()\n\n    showcapture = node.config.option.showcapture\n\n    for sectionname, content in (\n        (\"stdout\", rep.capstdout),\n        (\"stderr\", rep.capstderr),\n        (\"log\", rep.caplog),\n    ):\n        if showcapture in (sectionname, \"all\") and content:\n            tw.sep(\">\", \"captured \" + sectionname)\n            if content[-1:] == \"\\n\":\n                content = content[:-1]\n            tw.line(content)\n\n    tw.sep(\">\", \"traceback\")\n    rep.toterminal(tw)\n    tw.sep(\">\", \"entering PDB\")\n    tb = _postmortem_traceback(excinfo)\n    rep._pdbshown = True\n    post_mortem(tb)\n    return rep",
              "start_line": 304,
              "start_column": 10146,
              "end_line": 0,
              "end_column": 11019,
              "start_byte": 10146,
              "end_byte": 0
            },
            {
              "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_enter_pdb",
              "file_path": "src/_pytest/debugging.py",
              "file_content": "           import pdb\n\n            pdb_cls = pdb.Pdb\n\n        wrapped_cls = cls._get_pdb_wrapper_class(pdb_cls, capman)\n        cls._wrapped_pdb_cls = (usepdb_cls, wrapped_cls)\n        return wrapped_cls\n\n    @classmethod\n    def _",
              "start_line": 96,
              "start_column": 3425,
              "end_line": 4,
              "end_column": 3656,
              "start_byte": 3425,
              "end_byte": 4
            },
            {
              "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_exception_interact",
              "file_path": "src/_pytest/debugging.py",
              "file_content": "b_wrapper_class(cls, pdb_cls, capman):\n        import _pytest.config\n\n        class PytestPdbWrapper(pdb_cls):\n            _pytest_capman = capman\n            _continued = False\n\n            def do_debug(self, arg):\n                cls._recursive_debug += 1\n",
              "start_line": 104,
              "start_column": 3662,
              "end_line": 4,
              "end_column": 3920,
              "start_byte": 3662,
              "end_byte": 4
            },
            {
              "name": "_pytest.hookspec.pytest_exception_interact",
              "file_path": "src/_pytest/debugging.py",
              "file_content": "",
              "start_line": 675,
              "start_column": 22873,
              "end_line": 0,
              "end_column": 23146,
              "start_byte": 22873,
              "end_byte": 0
            },
            {
              "name": "_pytest.hookspec.pytest_enter_pdb",
              "file_path": "src/_pytest/debugging.py",
              "file_content": "",
              "start_line": 684,
              "start_column": 23149,
              "end_line": 0,
              "end_column": 23438,
              "start_byte": 23149,
              "end_byte": 0
            }
          ]
        },
        "symbols_evaluate": {
          "scratch_pad": "1. The issue describes that when running tests using pytest with the `--pdb` option, the `tearDown` method is executed even for tests that are marked as `skip`. This behavior occurs in pytest version 5.4.2 but not in version 5.4.1.\n\n2. To investigate the issue, the following steps will be taken:\n  - Examine the symbols provided to identify where the `tearDown` method might be getting called.\n  - Look for conditions related to skipping tests and the `--pdb` option.\n  - Determine any changes that need to be made to avoid `tearDown` being called for skipped tests when `--pdb` is used.",
          "symbols": [
            {
              "name": "_pytest.runner.runtestprotocol",
              "file_path": "src/_pytest/runner.py",
              "reason": "This portion controls the flow of running tests, including calling the teardown phase. It does not seem to conditionally skip the teardown based on skip markers when --pdb is enabled."
            },
            {
              "name": "_pytest.runner.call_and_report",
              "file_path": "src/_pytest/runner.py",
              "reason": "This function dispatches to `call_runtest_hook`, which may include `setup`, `call`, or `teardown`. There's potential that teardown should be omitted when `--pdb` is set and the test is skipped."
            },
            {
              "name": "_pytest.unittest.TestCaseFunction.runtest",
              "file_path": "src/_pytest/unittest.py",
              "reason": "The modification related to pdb usage is evident in the comment section about deferring `tearDown` when --pdb is used. This poses as the potential candidate of the error when combined logic for pdb doesn't handle skips."
            }
          ],
          "more_info": [],
          "is_done": false,
          "symbols_scratch_pad": [
            "The control of executing the 'setup', 'call', 'teardown' sequence is in this function. The teardown is unconditional after the setup passes. So if a test is skipped at setup, currently teardown will occur if no modifications handle this specifically.",
            "This function is the wrapper to call hooks like 'setup', 'call', and 'teardown' contained under 'runtestprotocol'. There may need to be additional logic to prevent teardown on skipping the test with pdb.",
            "If the pdb flag is on, there's logic for deferment of tearing down the test class entirely. Its indications are to substitute the real tearDown with a noop in scenarios demanding pdb tracing. So, any logic circumventing pdb should consider skips too."
          ]
        }
      },
      "response": {
        "file_paths": [
          "src/_pytest/runner.py",
          "src/_pytest/unittest.py"
        ],
        "symbols": [
          "_pytest.runner.runtestprotocol",
          "_pytest.runner.call_and_report",
          "_pytest.unittest.TestCaseFunction.runtest"
        ],
        "code_spans": {
          "_pytest.capture.CaptureManager.pytest_runtest_teardown": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "    duration=duration,\n            when=when,\n            result=result,\n            excinfo=excinfo,\n        )\n\n    def __repr__(self):\n        if",
            "start_line": 255,
            "start_column": 8576,
            "end_line": 4,
            "end_column": 8723,
            "start_byte": 8576,
            "end_byte": 4
          },
          "_pytest.hookspec.pytest_runtest_teardown": {
            "name": "_pytest.hookspec.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "raised an error\n        for col in self.stack:\n            if hasattr(col, \"_prepare_exc\"):\n                exc = col._prepare_exc\n                raise exc\n        for col in needed_collectors[len(self.stack) :]:\n            self.stack.append(col)\n            try:\n                col.setup()\n            except TEST_OUTCOME as e:\n                col._prepare_exc = e\n                raise e\n\n\ndef ",
            "start_line": 386,
            "start_column": 12830,
            "end_line": 0,
            "end_column": 13229,
            "start_byte": 12830,
            "end_byte": 0
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_teardown": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "",
            "start_line": 662,
            "start_column": 23491,
            "end_line": 4,
            "end_column": 23770,
            "start_byte": 23491,
            "end_byte": 4
          },
          "_pytest.runner.pytest_runtest_teardown": {
            "name": "_pytest.runner.pytest_runtest_teardown",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def pytest_runtest_teardown(item, nextitem):\n    _update_current_test_var(item, \"teardown\")\n    item.session._setupstate.teardown_exact(item, nextitem)\n    _update_current_test_var(item, None)",
            "start_line": 150,
            "start_column": 4467,
            "end_line": 0,
            "end_column": 4659,
            "start_byte": 4467,
            "end_byte": 0
          },
          "_pytest.runner.check_interactive_exception": {
            "name": "_pytest.runner.check_interactive_exception",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def check_interactive_exception(call, report):\n    return call.excinfo and not (\n        hasattr(report, \"wasxfail\")\n        or call.excinfo.errisinstance(Skipped)\n        or call.excinfo.errisinstance(bdb.BdbQuit)\n    )",
            "start_line": 200,
            "start_column": 5979,
            "end_line": 0,
            "end_column": 6199,
            "start_byte": 5979,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_runtest_makereport": {
            "name": "_pytest.hookspec.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 396,
            "start_column": 13232,
            "end_line": 0,
            "end_column": 13535,
            "start_byte": 13232,
            "end_byte": 0
          },
          "_pytest.runner.pytest_runtest_makereport": {
            "name": "_pytest.runner.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 284,
            "start_column": 8918,
            "end_line": 0,
            "end_column": 9013,
            "start_byte": 8918,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_makereport": {
            "name": "_pytest.skipping.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif call.when == \"call\":\n            strict_default = item.config.getini(\"xfail_strict\")\n            is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n            explanation = evalxfail.getexplanation()\n            if is_strict_xfail:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] {}\".format(explanation)\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = explanation\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n        filename, line = item.reportinfo()[:2]\n        rep.longrepr = str(filename), line + 1, reason",
            "start_line": 125,
            "start_column": 4192,
            "end_line": 0,
            "end_column": 6240,
            "start_byte": 4192,
            "end_byte": 0
          },
          "_pytest.unittest.UnitTestCase.collect": {
            "name": "_pytest.unittest.UnitTestCase.collect",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def collect(self):\n        from unittest import TestLoader\n\n        cls = self.obj\n        if not getattr(cls, \"__test__\", True):\n            return\n\n        skipped = getattr(cls, \"__unittest_skip__\", False)\n        if not skipped:\n            self._inject_setup_teardown_fixtures(cls)\n            self._inject_setup_class_fixture()\n\n        self.session._fixturemanager.parsefactories(self, unittest=True)\n        loader = TestLoader()\n        foundsomething = False\n        for name in loader.getTestCaseNames(self.obj):\n            x = getattr(self.obj, name)\n            if not getattr(x, \"__test__\", True):\n                continue\n            funcobj = getimfunc(x)\n            yield TestCaseFunction.from_parent(self, name=name, callobj=funcobj)\n            foundsomething = True\n\n        if not foundsomething:\n            runtest = getattr(self.obj, \"runTest\", None)\n            if runtest is not None:\n                ut = sys.modules.get(\"twisted.trial.unittest\", None)\n                if ut is None or runtest != ut.TestCase.runTest:\n                    # TODO: callobj consistency\n                    yield TestCaseFunction.from_parent(self, name=\"runTest\")",
            "start_line": 36,
            "start_column": 1112,
            "end_line": 4,
            "end_column": 2283,
            "start_byte": 1112,
            "end_byte": 4
          },
          "_pytest.unittest.pytest_runtest_makereport": {
            "name": "_pytest.unittest.pytest_runtest_makereport",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 243,
            "start_column": 8721,
            "end_line": 0,
            "end_column": 9351,
            "start_byte": 8721,
            "end_byte": 0
          },
          "_pytest.assertion.pytest_addoption": {
            "name": "_pytest.assertion.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "  \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n       ",
            "start_line": 20,
            "start_column": 497,
            "end_line": 0,
            "end_column": 1273,
            "start_byte": 497,
            "end_byte": 0
          },
          "_pytest.cacheprovider.pytest_addoption": {
            "name": "_pytest.cacheprovider.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 384,
            "start_column": 14196,
            "end_line": 0,
            "end_column": 15963,
            "start_byte": 14196,
            "end_byte": 0
          },
          "_pytest.cacheprovider.pytest_configure": {
            "name": "_pytest.cacheprovider.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 449,
            "start_column": 16127,
            "end_line": 0,
            "end_column": 16376,
            "start_byte": 16127,
            "end_byte": 0
          },
          "_pytest.capture.pytest_addoption": {
            "name": "_pytest.capture.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "e,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambd",
            "start_line": 28,
            "start_column": 587,
            "end_line": 0,
            "end_column": 1052,
            "start_byte": 587,
            "end_byte": 0
          },
          "_pytest.capture.CaptureManager.pytest_runtest_setup": {
            "name": "_pytest.capture.CaptureManager.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 245,
            "start_column": 8284,
            "end_line": 4,
            "end_column": 8425,
            "start_byte": 8284,
            "end_byte": 4
          },
          "_pytest.config.PytestPluginManager.pytest_configure": {
            "name": "_pytest.config.PytestPluginManager.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 409,
            "start_column": 12624,
            "end_line": 4,
            "end_column": 13256,
            "start_byte": 12624,
            "end_byte": 4
          },
          "_pytest.debugging.pytest_addoption": {
            "name": "_pytest.debugging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addiniva",
            "start_line": 21,
            "start_column": 539,
            "end_line": 0,
            "end_column": 1237,
            "start_byte": 539,
            "end_byte": 0
          },
          "_pytest.debugging.pytest_configure": {
            "name": "_pytest.debugging.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(conditio",
            "start_line": 45,
            "start_column": 1240,
            "end_line": 0,
            "end_column": 2049,
            "start_byte": 1240,
            "end_byte": 0
          },
          "_pytest.doctest.pytest_addoption": {
            "name": "_pytest.doctest.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "  \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" ",
            "start_line": 54,
            "start_column": 1486,
            "end_line": 0,
            "end_column": 2904,
            "start_byte": 1486,
            "end_byte": 0
          },
          "_pytest.faulthandler.pytest_addoption": {
            "name": "_pytest.faulthandler.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "es import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsucce",
            "start_line": 12,
            "start_column": 157,
            "end_line": 0,
            "end_column": 374,
            "start_byte": 157,
            "end_byte": 0
          },
          "_pytest.faulthandler.pytest_configure": {
            "name": "_pytest.faulthandler.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test f",
            "start_line": 20,
            "start_column": 377,
            "end_line": 0,
            "end_column": 1315,
            "start_byte": 377,
            "end_byte": 0
          },
          "_pytest.faulthandler.FaultHandlerHooks.pytest_configure": {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "s\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win3",
            "start_line": 48,
            "start_column": 1495,
            "end_line": 4,
            "end_column": 1768,
            "start_byte": 1495,
            "end_byte": 4
          },
          "_pytest.fixtures.pytest_addoption": {
            "name": "_pytest.fixtures.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 1202,
            "start_column": 45360,
            "end_line": 0,
            "end_column": 45548,
            "start_byte": 45360,
            "end_byte": 0
          },
          "_pytest.helpconfig.pytest_addoption": {
            "name": "_pytest.helpconfig.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": ".Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_set",
            "start_line": 38,
            "start_column": 1168,
            "end_line": 0,
            "end_column": 2499,
            "start_byte": 1168,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_addoption": {
            "name": "_pytest.hookspec.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            s",
            "start_line": 50,
            "start_column": 1425,
            "end_line": 0,
            "end_column": 3012,
            "start_byte": 1425,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_configure": {
            "name": "_pytest.hookspec.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n ",
            "start_line": 88,
            "start_column": 3015,
            "end_line": 0,
            "end_column": 3498,
            "start_byte": 3015,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_runtest_setup": {
            "name": "_pytest.hookspec.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 378,
            "start_column": 12657,
            "end_line": 0,
            "end_column": 12745,
            "start_byte": 12657,
            "end_byte": 0
          },
          "_pytest.junitxml.pytest_addoption": {
            "name": "_pytest.junitxml.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 365,
            "start_column": 12037,
            "end_line": 0,
            "end_column": 13376,
            "start_byte": 12037,
            "end_byte": 0
          },
          "_pytest.junitxml.pytest_configure": {
            "name": "_pytest.junitxml.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 410,
            "start_column": 13379,
            "end_line": 0,
            "end_column": 14153,
            "start_byte": 13379,
            "end_byte": 0
          },
          "_pytest.logging.pytest_addoption": {
            "name": "_pytest.logging.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "report.skipped:\n            return \"xfailed\", \"x\", \"XFAIL\"\n        elif report.passed:\n            return \"xpassed\", \"X\", \"XPASS\"\n",
            "start_line": 182,
            "start_column": 6378,
            "end_line": 0,
            "end_column": 8992,
            "start_byte": 6378,
            "end_byte": 0
          },
          "_pytest.logging.pytest_configure": {
            "name": "_pytest.logging.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 471,
            "start_column": 16133,
            "end_line": 0,
            "end_column": 16268,
            "start_byte": 16133,
            "end_byte": 0
          },
          "_pytest.logging.LoggingPlugin.pytest_runtest_setup": {
            "name": "_pytest.logging.LoggingPlugin.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 649,
            "start_column": 23082,
            "end_line": 4,
            "end_column": 23306,
            "start_byte": 23082,
            "end_byte": 4
          },
          "_pytest.main.pytest_addoption": {
            "name": "_pytest.main.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and eval",
            "start_line": 40,
            "start_column": 989,
            "end_line": 0,
            "end_column": 5037,
            "start_byte": 989,
            "end_byte": 0
          },
          "_pytest.mark.pytest_addoption": {
            "name": "_pytest.mark.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yi",
            "start_line": 53,
            "start_column": 1694,
            "end_line": 0,
            "end_column": 3297,
            "start_byte": 1694,
            "end_byte": 0
          },
          "_pytest.mark.pytest_configure": {
            "name": "_pytest.mark.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "",
            "start_line": 250,
            "start_column": 7889,
            "end_line": 0,
            "end_column": 8340,
            "start_byte": 7889,
            "end_byte": 0
          },
          "_pytest.pastebin.pytest_addoption": {
            "name": "_pytest.pastebin.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ort xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"run",
            "start_line": 12,
            "start_column": 231,
            "end_line": 0,
            "end_column": 559,
            "start_byte": 231,
            "end_byte": 0
          },
          "_pytest.pastebin.pytest_configure": {
            "name": "_pytest.pastebin.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "il\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(rea",
            "start_line": 25,
            "start_column": 562,
            "end_line": 0,
            "end_column": 1376,
            "start_byte": 562,
            "end_byte": 0
          },
          "_pytest.pytester.pytest_addoption": {
            "name": "_pytest.pytester.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "eason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n       ",
            "start_line": 55,
            "start_column": 1374,
            "end_line": 0,
            "end_column": 1985,
            "start_byte": 1374,
            "end_byte": 0
          },
          "_pytest.pytester.pytest_configure": {
            "name": "_pytest.pytester.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"",
            "start_line": 80,
            "start_column": 1988,
            "end_line": 0,
            "end_column": 2364,
            "start_byte": 1988,
            "end_byte": 0
          },
          "_pytest.python.pytest_addoption": {
            "name": "_pytest.python.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ine(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n      ",
            "start_line": 59,
            "start_column": 1875,
            "end_line": 0,
            "end_column": 3550,
            "start_byte": 1875,
            "end_byte": 0
          },
          "_pytest.python.pytest_configure": {
            "name": "_pytest.python.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "ail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skippe",
            "start_line": 129,
            "start_column": 4062,
            "end_line": 0,
            "end_column": 4962,
            "start_byte": 4062,
            "end_byte": 0
          },
          "_pytest.runner.pytest_addoption": {
            "name": "_pytest.runner.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "n.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_lin",
            "start_line": 34,
            "start_column": 933,
            "end_line": 0,
            "end_column": 1244,
            "start_byte": 933,
            "end_byte": 0
          },
          "_pytest.runner.pytest_runtest_setup": {
            "name": "_pytest.runner.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "alxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_s",
            "start_line": 125,
            "start_column": 3822,
            "end_line": 0,
            "end_column": 3940,
            "start_byte": 3822,
            "end_byte": 0
          },
          "_pytest.setuponly.pytest_addoption": {
            "name": "_pytest.setuponly.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgrou",
            "start_line": 4,
            "start_column": 58,
            "end_line": 0,
            "end_column": 457,
            "start_byte": 58,
            "end_byte": 0
          },
          "_pytest.setupplan.pytest_addoption": {
            "name": "_pytest.setupplan.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark",
            "start_line": 3,
            "start_column": 16,
            "end_line": 0,
            "end_column": 293,
            "start_byte": 16,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_addoption": {
            "name": "_pytest.skipping.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )",
            "start_line": 14,
            "start_column": 401,
            "end_line": 0,
            "end_column": 881,
            "start_byte": 401,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_configure": {
            "name": "_pytest.skipping.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "def pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(pytest, \"xfail\", nop)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"skip(reason=None): skip the given test function with an optional reason. \"\n        'Example: skip(reason=\"no way of currently testing this\") skips the '\n        \"test.\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"skipif(condition): skip the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )",
            "start_line": 33,
            "start_column": 884,
            "end_line": 0,
            "end_column": 2449,
            "start_byte": 884,
            "end_byte": 0
          },
          "_pytest.skipping.pytest_runtest_setup": {
            "name": "_pytest.skipping.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)",
            "start_line": 73,
            "start_column": 2452,
            "end_line": 0,
            "end_column": 3179,
            "start_byte": 2452,
            "end_byte": 0
          },
          "_pytest.stepwise.pytest_addoption": {
            "name": "_pytest.stepwise.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "skip/xfail functions and markers. \"\"\"\nfrom _pytest.config import hookimpl\nfrom _pytest.mark.evaluate import MarkEvaluator\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group",
            "start_line": 3,
            "start_column": 16,
            "end_line": 0,
            "end_column": 479,
            "start_byte": 16,
            "end_byte": 0
          },
          "_pytest.stepwise.pytest_configure": {
            "name": "_pytest.stepwise.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "doption(\n        \"--runxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"",
            "start_line": 20,
            "start_column": 482,
            "end_line": 0,
            "end_column": 604,
            "start_byte": 482,
            "end_byte": 0
          },
          "_pytest.terminal.pytest_addoption": {
            "name": "_pytest.terminal.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"mark the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific exception(s) are expected, you can list them in \"\n        \"raises, and if the test fails in other ways, it will be reported as \"\n        \"a true failure. See https://docs.pytest.org/en/latest/skipping.html\",\n    )\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item):\n    # Check if skip or skipif are specified as pytest marks\n    item._store[skipped_by_mark_key] = False\n    eval_skipif = MarkEvaluator(item, \"skipif\")\n    if eval_skipif.istrue():\n        item._store[skipped_by_mark_key] = True\n        skip(eval_skipif.getexplanation())\n\n    for skip_info in item.iter_markers(name=\"skip\"):\n        item._store[skipped_by_mark_key] = True\n        if \"reason\" in skip_info.kwargs:\n            skip(skip_info.kwargs[\"reason\"])\n        elif skip_info.args:\n            skip(skip_info.args[0])\n        else:\n            skip(\"unconditional skip\")\n\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    check_xfail_no_run(item)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    check_xfail_no_run(pyfuncitem)\n    outcome = yield\n    passed = outcome.excinfo is None\n    if passed:\n        check_strict_xfail(pyfuncitem)\n\n\ndef check_xfail_no_run(item):\n    \"\"\"check xfail(run=False)\"\"\"\n    if not item.config.option.runxfail:\n        evalxfail = item._store[evalxfail_key]\n        if evalxfail.istrue():\n            if not evalxfail.get(\"run\", True):\n                xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\n\n\ndef check_strict_xfail(pyfuncitem):\n    \"\"\"check xfail(strict=True) for the given PASSING test\"\"\"\n    evalxfail = pyfuncitem._store[evalxfail_key]\n    if evalxfail.istrue():\n        strict_default = pyfuncitem.config.getini(\"xfail_strict\")\n        is_strict_xfail = evalxfail.get(\"strict\", strict_default)\n        if is_strict_xfail:\n            del pyfuncitem._store[evalxfail_key]\n            explanation = evalxfail.getexplanation()\n            fail(\"[XPASS(strict)] \" + explanation, pytrace=False)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n",
            "start_line": 78,
            "start_column": 1896,
            "end_line": 0,
            "end_column": 4639,
            "start_byte": 1896,
            "end_byte": 0
          },
          "_pytest.terminal.pytest_configure": {
            "name": "_pytest.terminal.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "     else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        ",
            "start_line": 173,
            "start_column": 4642,
            "end_line": 0,
            "end_column": 5061,
            "start_byte": 4642,
            "end_byte": 0
          },
          "_pytest.tmpdir.pytest_configure": {
            "name": "_pytest.tmpdir.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " rep.outcome = \"failed\"\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and call.excinfo.errisinstance(xfail.Exception):\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif evalxfail and not rep.skipped and evalxfail.wasvalid() and evalxfail.istrue():\n        if call.excinfo:\n            if evalxfail.invalidraise(call.excinfo.value):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = evalxfail.getexplanation()\n        elif cal",
            "start_line": 137,
            "start_column": 4708,
            "end_line": 0,
            "end_column": 5308,
            "start_byte": 4708,
            "end_byte": 0
          },
          "_pytest.warnings.pytest_addoption": {
            "name": "_pytest.warnings.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": " the given test function if eval(condition) \"\n        \"results in a True value.  Evaluation happens within the \"\n        \"module global context. Example: skipif('sys.platform == \\\"win32\\\"') \"\n        \"skips the test if we are on the win32 platform. see \"\n        \"https://docs.pytest.org/en/latest/skipping.html\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"xfail(condition, reason=None, run=True, raises=None, strict=False): \"\n        \"ma",
            "start_line": 51,
            "start_column": 1531,
            "end_line": 0,
            "end_column": 1989,
            "start_byte": 1531,
            "end_byte": 0
          },
          "_pytest.warnings.pytest_configure": {
            "name": "_pytest.warnings.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "the test function as an expected failure if eval(condition) \"\n        \"has a True value. Optionally specify a reason for better reporting \"\n        \"and run=False if you don't even want to execute the test function. \"\n        \"If only specific except",
            "start_line": 68,
            "start_column": 1992,
            "end_line": 0,
            "end_column": 2242,
            "start_byte": 1992,
            "end_byte": 0
          },
          "_pytest.nose.pytest_runtest_setup": {
            "name": "_pytest.nose.pytest_runtest_setup",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "r\nfrom _pytest.outcomes import fail\nfrom _pytest.outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"--runxfail\",\n        action=\"store_true\",\n      ",
            "start_line": 6,
            "start_column": 136,
            "end_line": 0,
            "end_column": 548,
            "start_byte": 136,
            "end_byte": 0
          },
          "_pytest.resultlog.pytest_addoption": {
            "name": "_pytest.resultlog.pytest_addoption",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "outcomes import skip\nfrom _pytest.outcomes import xfail\nfrom _pytest.store import StoreKey\n\n\nskipped_by_mark_key = StoreKey[bool]()\nevalxfail_key = StoreKey[MarkEvaluator]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\ndef pytest_addoption(parser):\n    group = parser.getgroup(\"general\")\n    group.addoption(\n        \"-",
            "start_line": 13,
            "start_column": 185,
            "end_line": 0,
            "end_column": 501,
            "start_byte": 185,
            "end_byte": 0
          },
          "_pytest.resultlog.pytest_configure": {
            "name": "_pytest.resultlog.pytest_configure",
            "file_path": "src/_pytest/skipping.py",
            "file_content": "nxfail\",\n        action=\"store_true\",\n        dest=\"runxfail\",\n        default=False,\n        help=\"report the results of xfail tests as if they were not marked\",\n    )\n\n    parser.addini(\n        \"xfail_strict\",\n        \"default for the strict parameter of xfail \"\n        \"markers when not given explicitly (default: False)\",\n        default=False,\n        type=\"bool\",\n    )\n\n\ndef pytest_configure(config):\n    if config.option.runxfail:\n        # yay a hack\n        import pytest\n\n        old = pytest.xfail\n        config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n\n        def nop(*args, **kwargs):\n            pass\n\n        nop.Exception = xfail.Exception\n        setattr(",
            "start_line": 25,
            "start_column": 504,
            "end_line": 0,
            "end_column": 1195,
            "start_byte": 504,
            "end_byte": 0
          },
          "_pytest.runner.runtestprotocol": {
            "name": "_pytest.runner.runtestprotocol",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def runtestprotocol(item, log=True, nextitem=None):\n    hasrequest = hasattr(item, \"_request\")\n    if hasrequest and not item._request:\n        item._initrequest()\n    rep = call_and_report(item, \"setup\", log)\n    reports = [rep]\n    if rep.passed:\n        if item.config.getoption(\"setupshow\", False):\n            show_test_item(item)\n        if not item.config.getoption(\"setuponly\", False):\n            reports.append(call_and_report(item, \"call\", log))\n    reports.append(call_and_report(item, \"teardown\", log, nextitem=nextitem))\n    # after all teardown hooks have been called\n    # want funcargs and request info to go away\n    if hasrequest:\n        item._request = False\n        item.funcargs = None\n    return reports",
            "start_line": 93,
            "start_column": 2710,
            "end_line": 0,
            "end_column": 3437,
            "start_byte": 2710,
            "end_byte": 0
          },
          "_pytest.runner.call_and_report": {
            "name": "_pytest.runner.call_and_report",
            "file_path": "src/_pytest/runner.py",
            "file_content": "def call_and_report(\n    item, when: \"Literal['setup', 'call', 'teardown']\", log=True, **kwds\n):\n    call = call_runtest_hook(item, when, **kwds)\n    hook = item.ihook\n    report = hook.pytest_runtest_makereport(item=item, call=call)\n    if log:\n        hook.pytest_runtest_logreport(report=report)\n    if check_interactive_exception(call, report):\n        hook.pytest_exception_interact(node=item, call=call, report=report)\n    return report",
            "start_line": 187,
            "start_column": 5534,
            "end_line": 0,
            "end_column": 5976,
            "start_byte": 5534,
            "end_byte": 0
          },
          "_pytest.unittest._make_xunit_fixture": {
            "name": "_pytest.unittest._make_xunit_fixture",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def _make_xunit_fixture(obj, setup_name, teardown_name, scope, pass_self):\n    setup = getattr(obj, setup_name, None)\n    teardown = getattr(obj, teardown_name, None)\n    if setup is None and teardown is None:\n        return None\n\n    @pytest.fixture(scope=scope, autouse=True)\n    def fixture(self, request):\n        if getattr(self, \"__unittest_skip__\", None):\n            reason = self.__unittest_skip_why__\n            pytest.skip(reason)\n        if setup is not None:\n            if pass_self:\n                setup(self, request.function)\n            else:\n                setup()\n        yield\n        if teardown is not None:\n            if pass_self:\n                teardown(self, request.function)\n            else:\n                teardown()\n\n    return fixture",
            "start_line": 83,
            "start_column": 2910,
            "end_line": 0,
            "end_column": 3683,
            "start_byte": 2910,
            "end_byte": 0
          },
          "_pytest.unittest.TestCaseFunction.runtest": {
            "name": "_pytest.unittest.TestCaseFunction.runtest",
            "file_path": "src/_pytest/unittest.py",
            "file_content": "def runtest(self):\n        from _pytest.debugging import maybe_wrap_pytest_function_for_tracing\n\n        maybe_wrap_pytest_function_for_tracing(self)\n\n        # let the unittest framework handle async functions\n        if is_async_function(self.obj):\n            self._testcase(self)\n        else:\n            # when --pdb is given, we want to postpone calling tearDown() otherwise\n            # when entering the pdb prompt, tearDown() would have probably cleaned up\n            # instance variables, which makes it difficult to debug\n            # arguably we could always postpone tearDown(), but this changes the moment where the\n            # TestCase instance interacts with the results object, so better to only do it\n            # when absolutely needed\n            if self.config.getoption(\"usepdb\"):\n                self._explicit_tearDown = self._testcase.tearDown\n                setattr(self._testcase, \"tearDown\", lambda *args: None)\n\n            # we need to update the actual bound method with self.obj, because\n            # wrap_pytest_function_for_tracing replaces self.obj by a wrapper\n            setattr(self._testcase, self.name, self.obj)\n            try:\n                self._testcase(result=self)\n            finally:\n                delattr(self._testcase, self.name)",
            "start_line": 207,
            "start_column": 7152,
            "end_line": 4,
            "end_column": 8447,
            "start_byte": 7152,
            "end_byte": 4
          },
          "_pytest.debugging.PdbInvoke.pytest_exception_interact": {
            "name": "_pytest.debugging.PdbInvoke.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "def pytest_exception_interact(self, node, call, report):\n        capman = node.config.pluginmanager.getplugin(\"capturemanager\")\n        if capman:\n            capman.suspend_global_capture(in_=True)\n            out, err = capman.read_global_capture()\n            sys.stdout.write(out)\n            sys.stdout.write(err)\n        _enter_pdb(node, call.excinfo, report)",
            "start_line": 257,
            "start_column": 8431,
            "end_line": 4,
            "end_column": 8796,
            "start_byte": 8431,
            "end_byte": 4
          },
          "_pytest.debugging._enter_pdb": {
            "name": "_pytest.debugging._enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "def _enter_pdb(node, excinfo, rep):\n    # XXX we re-use the TerminalReporter's terminalwriter\n    # because this seems to avoid some encoding related troubles\n    # for not completely clear reasons.\n    tw = node.config.pluginmanager.getplugin(\"terminalreporter\")._tw\n    tw.line()\n\n    showcapture = node.config.option.showcapture\n\n    for sectionname, content in (\n        (\"stdout\", rep.capstdout),\n        (\"stderr\", rep.capstderr),\n        (\"log\", rep.caplog),\n    ):\n        if showcapture in (sectionname, \"all\") and content:\n            tw.sep(\">\", \"captured \" + sectionname)\n            if content[-1:] == \"\\n\":\n                content = content[:-1]\n            tw.line(content)\n\n    tw.sep(\">\", \"traceback\")\n    rep.toterminal(tw)\n    tw.sep(\">\", \"entering PDB\")\n    tb = _postmortem_traceback(excinfo)\n    rep._pdbshown = True\n    post_mortem(tb)\n    return rep",
            "start_line": 304,
            "start_column": 10146,
            "end_line": 0,
            "end_column": 11019,
            "start_byte": 10146,
            "end_byte": 0
          },
          "_pytest.faulthandler.FaultHandlerHooks.pytest_enter_pdb": {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "           import pdb\n\n            pdb_cls = pdb.Pdb\n\n        wrapped_cls = cls._get_pdb_wrapper_class(pdb_cls, capman)\n        cls._wrapped_pdb_cls = (usepdb_cls, wrapped_cls)\n        return wrapped_cls\n\n    @classmethod\n    def _",
            "start_line": 96,
            "start_column": 3425,
            "end_line": 4,
            "end_column": 3656,
            "start_byte": 3425,
            "end_byte": 4
          },
          "_pytest.faulthandler.FaultHandlerHooks.pytest_exception_interact": {
            "name": "_pytest.faulthandler.FaultHandlerHooks.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "b_wrapper_class(cls, pdb_cls, capman):\n        import _pytest.config\n\n        class PytestPdbWrapper(pdb_cls):\n            _pytest_capman = capman\n            _continued = False\n\n            def do_debug(self, arg):\n                cls._recursive_debug += 1\n",
            "start_line": 104,
            "start_column": 3662,
            "end_line": 4,
            "end_column": 3920,
            "start_byte": 3662,
            "end_byte": 4
          },
          "_pytest.hookspec.pytest_exception_interact": {
            "name": "_pytest.hookspec.pytest_exception_interact",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "",
            "start_line": 675,
            "start_column": 22873,
            "end_line": 0,
            "end_column": 23146,
            "start_byte": 22873,
            "end_byte": 0
          },
          "_pytest.hookspec.pytest_enter_pdb": {
            "name": "_pytest.hookspec.pytest_enter_pdb",
            "file_path": "src/_pytest/debugging.py",
            "file_content": "",
            "start_line": 684,
            "start_column": 23149,
            "end_line": 0,
            "end_column": 23438,
            "start_byte": 23149,
            "end_byte": 0
          }
        }
      }
    }
  ]
}